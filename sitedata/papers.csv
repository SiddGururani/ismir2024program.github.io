UID,day,session,position,paper_presentation,long_presentation,title,Abstract,primary_author,authors_and_affil,authors,primary_subject,secondary_subject,SpecialTrack,abstract_short,StudentAuthor,StudentAuthor.1,AwardNominee,pdf_path,video,poster_pdf,thumbnail,slides_pdf,channel_url,slack_channel
1,3,5,5,Remote,FALSE,On the validity of employing ChatGPT for distant reading of music similarity,"In this work we explore whether large language models (LLM) can be a useful and valid tool for music knowledge discovery. LLMs offer an interface to enormous quantities of text and hence can be seen as a new tool for 'distant reading', i.e. the computational analysis of text including sources about music. More specifically we investigated whether ratings of music similarity, as measured via human listening tests, can be recovered from textual data by using ChatGPT. We examined the inferences that can be drawn from these experiments through the formal lens of validity. We showed that correlation of ChatGPT with human raters is of of moderate positive size but also lower than the average human inter-rater agreement. By evaluating a number of threats to validity and conducting additional experiments with ChatGPT, we were able to show that especially construct validity of such an approach is seriously compromised. The opaque black box nature of ChatGPT makes it close to impossible to judge the experiment's construct validity, i.e. the relationship between what is meant to be inferred from the experiment, which are estimates of music similarity, and what is actually being measured. As a consequence the use of LLMs for music knowledge discovery cannot be recommended.",Arthur Flexer,Arthur Flexer (Johannes Kepler University Linz)*,"Flexer, Arthur*","Evaluation, datasets, and reproducibility","Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> reproducibility; MIR fundamentals and methodology -> web mining, and natural language processing; MIR tasks -> similarity metrics; Philosophical and ethical discussions -> philosophical and methodological foundations",,,,,No,https://drive.google.com/file/d/1dA6YiGRw7WLmcSBG-dCbmqGRjp7DpgnE/view?usp=sharing,https://drive.google.com/file/d/1XuSiWBgkUfBCnZLa-s5uAkZIua7E-p0C/view?usp=drive_link,https://drive.google.com/file/d/1VAG-uC-9QPHuPzJsxziqRpipsaG-LYLI/view?usp=sharing,https://drive.google.com/file/d/1bDKJUha7CfprIh1jVxLcMZ1aUzb7_kJK/view?usp=drive_link,https://drive.google.com/file/d/1iebn1lBmmhCCCeFtp1VsaZyHqPVmCDBA/view?usp=drive_link,https://ismir2024.slack.com/archives/C07USGCGWSY,p5-05-on-the-validity
5,2,4,12,San Francisco,FALSE,Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music and Audio Generation,"Representing symbolic music with compound tokens, where each token consists of several different sub-tokens representing a distinct musical feature or attribute, offers the advantage of reducing sequence length. While previous research has validated the efficacy of compound tokens in music sequence modeling, predicting all sub-tokens simultaneously can lead to suboptimal results as it may not fully capture the interdependencies between them. We introduce the Nested Music Transformer (NMT), an architecture tailored for decoding compound tokens autoregressively, similar to processing flattened tokens, but with low memory usage. The NMT consists of two transformers: the main decoder that models a sequence of compound tokens and the sub-decoder for modeling sub-tokens of each compound token. The experiment results showed that applying the NMT to compound tokens can enhance the performance in terms of better perplexity in processing various symbolic music datasets and discrete audio tokens from the MAESTRO dataset.",Dasaem Jeong,Jiwoo Ryu (Sogang University); Hao-Wen Dong (University of Michigan); Jongmin Jung (Sogang University); Dasaem Jeong (Sogang University)*,"Ryu, Jiwoo; Dong, Hao-Wen; Jung, Jongmin; Jeong, Dasaem*",MIR tasks -> music generation,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing,,,,,No,https://drive.google.com/file/d/1E8CTFJvSUaksQAHwn5DzFUtPTiuSNZ4L/view?usp=share_link,https://drive.google.com/file/d/1d4vhwOeI7Ve1w0niVr5Y-qRko_ONQcoN/view?usp=share_link,https://drive.google.com/file/d/19ZkRhN2wiJdL1LnQt8dfOMXD__2YV6LW/view?usp=sharing,https://drive.google.com/file/d/1ulzf3JND3fCeQwGLr8ueKujOh5F-arSI/view?usp=sharing,https://drive.google.com/file/d/1zpIzBSfRxyU4f4PIJR_kc6hxOb41Dud7/view?usp=sharing,https://ismir2024.slack.com/archives/C07V2ML0VSM,p4-12-nested-music-transformer
8,1,1,9,San Francisco,FALSE,Robust and Accurate Audio Synchronization Using Raw Features From Transcription Models,"In Music Information Retrieval (MIR), precise synchronization of musical events is crucial for tasks like aligning symbolic information with music recordings or transferring annotations between audio versions. To achieve high temporal accuracy, synchronization approaches integrate onset-related information extracted from music recordings using either traditional signal processing techniques or exploiting symbolic representations obtained by data-driven automatic music transcription (AMT) approaches. In line with this research direction, our paper introduces a high-resolution synchronization approach that combines recent AMT techniques with traditional synchronization methods. Rather than relying on the final symbolic AMT results, we show how to exploit raw onset and frame predictions obtained as intermediate outcomes from a state-of-the-art AMT approach. Through extensive evaluations conducted on piano recordings under varied acoustic conditions across different transcription models, audio features, and dynamic time warping variants, we illustrate the advantages of our proposed method in both audio–audio and audio–score synchronization tasks. Specifically, we emphasize the effectiveness of our approach in aligning historical piano recordings with poor audio quality. We underscore how additional fine-tuning steps of the transcription model on the target dataset enhance alignment robustness, even in challenging acoustic environments.",Johannes Zeitler,Johannes Zeitler (International Audio Laboratories Erlangen)*; Ben Maman (Tel Aviv University); Meinard Müller (International Audio Laboratories Erlangen),"Zeitler, Johannes*; Maman, Ben; Müller, Meinard","MIR tasks -> alignment, synchronization, and score following",MIR tasks -> music transcription and annotation,,,,,No,https://drive.google.com/file/d/1ovrLChf92BfHq_ilyOqp6VIm6DOYTKMZ/view?usp=sharing,https://drive.google.com/file/d/1YObvTral3sonXEVYqTR-gIs_grqizI72/view?usp=sharing,https://drive.google.com/file/d/1Yeeif-eM52-Vv73rbePu4_580JjPKdNH/view?usp=sharing,https://drive.google.com/file/d/1Xhbn8tKt43J3OItlISBo2JOUww4rs3AS/view?usp=sharing,https://drive.google.com/file/d/10_RIbxSnAKvAcqTRndJQvS6l2PGkU7sE/view?usp=sharing,https://ismir2024.slack.com/archives/C07UPU6SEN7,p1-09-robust-and-accurate
9,2,4,1,San Francisco,TRUE,Cluster and Separate: a GNN Approach to Voice and Staff Prediction for Score Engraving,"This paper approaches the problem of separating the notes from a quantized symbolic music piece (e.g., a MIDI file) into multiple voices and staves. This is a fundamental part of the larger task of music score engraving (or score typesetting), which aims to produce readable musical scores for human performers. We focus on piano music and support homophonic voices, i.e., voices that can contain chords, and cross-staff voices, which are notably difficult tasks that have often been overlooked in previous research. We propose an end-to-end system based on graph neural networks that clusters notes that belong to the same chord and connects them with edges if they are part of a voice. Our results show clear and consistent improvements over a previous approach on two datasets of different styles. To aid the qualitative analysis of our results, we support the export in symbolic music formats and provide a direct visualization of our outputs graph over the musical score. All code and pre-trained models are available at https://github.com/CPJKU/piano_svsep.",Francesco Foscarin,Francesco Foscarin (Johannes Kepler University Linz)*; Emmanouil Karystinaios (Johannes Kepler University); Eita Nakamura (Kyoto University); Gerhard Widmer (Johannes Kepler University),"Foscarin, Francesco*; Karystinaios, Emmanouil; Nakamura, Eita; Widmer, Gerhard",MIR fundamentals and methodology -> symbolic music processing,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music; MIR tasks -> music transcription and annotation; Musical features and properties -> representations of music,,,,,Yes,https://drive.google.com/file/d/1PmF0u_7o7ZK5P0SYyGiQOs18Xog2OOlS/view?usp=sharing,https://drive.google.com/file/d/1QAMVD9519WvHgz5Dlh39j8YCZtjUZ62O/view?usp=sharing,https://drive.google.com/file/d/1PamUFD3n7kGV__DCVZPZd3TVHjgpDztW/view?usp=sharing,https://drive.google.com/file/d/17pv5SIysX3ao3sQ187USvJryc5unEp4Q/view?usp=share_link,https://docs.google.com/presentation/d/1Pac9xklCcyApJsq4x-Erg6rz208Qemzk/edit?usp=share_link&ouid=109392096769076802764&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07U9EY9AAK,p4-01-cluster-and-separate
10,3,6,19,San Francisco,FALSE,Beat this! Accurate beat tracking without DBN postprocessing,"We propose a system for tracking beats and downbeats with two objectives: generality across a diverse music range, and high accuracy. We achieve generality by training on multiple datasets -- including solo instrument recordings, pieces with time signature changes, and classical music with high tempo variations -- and by removing the commonly used Dynamic Bayesian Network (DBN) postprocessing, which introduces constraints on the meter and tempo. For high accuracy, among other improvements, we develop a loss function tolerant to small time shifts of annotations, and an architecture alternating convolutions with transformers either over frequency or time. Our system surpasses the current state of the art in F1 score despite using no DBN. However, it can still fail, especially for difficult and underrepresented genres, and performs worse on continuity metrics, so we publish our model, code, and preprocessed datasets, and invite others to beat this.",Francesco Foscarin,Francesco Foscarin (Johannes Kepler University Linz)*; Jan Schlüter (JKU Linz); Gerhard Widmer (Johannes Kepler University),"Foscarin, Francesco*; Schlüter, Jan; Widmer, Gerhard","Musical features and properties -> rhythm, beat, tempo",Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,,,,,No,https://drive.google.com/file/d/1h9AImUGC_47nMT_rV1AYJOGvFkdoAEu8/view?usp=sharing,https://drive.google.com/file/d/1-ll3kj92H0jJTdQ851VIO8lXCBDw_qge/view?usp=sharing,https://drive.google.com/file/d/15AS9uKpRbx36lXTLVpiOlfpfXnXAw-tY/view?usp=sharing,https://drive.google.com/file/d/18I-8CqusUouwV1uqsWd8r2pq_NrqQR-X/view?usp=sharing,https://drive.google.com/file/d/187xf2RN999NK1O6G-7VP1ui8NGibiS3W/view?usp=sharing,https://ismir2024.slack.com/archives/C07UM5S4VJR,p6-19-beat-this-accurate
12,1,1,18,San Francisco,FALSE,Learning Multifaceted Self-Similarity over Time and Frequency for Music Structure Analysis,"This paper describes a deep learning method for music structure analysis (MSA) that aims to split a music signal into temporal segments and assign a function label (e.g., intro, verse, or chorus) to each segment. The computational base for MSA is a spectro-temporal representation of input audio such as the spectrogram, where the compositional relationships of the spectral components provide valuable clues (e.g., chords) to the identification of structural units. However, such implicit features might be vulnerable to local operations such as convolution and pooling operations. In this paper, we hypothesize that the self-attention over the spectral domain as well as the temporal domain plays a key role in tackling MSA. Based on this hypothesis, we propose a novel MSA model built on the Transformer-in-Transformer architecture that alternately stacks spectral and temporal self-attention layers. Experiments with the Beatles, RWC, and SALAMI datasets showed the superiority of the dual-aspect self-attention. In particular, the differentiation between spectral and temporal self-attentions can provide extra performance gain. By analyzing the attention maps, we also demonstrate that self-attention can unfold tonal relationships and the internal structure of music.",Tsung-Ping Chen,Tsung-Ping Chen (Kyoto University)*; Kazuyoshi Yoshii (Kyoto University),"Chen, Tsung-Ping*; Yoshii, Kazuyoshi","Musical features and properties -> structure, segmentation, and form",Musical features and properties -> representations of music,,,,,No,https://drive.google.com/file/d/13xy02rQcrgv5C8TZKQF17dUUO3CkQP52/view?usp=sharing,https://drive.google.com/file/d/112O5L8XUnDxGENRx4lG4MJSImunVRW5p/view?usp=sharing,https://drive.google.com/file/d/14RwLEkmKADHV1DAkWTXs6rCRe0KDXOoa/view?usp=sharing,https://drive.google.com/file/d/1gw9zu9cfPC_Z8Ar-nB8AiYEhOUKOWORL/view?usp=sharing,https://drive.google.com/file/d/1S9UiInHkTAylonAZoTPHrnfTgZzq6L30/view?usp=sharing,https://ismir2024.slack.com/archives/C07VCQWBJ3A,p1-18-learning-multifaceted-self
13,1,2,5,San Francisco,FALSE,Notewise Evaluation of Source Separation: A Case Study For Separated Piano Tracks,"Deep learning has significantly advanced music source separation (MSS), aiming to decompose music recordings into individual tracks corresponding to singing or specific instruments. Typically, results are evaluated using quantitative measures like signal-to-distortion ratio (SDR) computed for entire excerpts or songs. As the main contribution of this article, we introduce a novel evaluation approach that decomposes an audio track into musically meaningful sound events and applies the evaluation metric based on these units. In a case study, we apply this strategy to the challenging task of separating piano concerto recordings into piano and orchestra tracks. To assess piano separation quality, we use a score-informed nonnegative matrix factorization approach to decompose the reference and separated piano tracks into notewise sound events. In our experiments assessing various MSS systems, we demonstrate that our notewise evaluation, which takes into account factors such as pitch range and musical complexity, enhances the comprehension of both the results of source separation and the intricacies within the underlying music.",Yigitcan Özer,"Yigitcan Özer (International Audio Laboratories Erlangen)*; Hans-Ulrich Berendes (International Audio Laboratories Erlangen); Vlora Arifi-Müller (International Audio Laboratories Erlangen ); Fabian-Robert Stöter (AudioShake, Inc.); Meinard Müller (International Audio Laboratories Erlangen)","Özer, Yigitcan*; Berendes, Hans-Ulrich; Arifi-Müller, Vlora; Stöter, Fabian-Robert; Müller, Meinard","Evaluation, datasets, and reproducibility -> evaluation metrics","Evaluation, datasets, and reproducibility -> annotation protocols; Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> sound source separation",,,,,No,https://drive.google.com/file/d/1oJhm0Gj0qHwbdpcwU5kuagau9KlttYKn/view?usp=sharing,https://drive.google.com/file/d/1zvXE3Zc-RS1CJEoRxMBhb4vsW6K-FXzZ/view?usp=drive_link,https://drive.google.com/file/d/1I-RG-YNaUPQlj5PR26JqkwFtNroKfMXf/view?usp=drive_link,https://drive.google.com/file/d/1GZzsLWg_k1nqfpWHE63d07ZcfwtJw1yu/view?usp=drive_link,https://docs.google.com/presentation/d/1bvPSK0XIQmb5AMwW9wA3Wz6QfNpVHrgm/edit?usp=drive_link&ouid=103065613706145796406&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07UQ1C8K5Y,p2-05-notewise-evaluation-of
18,3,6,12,San Francisco,FALSE,Streaming Piano Transcription Based on Consistent Onset and Offset Decoding with Sustain Pedal Detection,"This paper describes a streaming audio-to-MIDI piano transcription approach that aims to sequentially translate a music signal into a sequence of note onset and offset events. The sequence-to-sequence nature of this task may call for the computationally-intensive transformer model for better performance, which has recently been used for offline transcription benchmarks and could be extended for streaming transcription with causal attention mechanisms. We assume that the performance limitation of this naive approach lies in the decoder. Although time-frequency features useful for onset detection are considerably different from those for offset detection, the single decoder is trained to output a mixed sequence of onset and offset events without guarantee of the correspondence between the onset and offset events of the same note. To overcome this limitation, we propose a streaming encoder-decoder model that uses a convolutional encoder aggregating local acoustic features, followed by an autoregressive Transformer decoder detecting a variable number of onset events and another decoder detecting the offset events for the active pitches with validation of the sustain pedal at each time frame. Experiments using the MAESTRO dataset showed that the proposed streaming method performed comparably with or even better than the state-of-the-art offline methods while significantly reducing the computational cost.",Weixing Wei,Weixing Wei (Kyoto University)*; Jiahao Zhao (Kyoto University); Yulun Wu (Fudan University); Kazuyoshi Yoshii (Kyoto University),"Wei, Weixing*; Zhao, Jiahao; Wu, Yulun; Yoshii, Kazuyoshi",MIR tasks -> music transcription and annotation,,,,,,No,https://drive.google.com/file/d/1W4gBY_EmjutsJSn1xUUT4YikW3Nmbe1G/view?usp=sharing,https://drive.google.com/file/d/1_iXRxgXqrGNFVHh5XESpnD5jm169EMfM/view?usp=sharing,https://drive.google.com/file/d/16lZk-YBqSk_ypmZkZlSXqRroFivWVxY3/view?usp=sharing,https://drive.google.com/file/d/1ORPgbshIbOPrG7xibK3s_sUx0GZGmTDU/view?usp=sharing,https://drive.google.com/file/d/1xDpsdwRcGbmbXnzzerRpqxIN0lyctJ9Z/view?usp=sharing,https://ismir2024.slack.com/archives/C07USGD286Q,p6-12-streaming-piano-transcription
19,2,3,5,San Francisco,FALSE,Cue Point Estimation using Object Detection,"Cue points indicate possible temporal boundaries in a transition between two pieces of music in DJ mixing and constitute a crucial element in autonomous DJ systems as well as for live mixing. In this work, we present a novel method for automatic cue point estimation, interpreted as a computer vision object detection task. Our proposed system is based on a pre-trained object detection transformer which we fine-tune on our novel cue point dataset. Our provided dataset contains 21k manually annotated cue points from human experts as well as metronome information for nearly 5k individual tracks, making this dataset 35x larger than the previously available cue point dataset. Unlike previous methods, our approach does not require low-level musical information analysis, while demonstrating increased precision in retrieving cue point positions. Moreover, our proposed method demonstrates high adherence to phrasing, a type of high-level music structure commonly emphasized in electronic dance music. The code, model checkpoints, and dataset are made publicly available.",Luca A Lanzendoerfer,Giulia Arguello (ETH Zurich); Luca A Lanzendoerfer (ETH Zurich)*; Roger Wattenhofer (ETH Zurich),"Arguello, Giulia; Lanzendoerfer, Luca A*; Wattenhofer, Roger","Evaluation, datasets, and reproducibility -> novel datasets and use cases",Creativity -> tools for artists; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> pattern matching and detection,,,,,No,https://drive.google.com/file/d/1qAbrZKi-ItKh6F5IcxY9SVbtROgDQR2G/view?usp=sharing,https://drive.google.com/file/d/1MLy__nYjpuhpzC8A7JoXcbv2S3G-h8V0/view?usp=sharing,https://drive.google.com/file/d/1-MPgcWlKym7T6k4lLhTrt7VIa016_Eqq/view?usp=sharing,https://drive.google.com/file/d/1qOroLJ5UcefxgrALmSFTpQtjy1xmlqcU/view?usp=sharing,https://drive.google.com/file/d/1y-Vitk0xu9FmOKt0hQsucMnMyZwiEvhP/view?usp=sharing,https://ismir2024.slack.com/archives/C07UPU7B599,p3-05-cue-point-estimation
22,4,7,6,San Francisco,FALSE,Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models,"In this paper, we propose and investigate the use of neural audio codec language models for automatic generation of sample-based musical instruments based on text or reference audio prompts.  Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes.  We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.",Shahan Nercessian,Shahan Nercessian (Native Instruments)*; Johannes Imort (Native Instruments); Ninon Devis (Native Instruments); Frederik Blang (Native Instruments),"Nercessian, Shahan*; Imort, Johannes; Devis, Ninon; Blang, Frederik",Generative Tasks -> music and audio synthesis,Generative Tasks -> artistically-inspired generative tasks ; Generative Tasks -> evaluation metrics,,,,,No,https://drive.google.com/file/d/1JSNa6kdSy1HbTaV90ziXODAfPvCMRS3W/view?usp=sharing,https://drive.google.com/file/d/1e2NU_vLLkHDZIDgUnV3coAUFXcYRkHDo/view?usp=sharing,https://drive.google.com/file/d/1cz6Qnzc3sW1F5EOUEFICXT1oEd6PMDyK/view?usp=sharing,https://drive.google.com/file/d/19NV6asW0uyiNpY8JILdYGemXZ_T5vo53/view?usp=sharing,https://drive.google.com/file/d/18BRzZyJt-TQZ6cylCtiPpDQGtqTEFFIt/view?usp=sharing,https://ismir2024.slack.com/archives/C07UHCM7A9L,p7-06-generating-sample-based
24,2,4,5,San Francisco,FALSE,El Bongosero: A Crowd-sourced Symbolic Dataset of Improvised Hand Percussion Rhythms Paired with Drum Patterns,"We present El Bongosero, a large-scale, open-source symbolic dataset comprising expressive, improvised drum performances crowd-sourced from a pool of individuals with varying levels of musical expertise. Originating from an interactive installation hosted at Centre de Cultura Contemporània de Barcelona, our dataset consists of 6,035 unique tapped sequences performed by 3,184 participants. To our knowledge, this is the only symbolic dataset of its size and type that includes expressive timing and dynamics information as well as each participant's level of expertise. These unique characteristics could prove to be valuable to future research, particularly in the areas of music generation and music education. Preliminary analysis, including a step-wise Jaccard similarity analysis on a subset of the data, demonstrate that this dataset is a diverse, non-random, and musically meaningful collection. To facilitate prompt exploration and understanding of the data, we have also prepared a dedicated website and an open-source API in order to interact with the data.",Nicholas Evans,Behzad Haki (Universitat Pompeu Fabra); Nicholas Evans (Universitat Pompeu Fabra)*; Daniel Gómez (MTG); Sergi Jordà (Universitat Pompeu Fabra),"Haki, Behzad; Evans, Nicholas*; Gómez, Daniel; Jordà, Sergi","Evaluation, datasets, and reproducibility -> novel datasets and use cases","Musical features and properties -> rhythm, beat, tempo",,,,,No,https://drive.google.com/file/d/1HWtECEdTCXBadHE1SZX61uFCqSA5w3Zr/view?usp=sharing,https://drive.google.com/file/d/13GqBs9WeVeweyL3J4_42sBlSgEES6eMq/view?usp=sharing,https://drive.google.com/file/d/1_XdGgImckqE1SPlOhdFutotqmWzKJBvF/view?usp=sharing,https://drive.google.com/file/d/1unWOlZ-Q98Qpc3py14DSmOx_u_Sz8G8q/view?usp=sharing,https://drive.google.com/file/d/10r-HxMuu-bhhBzwjAnqvw6OZTA9haytt/view?usp=sharing,https://ismir2024.slack.com/archives/C07U9EYS9QF,p4-05-el-bongosero-a
31,1,1,7,San Francisco,FALSE,"Can LLMs ""Reason"" in Music? An Evaluation of LLMs' Capability of Music Understanding and Generation","Symbolic Music, akin to language, can be encoded in discrete symbols. Recent research has extended the application of large language models (LLMs) such as GPT-4 and Llama2 to the symbolic music domain including understanding and generation. Yet scant research explores the details of how these LLMs perform on advanced music understanding and conditioned generation, especially from the multi-step reasoning perspective, which is a critical aspect in the conditioned, editable, and interactive human-computer co-creation process.  This study conducts a thorough investigation of LLMs' capability and limitations in symbolic music processing. We identify that current LLMs exhibit poor performance in song-level multi-step music reasoning, and typically fail to leverage learned music knowledge when addressing complex musical tasks. An analysis of LLMs' responses highlights distinctly their pros and cons. Our findings suggest achieving advanced musical capability is not a free lunch for LLMs, and future research should focus more on bridging the gap between music knowledge and reasoning, to improve the co-creation experience for musicians.",Ziya Zhou,Ziya Zhou (HKUST)*; Yuhang Wu (Multimodal Art Projection); Zhiyue Wu (Shenzhen University); Xinyue Zhang (Multimodal Art Projection); Ruibin Yuan (CMU); Yinghao MA (Queen Mary University of London); Lu Wang (Shenzhen University); Emmanouil Benetos (Queen Mary University of London); Wei Xue (The Hong Kong University of Science and Technology); Yike Guo (Hong Kong University of Science and Technology),"Zhou, Ziya*; Wu, Yuhang; Wu, Zhiyue; Zhang, Xinyue; Yuan, Ruibin; MA, Yinghao; Wang, Lu; Benetos, Emmanouil; Xue, Wei; Guo, Yike",Generative Tasks -> artistically-inspired generative tasks,Creativity -> computational creativity; Creativity -> human-ai co-creativity; Human-centered MIR -> human-computer interaction; Human-centered MIR -> user-centered evaluation,,,,,No,https://drive.google.com/file/d/1WF46yo_uNSFyI1eaPBltBD7KXN_H9Rp2/view?usp=drive_link,https://drive.google.com/file/d/1sN3MNgFzS8TKqubiNFncKUlVPjsvpF02/view?usp=sharing,https://drive.google.com/file/d/1Kf0_BAAu_F9nHnu5naMyzlHcUsLZYZK0/view?usp=sharing,,https://drive.google.com/file/d/1SrAGLKs1mbFAUDDSR2XLkDcEMnYsnDFl/view?usp=sharing,https://ismir2024.slack.com/archives/C07VCQWS056,p1-07-can-llms-reason
32,4,7,12,San Francisco,FALSE,A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems,"Despite significant recent progress across multiple subtasks of audio source separation, few music source separation systems support separation beyond the four-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current systems that support source separation beyond this setup, most continue to rely on an inflexible decoder setup that can only support a fixed pre-defined set of stems. Increasing stem support in these inflexible systems correspondingly requires increasing computational complexity, rendering extensions of these systems computationally infeasible for long-tail instruments. We propose Banquet, a system that allows source separation of multiple stems using just one decoder. A bandsplit source separation model is extended to work in a query-based setup in tandem with a music instrument recognition PaSST model. On the MoisesDB dataset, Banquet — at only 24.9 M trainable parameters — performed on par with or better than the significantly more complex 6-stem Hybrid Transformer Demucs. The query-based setup allows for the separation of narrow instrument classes such as clean acoustic guitars, and can be successfully applied to the extraction of less common stems such as reeds and organs.",Karn N Watcharasupat,Karn N Watcharasupat (Georgia Institute of Technology)*; Alexander Lerch (Georgia Institute of Technology),"Watcharasupat, Karn N*; Lerch, Alexander",MIR tasks -> sound source separation,"Creativity -> tools for artists; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> indexing and querying; Musical features and properties -> timbre, instrumentation, and singing voice",,,,,No,https://drive.google.com/file/d/1o7k0dD9lJ6aP0DDtX2SxXA-UO8QwSQXs/view?usp=drive_link,https://drive.google.com/file/d/1mujQfs5mHI_SB15pzwPf0CUSTuoPyDXg/view?usp=drive_link,https://drive.google.com/file/d/1-A7wIS_Eo0VTqp6yVFjC7x_nyyhnTvW_/view?usp=drive_link,https://drive.google.com/file/d/1x1wJNvbQ6PwtUhqMOxJVfLOhPviARX2I/view?usp=drive_link,https://drive.google.com/file/d/1TZ3_tHjL8ydmwlR5qfBwexzI96rFJhN8/view?usp=drive_link,https://ismir2024.slack.com/archives/C07U9EZ2NH5,p7-12-a-stem-agnostic
35,1,2,1,San Francisco,TRUE,Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding,"We introduce a project that revives a piece of 15th-century Korean court music, Chwipunghyeong, composed upon the poem 'Songs of the Dragon Flying to Heaven'. One of the earliest examples of Jeongganbo, a Korean musical notation system, the remaining version only consists of a rudimentary melody. Our research team, commissioned by the National Gugak (Korean Traditional Music) Center, aimed to transform this old melody into a performable arrangement for a six-part ensemble. Using Jeongganbo data acquired through bespoke optical music recognition, we trained a BERT-like masked language model and an encoder-decoder transformer model. We also propose an encoding scheme that strictly follows the structure of Jeongganbo and denotes note durations as positions. The resulting machine-transformed version of Chwipunghyeong was evaluated by experts and is scheduled to be performed by the Court Music Orchestra of National Gugak Center. Our work demonstrates that generative models can successfully be applied to traditional music with limited training data if combined with careful design.",Dasaem Jeong,Danbinaerin Han (KAIST); Mark R H Gotham (Durham); DongMin Kim (Sogang University); Hannah Park (Sogang University); Sihun Lee (Sogang University); Dasaem Jeong (Sogang University)*,"Han, Danbinaerin; Gotham, Mark R H; Kim, DongMin; Park, Hannah; Lee, Sihun; Jeong, Dasaem*","Applications -> music composition, performance, and production",Applications -> music heritage and sustainability; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music generation,,,,,Yes,https://drive.google.com/file/d/1DeBMx3GiwXq6Jss-UNAHqg3knxh3NT38/view?usp=drive_link,https://drive.google.com/file/d/1niUE0tEkH0PMP_GE9EhMe1FyckKUHpSr/view?usp=drive_link,https://drive.google.com/file/d/1ToZXB9UB1VjG19bVSX2NzE1xLd10d4E-/view?usp=drive_link,https://drive.google.com/file/d/1zQ_ErYB9wyqKxrPflKBkMw0Y-cGPqtZc/view?usp=drive_link,https://drive.google.com/file/d/1nszkf0Pd-htd_eHk6aTP8PpdQkgSVqO0/view?usp=drive_link,https://ismir2024.slack.com/archives/C07USGDGH1A,p2-01-six-dragons-fly
38,1,1,5,San Francisco,FALSE,FruitsMusic: A Real-World Corpus of Japanese Idol-Group Songs,"This study presents FruitsMusic, a metadata corpus of Japanese idol-group songs in the real world, precisely annotated with who sings what and when. Japanese idol-group songs, vital to Japanese pop culture, feature a unique vocal arrangement style, where songs are divided into several segments, and a specific individual or multiple singers are assigned to each segment. To enhance singer diarization methods for recognizing such structures, we constructed FruitsMusic as a resource using 40 music videos of Japanese idol groups from YouTube. The corpus includes detailed annotations, covering songs across various genres, division and assignment styles, and groups ranging from 4 to 9 members. FruitsMusic also facilitates the development of various music information retrieval techniques, such as lyrics transcription and singer identification, benefiting not only Japanese idol-group songs but also a wide range of songs featuring single or multiple singers from various cultures. This paper offers a comprehensive overview of FruitsMusic, including its creation methodology and unique characteristics compared to conversational speech. Additionally, this paper evaluates the efficacy of current methods for singer embedding extraction and diarization in challenging real-world conditions using FruitsMusic. Furthermore, this paper examines potential improvements in automatic diarization performance through evaluating human performance.",Hitoshi Suda,Hitoshi Suda (National Institute of Advanced Industrial Science and Technology (AIST))*; Shunsuke Yoshida (The University of Tokyo); Tomohiko Nakamura (National Institute of Advanced Industrial Science and Technology (AIST)); Satoru Fukayama (National Institute of Advanced Industrial Science and Technology (AIST)); Jun Ogata (AIST),"Suda, Hitoshi*; Yoshida, Shunsuke; Nakamura, Tomohiko; Fukayama, Satoru; Ogata, Jun","Evaluation, datasets, and reproducibility -> novel datasets and use cases","Musical features and properties -> timbre, instrumentation, and singing voice",,,,,No,https://drive.google.com/file/d/1tP9dkZCAXSH3EY2iZk5znhBUFDXitWFr/view?usp=drive_link,https://drive.google.com/file/d/1uO7oXRmT8aqi3TqoO7X9bXHMi77WpUiP/view?usp=drive_link,https://drive.google.com/file/d/14XSOt9VO4srbVQIc08K1PqxXeJsczxbX/view?usp=drive_link,https://drive.google.com/file/d/1wEaOrGd12pgHCnwduMjbyzjuChxdbBK1/view?usp=drive_link,https://drive.google.com/file/d/1dCzWpoTgQK_K3uz6JWMlzAw9u9bwDM56/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UM5SUSMB,p1-05-fruitsmusic-a-real
41,1,1,13,San Francisco,FALSE,Audio Conditioning for Music Generation via Discrete Bottleneck Features,"While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding ""pseudowords"" in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies and provide music samples in order to show the quality of our model.",Simon Rouard,Simon Rouard (Meta AI Research)*; Alexandre Defossez (Kyutai); Yossi Adi (Facebook AI Research ); Jade Copet (Meta AI Research); Axel Roebel (IRCAM),"Rouard, Simon*; Defossez, Alexandre; Adi, Yossi; Copet, Jade; Roebel, Axel",MIR tasks -> music generation,Creativity -> human-ai co-creativity; Generative Tasks -> music and audio synthesis; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing,,,,,No,https://drive.google.com/file/d/1AXwd4qZXBWY7jJ12ASDW13nKpzPeW910/view?usp=drive_link,https://drive.google.com/file/d/1C_XUyhaIMcDv04fFALbIhQemHpVAyYw9/view?usp=drive_link,https://drive.google.com/file/d/1I1TzCEWABu-BD52VU9EDr0hjPFr3Cg1k/view?usp=drive_link,https://drive.google.com/file/d/1KIzQlQbxxz8C4TrXwWVAgSnKHl5QL_vU/view?usp=drive_link,https://drive.google.com/file/d/1cFY3SlwvKzG8MAkarCEDr_GxzLj8ps9z/view?usp=drive_link,https://ismir2024.slack.com/archives/C07V2MM27CZ,p1-13-audio-conditioning-for
42,1,2,13,San Francisco,FALSE,A Method for MIDI Velocity Estimation for Piano Performance by a U-Net with Attention and FiLM,"It is well known fact that the dynamics in piano performance gives significant effect in expressiveness. Taking the polyphonic nature of the instrument into account, analysing information to form dynamics for each performed note has significant meaning to understand piano performance in a quantitative way. It is also a key element in an education context for piano learners.   In this study, we developed a model for estimating MIDI velocity for each note, as one of indicators to represent loudness, with a condition of score assuming educational use case, by a Deep Neural Network (DNN) utilizing a U-Net with Scaled Dot-Product Attention (Attention) and Feature-wise Linear Modulation (FiLM) conditioning.  As a result, we prove that effectiveness of Attention and FiLM conditioning, improved estimation accuracy and achieved the best result among previous researches using DNNs and showed its robustness across the various domain of test data.",Hyon Kim,Hyon Kim (Universitat Pompeu Fabra)*; Xavier Serra (Universitat Pompeu Fabra ),"Kim, Hyon*; Serra, Xavier",MIR tasks -> music transcription and annotation,MIR fundamentals and methodology -> music signal processing; Musical features and properties -> expression and performative aspects of music,,,,,No,https://drive.google.com/file/d/1j-WrXG6w8826Xd_mmCnsdDVdCBcg_AjI/view?usp=drive_link,https://drive.google.com/file/d/1T_Ldu-OZ5QK6O68vFquQHNZWz_x0lzAd/view?usp=drive_link,https://drive.google.com/file/d/1Roqh2RqoMLQfxwRgH-Wtxocb58su2ODH/view?usp=drive_link,https://drive.google.com/file/d/1aOPRZRHwkysFf7uyhlBGLG2HTJhJeEMN/view?usp=drive_link,https://drive.google.com/file/d/1Ku_YxcNrKjJy7-IeAJiCpfu3bgyftHAF/view?usp=drive_link,https://ismir2024.slack.com/archives/C07USGDLYLU,p2-13-a-method-for
43,4,7,11,Remote,FALSE,"Lyrically Speaking: Exploring the Link Between Lyrical Emotions, Themes and Depression Risk","Lyrics play a crucial role in affecting and reinforcing emotional states by providing meaning and emotional connotations that interact with the acoustic properties of the music. Specific lyrical themes and emotions may intensify existing negative states in listeners and may lead to undesirable outcomes, especially in listeners with mood disorders such as depression. Hence, it is important for such individuals to be mindful of their listening strategies. In this study, we examine online music consumption of individuals at risk of depression in light of lyrical themes and emotions. Lyrics obtained from the listening histories of 541 Last.fm users, divided into At-Risk and No-Risk based on their mental well-being scores, were analyzed using natural language processing techniques. Statistical analyses of the results revealed that individuals at risk for depression prefer songs with lyrics associated with low valence and low arousal. Additionally, lyrics associated with themes of denial, self-reference, and ambivalence were preferred. In contrast, themes such as liberation, familiarity, and activity are not as favored. This study opens up the possibility of an approach to assessing depression risk from the digital footprint of individuals and potentially developing personalized recommendation systems.",Pavani B Chowdary,"Pavani B Chowdary (International Institute of Information Technology, Hyderabad)*; Bhavyajeet Singh (International Institute of Information Technology, Hyderabad ); Rajat Agarwal (International Institute of Information Technology); Vinoo  Alluri (IIIT - Hyderabad)","Chowdary, Pavani B*; Singh, Bhavyajeet; Agarwal, Rajat; Alluri, Vinoo",Human-centered MIR,"Applications -> music and health, well-being and therapy; Human-centered MIR -> user behavior analysis and mining, user modeling; Human-centered MIR -> user-centered evaluation; MIR fundamentals and methodology -> lyrics and other textual data; Musical features and properties -> musical affect, emotion and mood",,,,,No,https://drive.google.com/file/d/1h1h8IbGas1Eq65yRk8IoohDgh6ZVh67e/view?usp=drive_link,https://drive.google.com/file/d/1WqcSFrn16ZcX2us_cF3Y4JbxaWEfnJ5c/view?usp=drive_link,https://drive.google.com/file/d/1LokDJCDPPXaforbEr7DhcxuOGm3Pcg_U/view?usp=drive_link,https://drive.google.com/file/d/1CTC9XD66wpDs9bwasfXRg6nleMZu2zs8/view?usp=drive_link,https://drive.google.com/file/d/1tt0UZjsfgoHbJthJ4RcXXY6QIl3iB3Nw/view?usp=drive_link,https://ismir2024.slack.com/archives/C07V2MM4X7T,p7-11-lyrically-speaking-exploring
45,2,3,13,San Francisco,FALSE,Unsupervised Synthetic-to-Real Adaptation for Optical Music Recognition,"The field of Optical Music Recognition (OMR) focuses on models capable of reading music scores from document images. Despite its growing popularity, OMR is still confined to settings where the target scores are similar in both musical context and visual presentation to the data used for training the model. The common scenario, therefore, involves manually annotating data for each specific case, a process that is not only labor-intensive but also raises concerns regarding practicality. We present a methodology based on training a neural model with synthetic images, thus reducing the difficulty of obtaining labeled data. As sheet music renderings depict regular visual characteristics compared to scores from real collections, we propose an unsupervised neural adaptation approach consisting of loss functions that promote alignment between the features learned by the model and those of the target collection while preventing the model from converging to undesirable solutions. This unsupervised adaptation bypasses the need for extensive retraining, requiring only the unlabeled target images. Our experiments, focused on music written in Mensural notation, demonstrate that the methodology is successful and that synthetic-to-real adaptation is indeed a promising way to create practical OMR systems with little human effort.",Jorge Calvo-Zaragoza,Noelia N Luna-Barahona (Universidad de Alicante); Adrián Roselló (Universidad de Alicante); María Alfaro-Contreras (University of Alicante); David Rizo (University of Alicante. Instituto Superior de Enseñanzas Artísrticas de la Comunidad Valenciana); Jorge Calvo-Zaragoza (University of Alicante)*,"Luna-Barahona, Noelia N; Roselló, Adrián; Alfaro-Contreras, María; Rizo, David; Calvo-Zaragoza, Jorge*",MIR tasks -> optical music recognition,Applications -> digital libraries and archives,,,,,No,https://drive.google.com/file/d/1uAPEcIw9skchg5UQxmgCkLc1BsClOgFF/view?usp=drive_link,https://drive.google.com/file/d/17uyevRjXv8eJp77erGQKQP9Z9AzlI89B/view?usp=sharing,https://drive.google.com/file/d/1bzCCw1CM6f0GbA-yFkX4XjLxGClfb6P0/view?usp=drive_link,https://drive.google.com/file/d/1pS1if-khfQb_HZLoWqkpYsGqsWCTeT5J/view?usp=drive_link,https://drive.google.com/file/d/1QHmmdPl96ypiMm-3RIC6vmhRpUidarsO/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UM5T53C5,p3-13-unsupervised-synthetic-to
46,2,4,13,San Francisco,FALSE,Continual Learning for Music Classification,"Music classification is a prominent research area within Music Information Retrieval. While Deep Learning methods are capable of adequately performing this task, their classification space remains fixed once trained, which conflicts with the dynamic nature of the ever-evolving music landscape. This work explores, for the first time, the application of Continual Learning (CL) in the context of music classification. Specifically, we thoroughly evaluate five state-of-the-art CL approaches across four different music classification tasks. Additionally, we showcase that a foundation model might be the key to CL in music classification. For that, we study a new approach called Pre-trained Class Centers, which leverages pre-trained features to create dynamic class-center spaces. Our results reveal that existing CL methods struggle when applied to music classification tasks, whereas this simple method consistently outperforms them. This highlights the interest in CL methods tailored specifically for music classification.",Jorge Calvo-Zaragoza,Pedro González-Barrachina (University of Alicante); María Alfaro-Contreras (University of Alicante); Jorge Calvo-Zaragoza (University of Alicante)*,"González-Barrachina, Pedro; Alfaro-Contreras, María; Calvo-Zaragoza, Jorge*",MIR tasks -> automatic classification,MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics,,,,,No,https://drive.google.com/file/d/1C62NxDl4MWhctcXKlDNq2cBbsRMCLFYQ/view?usp=drive_link,https://drive.google.com/file/d/1j8PN64wnrXGHAQ8QN5rDLoRpqGPhoDNn/view?usp=sharing,https://drive.google.com/file/d/1UmdgpGdCSDy7rDeRGwyHYH_Qj4_ZiD_S/view?usp=drive_link,https://drive.google.com/file/d/1BUDLfutqRkVEJFy1z2culI2uv_G1lrYL/view?usp=drive_link,https://drive.google.com/file/d/1F2fO98SmziC2jRrmT7shJTDivddKSeCF/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UQ1D4V50,p4-13-continual-learning-for
48,1,2,18,San Francisco,FALSE,EFFICIENT ADAPTER TUNING FOR JOINT SINGING VOICE BEAT AND DOWNBEAT TRACKING WITH SELF-SUPERVISED LEARNING FEATURES,"Singing voice beat tracking is a challenging task, due to the lack of musical accompaniment that often contains robust rhythmic and harmonic patterns, something most existing beat tracking systems utilize and can be essential for estimating beats. In this paper, a novel temporal convolutional network-based beat-tracking approach featuring self-supervised learning (SSL) representations and adapter tuning is proposed to track the beat and downbeat of singing voices jointly. The SSL DistilHuBERT representations are utilized to capture the semantic information of singing voices and are further fused with the generic spectral features to facilitate beat estimation. Sources of variabilities that are particularly prominent with the non-homogeneous singing voice data are reduced by the efficient adapter tuning. Extensive experiments show that feature fusion and adapter tuning improve the performance individually, and the combination of both leads to significantly better performances than the un-adapted baseline system, with up to 31.6% and 42.4% absolute F1-score improvements on beat and downbeat tracking, respectively.",Jiajun Deng,Jiajun Deng (The Chinese University of HongKong)*; Yaolong Ju (Huawei); Jing Yang (Huawei 2012 Labs); Simon Lui (Huawei); Xunying Liu (The Chinese University of Hong Kong),"Deng, Jiajun*; Ju, Yaolong; Yang, Jing; Lui, Simon; Liu, Xunying","Musical features and properties -> rhythm, beat, tempo","Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; Musical features and properties -> timbre, instrumentation, and singing voice",,,,,No,https://drive.google.com/file/d/1CG6ZsCT7eqBT-FHt5XzVXU6JleGoe-FQ/view?usp=drive_link,https://drive.google.com/file/d/1_QEmBe1S2hLH83IqJlc_z89AlwAbSMjq/view?usp=drive_link,https://drive.google.com/file/d/1TJ7tKyht_8XhKfcKFU6X-yl5rINUj9Eo/view?usp=drive_link,https://drive.google.com/file/d/1qLg3J2L7_BQfBfCtWQe9vWm8s7pHWwcB/view?usp=drive_link,https://drive.google.com/file/d/1i2-guqyg0b9pywBcb44vVgYhTwdfbvBw/view?usp=drive_link,https://ismir2024.slack.com/archives/C07V2MM9281,p2-18-efficient-adapter-tuning
57,1,2,3,San Francisco,FALSE,The Changing Sound of Music: An Exploratory Corpus Study of Vocal Trends Over Time,"Recent advancements in audio processing provide a new opportunity to study musical trends using quantitative methods. While past work has investigated trends in music over time, there has been no large-scale study on the evolution of vocal lines. In this work, we conduct an exploratory study of 145,912 vocal tracks of popular songs spanning 55 years, from 1955 to 2010. We use source separation to extract the vocal stem and fundamental frequency (f0) estimation to analyze pitch tracks. Additionally, we extract pitch characteristics including  mean pitch, total variation, and pitch class entropy of each song. We conduct statistical analysis of vocal pitch across years and genres, and report significant trends in our metrics over time, as well as significant differences in trends between genres. Our study demonstrates the utility of this method for studying vocals, contributes to the understanding of vocal trends, and showcases the potential of quantitative approaches in musicology.",Elena Georgieva,Elena Georgieva (NYU)*; Pablo Ripollés (New York University); Brian McFee (New York University),"Georgieva, Elena*; Ripollés, Pablo; McFee, Brian",Computational musicology,"Computational musicology -> digital musicology; MIR fundamentals and methodology -> music signal processing; Musical features and properties -> timbre, instrumentation, and singing voice",,,,,No,https://drive.google.com/file/d/1xzg8GGGmGb1t9bAm9cqIDJcrH_tT6vOl/view?usp=sharing,https://drive.google.com/file/d/1SGdVso59OcRTJKrPKPv_X_segXmtNzcn/view?usp=drive_link,https://drive.google.com/file/d/1L3qtZakVzi-56K2d_mwi1J6wJ2loBEaF/view?usp=drive_link,https://drive.google.com/file/d/1BpZHxeCiXiUja41F8lU9_CH3nz4_yUze/view?usp=drive_link,https://drive.google.com/file/d/10CrF7IRwtC0Tv5NuA6idadfAeJppOaW1/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UM5T97PX,p2-03-the-changing-sound
60,2,3,9,San Francisco,FALSE,Composer's Assistant 2: Interactive Multi-Track MIDI Infilling with Fine-Grained User Control,"We introduce Composer's Assistant 2, a system for interactive human-computer composition in the REAPER digital audio workstation. Our work upgrades the Composer's Assistant system (which performs multi-track infilling of symbolic music at the track-measure level) with a wide range of new controls to give users fine-grained control over the system's outputs. Controls introduced in this work include two types of rhythmic conditioning controls, horizontal and vertical note onset density controls, several types of pitch controls, and a rhythmic interest control. We train a T5-like transformer model to implement these controls and to serve as the backbone of our system. With these controls, we achieve a dramatic improvement in objective metrics over the original system. We also study how well our model understands the meaning of our controls, and we conduct a listening study that does not find a significant difference between real music and music composed in a co-creative fashion with our system. We release our complete system, consisting of source code, pretrained models, and REAPER scripts.",Martin E Malandro,Martin E Malandro (Sam Houston State University)*,"Malandro, Martin E*",Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,Creativity -> human-ai co-creativity; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music generation,,,,,No,https://drive.google.com/file/d/1tjF6ljH2xbNTv6enbQfeU4UecpR-C3vH/view?usp=drive_link,https://drive.google.com/file/d/1PDuCcmOrjiQU0Gxzm0JIy3C6jCases84/view?usp=drive_link,https://drive.google.com/file/d/1yI-Ig00wI16BRqVhS7cqLuLwPHqXoiNy/view?usp=drive_link,https://drive.google.com/file/d/1GIhTiaj-zJipZdmgeIm2H8uoBPfU77vZ/view?usp=drive_link,https://drive.google.com/file/d/1B7pg2jXEmjdu2bM3maW1y0USTis80DvU/view?usp=drive_link,https://ismir2024.slack.com/archives/C07USGDV12Q,p3-09-composer-s-assistant
61,3,5,13,San Francisco,FALSE,Content-based Controls for Music Large-scale Language Modeling,"Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music \textit{indirectly} through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). \textit{We aim to further equip the models with direct and \textbf{content-based} controls on innate music languages} such as pitch, chords and drum track. To this end, we contribute \textit{Coco-Mulla}, a \textbf{co}ntent-based \textbf{co}ntrol method for \textbf{mu}sic \textbf{l}arge \textbf{la}nguage modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieves high-quality music generation with \textbf{low-resource} semi-supervised learning. We fine-tune the model with less than 4$\%$ of the orignal parameters on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls. We illustrate its controllability via chord and rhythm conditions, two of the most salient features of pop music. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and arrangement. Our source codes and demos are available online\footnote{\url{https://github.com/Kikyo-16/coco-mulla-repo}.}\footnote{\url{https://kikyo-16.github.io/coco-mulla/}.}.",Liwei Lin,Liwei Lin (New York University Shanghai)*; Gus Xia (New York University Shanghai); Junyan Jiang (New York University Shanghai); Yixiao Zhang (Queen Mary University of London),"Lin, Liwei*; Xia, Gus; Jiang, Junyan; Zhang, Yixiao",MIR tasks -> music generation,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> music synthesis and transformation,,,,,No,https://drive.google.com/file/d/1HutKNTlmekAao1OWMxx7J-4HFwD3EA0K/view?usp=drive_link,https://drive.google.com/file/d/1s9CKLuAQRybFBY1b20UyGI6-JLmKv9Lw/view?usp=drive_link,https://drive.google.com/file/d/1g1WMrdURcqIJpRyjJrpz4kpifCg57BZl/view?usp=drive_link,https://drive.google.com/file/d/1G0Z008KDxw7aYe1eUq7Z1N53egh15jZT/view?usp=drive_link,https://docs.google.com/presentation/d/1BU6k9q5eFC2XX8KsaQGRATT77YlY2-CwD3-KQzhnOmI/edit#slide=id.g30169ebf144_1_375,https://ismir2024.slack.com/archives/C07UHCN1N8N,p5-13-content-based-controls
65,1,2,7,San Francisco,FALSE,Joint Audio and Symbolic Audio Conditioning For Temporally Controlled Text-to-Music Generation,"We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method that allows for both locally (e.g., chords) and globally (text description) controlled music generation. Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This al- lows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence using objective metrics and human studies. Results suggest that JASCO is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page: https://pages.https://pages.cs.huji.ac.il/adiyoss-lab/JASCO",Or Tal,Or Tal (The Hebrew University of Jerusalem)*; Alon Ziv (The Hebrew University of Jerusalem); Felix Kreuk (Bar-Ilan University); Itai Gat (Meta); Yossi Adi (The Hebrew University of Jerusalem),"Tal, Or*; Ziv, Alon; Kreuk, Felix; Gat, Itai; Adi, Yossi",Generative Tasks -> music and audio synthesis,MIR and machine learning for musical acoustics -> applications of musical acoustics to signal synthesis; MIR tasks -> music generation,,,,,No,https://drive.google.com/file/d/1xXUD7VbtkvkfnQXtj81R-7ICInSEFIEB/view?usp=drive_link,https://drive.google.com/video/captions/edit?id=1-5sdVfvISw1c21AGYwr4wjQ3ke2__1VG,https://drive.google.com/file/d/1kRm2KDVPxnlLMlVFBfI1KvyXlePrLr19/view?usp=drive_link,https://drive.google.com/file/d/17bUEpA6WnKZn6bcPrHmY82DqvQ8_VXrd/view?usp=drive_link,https://docs.google.com/presentation/d/1-cpUayRz1aCuMc02gvqsuo83wKJ3mU16/edit?usp=drive_web&ouid=102045217384450827259&rtpof=true,https://ismir2024.slack.com/archives/C07UQ1DBBS6,p2-07-joint-audio-and
66,3,6,13,San Francisco,FALSE,Towards Universal Optical Music Recognition: A Case Study on Notation Types,"Recent advances in Deep Learning have propelled the development of fields such as Optical Music Recognition (OMR), which is responsible for extracting the content from music score images. Despite progress in the field, existing literature scarcely addresses core issues like performance in real-world scenarios, user experience, maintainability of multiple pipelines, reusability of architectures and data, among others. These factors result in high costs for both users and developers of such systems. Furthermore, research has often been conducted under certain constraints, such as using a single musical texture or type of notation, which may not align with the end-user requirements of OMR systems. For the first time, our study involves a comprehensive and extensive experimental setup to explore new ideas towards the development of a universal OMR system---capable of transcribing all textures and notation types. Our investigation provides valuable insights into several aspects, such as the ability of a model to leverage knowledge from different domains despite significant differences in music notation types.",Juan Carlos Martinez-Sevilla,Juan Carlos Martinez-Sevilla (University of Alicante)*; David Rizo (University of Alicante. Instituto Superior de Enseñanzas Artísrticas de la Comunidad Valenciana); Jorge Calvo-Zaragoza (University of Alicante),"Martinez-Sevilla, Juan Carlos*; Rizo, David; Calvo-Zaragoza, Jorge",MIR tasks -> optical music recognition,Applications -> digital libraries and archives; Applications -> music heritage and sustainability; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> music transcription and annotation,,,,,No,https://drive.google.com/file/d/1msfdNh_Jg391bVoLqiGLcqreH9QfuOoL/view?usp=drive_link,https://drive.google.com/file/d/1-ry2zGPZyw5DdrpHfa-gpeXQEQAfWVh-/view?usp=drive_link,https://drive.google.com/file/d/1N_O2bUMsp4wYxxdc16ZrkYTWUqsY4F4K/view?usp=drive_link,https://drive.google.com/file/d/1z2XhRwBCux5aiPatuuh-1UxL5-EAyGT8/view?usp=drive_link,https://drive.google.com/file/d/1caXgwkuWp5Q2pJ2Q0rJb1DvoORCFUzFf/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UHCN4FQE,p6-13-towards-universal-optical
68,4,7,13,Remote,FALSE,In-depth performance analysis of the ADTOF-based algorithm for automatic drum transcription,"The importance of automatic drum transcription lies in the potential to extract useful information from a musical track; however, the low reliability of the models for this task represents a limiting factor. Indeed, even though in the recent literature the quality of the generated transcription has improved thanks to the curation of large training datasets via crowdsourcing, there is still a large margin of improvement for this task to be considered solved. Aiming to steer the development of future models, we identify the most common errors from training and testing on the aforementioned crowdsourced datasets. We perform this study in three steps: First, we detail the quality of the transcription for each class of interest; second, we employ a new metric and a pseudo confusion matrix to quantify different mistakes in the estimations; last, we compute the agreement between different annotators of the same track to estimate the accuracy of the ground-truth. Our findings are twofold: On the one hand, we observe that the previously reported issue that less represented instruments (e.g., toms) are less reliably transcribed is mostly solved now. On the other hand, cymbal instruments have unprecedented relative low performance. We provide intuitive explanations as to why cymbal instruments are difficult to transcribe and we identify that they represent the main source of disagreement among annotators.",Mickael Zehren,Mickael Zehren (Umeå University)*,"Zehren, Mickael*",MIR tasks -> music transcription and annotation,"Evaluation, datasets, and reproducibility; Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> evaluation metrics",,,,,No,https://drive.google.com/file/d/1gEOvHDCXQgJj7FRjub0iJSYmgjmzMLSo/view?usp=drive_link,https://drive.google.com/file/d/1MZQ9UD99OaWpbzK6z2m1h28hCphyEh3U/view?usp=sharing,https://drive.google.com/file/d/1Npf1inFFhujGu5KnDzRcUL7plPCoP3AT/view?usp=drive_link,https://drive.google.com/file/d/1mRt3ZrQqeDvZexbXDGXITLTB_vTM9ok0/view?usp=drive_link,https://drive.google.com/file/d/1dcuhK2-Ilmm-r5sqtTMUGiMHdfwgbQYU/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UQ1DE84S,p7-13-in-depth-performance
72,1,1,14,San Francisco,FALSE,"Variation Transformer: New datasets, models, and comparative evaluation for symbolic music variation generation","Variation in music is defined as repetition of a theme, but with various modifications, playing an important role in many musical genres in developing core music ideas into longer passages. Existing research on variation in music is mostly confined to datasets consisting of classical theme-and-variation pieces, and generative models limited to melody-only representations. In this paper, to address the problem of the lack of datasets, we propose an algorithm to extract theme-and-variation pairs automatically, and use it to annotate two datasets called POP909-TVar (2,871 theme-and-variation pairs) and VGMIDI-TVar (7,830 theme-and-variation pairs). We propose both non-deep learning and deep learning based symbolic music variation generation models, and report the results of a listening study and feature-based evaluation for these models. One of our two newly proposed models, called Variation Transformer, outperforms all other models that listeners evaluated for ""variation success"", including non-deep learning and deep learning based approaches. An implication of this work for the wider field of music making is that we now have a model that can generate material with stronger and perceivably more successful relationships to some given prompt or theme.",Chenyu Gao,"Chenyu Gao (University of York)*; Federico Reuben (University of York); Tom Collins (University of York; MAIA, Inc.)","Gao, Chenyu*; Reuben, Federico; Collins, Tom",MIR tasks -> music generation,"Evaluation, datasets, and reproducibility -> novel datasets and use cases; Generative Tasks -> artistically-inspired generative tasks ; Generative Tasks -> evaluation metrics",,,,,No,https://drive.google.com/file/d/1N4ey26D4sZQphIzi74uNjr9fQNtuBCSx/view?usp=drive_link,https://drive.google.com/file/d/14-NxP7Q7c5tEO4UaKWGkEhVLwtTjgT0E/view?usp=drive_link,https://drive.google.com/file/d/1vPlg5tf9-nwKr5mi20Xp1qt44iIR6ZDO/view?usp=drive_link,https://drive.google.com/file/d/1ZtDDsSuSB4YVng0Vj5L8X30Pa4TYFeIA/view?usp=drive_link,https://drive.google.com/file/d/19XvMksJQjfcWwI5s5kzKvRy6Wz8LYasF/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UHCN7K5L,p1-14-variation-transformer-new
77,1,2,4,San Francisco,FALSE,Music Proofreading with RefinPaint: Where and How to Modify Compositions given Context,"Autoregressive generative transformers are key in music generation, producing coherent compositions but facing challenges in human-machine collaboration. We propose RefinPaint, an iterative technique that improves the sampling process. It does this by identifying the weaker music elements using a feedback model, which then informs the choices for resampling by an inpainting model. This dual-focus methodology not only facilitates the machine's ability to improve its automatic inpainting generation through repeated cycles but also offers a valuable tool for humans seeking to refine their compositions with automatic proofreading.  Experimental results suggest RefinPaint's effectiveness in inpainting and proofreading tasks, demonstrating its value for refining music created by both machines and humans. This approach not only facilitates creativity but also aids amateur composers in improving their work.",Pedro Ramoneda,Pedro Ramoneda (Universitat Pompeu Fabra)*; Martín Rocamora (Universidad de la República); Taketo Akama (Sony CSL),"Ramoneda, Pedro*; Rocamora, Martín; Akama, Taketo",Creativity -> human-ai co-creativity,Creativity; Creativity -> creativity and learning; Generative Tasks; Generative Tasks -> interactions; MIR tasks -> music generation,,,,,No,https://drive.google.com/file/d/1ia2OxA9wgadP1SiWWYN7clsiaz9kJbmA/view?usp=drive_link,video_77_ramoneda.mp4,https://drive.google.com/file/d/1VGJLTy1natiGB5LEP1zOBPUmO6oVdVFm/view?usp=drive_link,https://drive.google.com/file/d/1MMgr2dhkmNjdfdcLItJes-4fwbYPNM3a/view?usp=drive_link,https://docs.google.com/presentation/d/1iYX8GBypuG0bKPD_Llf-D7Jf7t7bladrtw3YgKG7kHM/edit?usp=sharing,https://ismir2024.slack.com/archives/C07USGE89T6,p2-04-music-proofreading-with
78,2,4,3,San Francisco,FALSE,Towards Explainable and Interpretable Musical Difficulty Estimation: A Parameter-efficient Approach,"Estimating music piece difficulty is important for organizing educational music collections. This process could be partially automatized to facilitate the educators role. Nevertheless, the decisions performed by prevalent deep-learning models are hardly understandable, which may impair the acceptance of such a technology in music education curricula. Our work employs explainable descriptors for difficulty estimation in symbolic music representations. Furthermore, through a novel parameter-efficient white-box model, we outperform previous efforts while delivering interpretable results. These comprehensible outcomes emulate the functionality of a rubric, a tool widely used in music education. Our approach, evaluated in piano repertoire, achieved 41.4% accuracy independently, with a mean squared error (MSE) of 1.7, showing precise difficulty estimation. This work illustrates how building on top of past research can  offer alternatives for music difficulty assessment which are explainable and interpretable. With this, we aim to promote a more effective communication between the MIR community and the music education one.",Pedro Ramoneda,Pedro Ramoneda (Universitat Pompeu Fabra)*; Vsevolod E Eremenko (Music Technology Group at Universitat Pompeu Fabra); Alexandre D'Hooge (Université de Lille); Emilia Parada-Cabaleiro (Nuremberg University of Music); Xavier Serra (Universitat Pompeu Fabra ),"Ramoneda, Pedro*; Eremenko, Vsevolod E; D'Hooge, Alexandre; Parada-Cabaleiro, Emilia; Serra, Xavier",Computational musicology,Applications -> music training and education,,,,,No,https://drive.google.com/file/d/1hlcxltpSxXHhMIYDO3tOYRB8-4MknL8b/view?usp=drive_link,https://drive.google.com/file/d/1MZQ9UD99OaWpbzK6z2m1h28hCphyEh3U/view?usp=sharing,https://drive.google.com/file/d/165PCbniHB28GHqVK2Lu6IxOe6a6wZAIM/view?usp=drive_link,https://drive.google.com/file/d/1Vir2OlbagGVMM8WMiUtdpEGM1RzC5-UD/view?usp=drive_link,https://docs.google.com/presentation/d/1tTKwWmfqflU1eY9_PGDGkQB2ugT9ErrC/edit#slide=id.g30b26252a7d_0_73,https://ismir2024.slack.com/archives/C07U9EZSR4P,p4-03-towards-explainable-and
79,2,3,4,San Francisco,FALSE,Human-AI Music Process: A Dataset of AI-Supported Songwriting Processes from the AI Song Contest,"The advent of accessible artificial intelligence (AI) tools and systems has begun a new era for creative expression, challenging us to gain a better understanding of human-AI collaboration and creativity. In this paper, we introduce Human-AI Songwriting Processes Dataset (HAISP), consisting of 34 coded submissions from the 2023 AI Song Contest teams. This dataset offers a resource for exploring the complex dynamics of AI-supported songwriting processes, facilitating investigations into the possibilities and challenges posed by AI in creative endeavors. Overall, HAISP contributes to advancing understanding of human-AI co-creation from the users' perspective. Furthermore, we outline potential use cases for the dataset, ranging from analyzing AI tools utilized in songwriting to gaining insights into users' ethical considerations and expanding creative possibilities. This can help to inform both scholarly inquiry and practical applications in music composition and beyond.",Lidia J Morris,Lidia J Morris (University of Washington)*; Rebecca Leger (Fraunhofer IIS); Michele Newman (University of Washington); John Ashley Burgoyne (University of Amsterdam); Ryan Groves (Self-employed); Natasha Mangal (CISAC); Jin Ha Lee (University of Washington),"Morris, Lidia J*; Leger, Rebecca; Newman, Michele; Burgoyne, John Ashley; Groves, Ryan; Mangal, Natasha; Lee, Jin Ha",Creativity -> human-ai co-creativity,"Creativity -> computational creativity; Creativity -> creative practice involving MIR or generative technology ; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Generative Tasks -> music and audio synthesis; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",,,,,No,https://drive.google.com/file/d/1VjFIWqMwUxnsBKdPsG3jok97RngcMBSQ/view?usp=drive_link,https://drive.google.com/file/d/1Ple4x4zVPxCMeHRgnjA-WEE5OQt-epk3/view?usp=drive_link,https://drive.google.com/file/d/1GHUgD19TlzgmMjZSe60cgR2uMBZqGWqv/view?usp=drive_link,https://drive.google.com/file/d/1G7LXNeVNfho8g_TVWUYUFRe93En4BY-P/view?usp=drive_link,https://drive.google.com/file/d/1szcAXxXT5hLkybVhCS1F2XEH_6lc0VVa/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UHCNBHFG,p3-04-human-ai-music
81,1,2,20,San Francisco,FALSE,Exploring Musical Roots: Applying Audio Embeddings to Empower Influence Attribution for a Generative Music Model,"Every artist has a creative process that draws inspiration from previous artists and their works. Today, ""inspiration"" has been automated by generative music models. The black box nature of these models obscures the identity of the works that influence their creative output. As a result, users may inadvertently appropriate or copy existing artists' works. We establish a replicable methodology to systematically identify similar pieces of music audio in a manner that is useful for understanding training data attribution. We compare the effect of applying CLMR and CLAP embeddings to similarity measurement in a set of 5 million audio clips used to train VampNet, a recent open source generative music model. We validate this approach with a human listening study. We also explore the effect that modifications of an audio example (e.g., pitch shifting) have on similarity measurements. This work is foundational to incorporating automated influence attribution into generative modeling, which promises to let model creators and users move from ignorant appropriation to informed creation. Audio samples accompanying this paper are available at tinyurl.com/exploring-musical-roots.",Julia Barnett,Julia Barnett (Northwestern University)*; Bryan Pardo (Northwestern University); Hugo Flores García (Northwestern University),"Barnett, Julia*; Pardo, Bryan; Flores García, Hugo",Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies,MIR tasks -> similarity metrics,,,,,No,https://drive.google.com/file/d/1CrDjx5hn-KAF72RcynMYQUtVrP--ZzxX/view?usp=drive_link,https://drive.google.com/file/d/1enH3tfkgI9otV5uK-0IuExK_xaIaybXM/view?usp=drive_link,https://drive.google.com/file/d/10FYn52ikEsrMZPj4oNGGvr7_wp95L-vM/view?usp=drive_link,https://drive.google.com/file/d/1kMAvAAFtGGHfOLJq58b734Yk463Ykcnh/view?usp=drive_link,https://docs.google.com/presentation/d/1kt-xNnwNW1cIbn2DtJYhcg3ns7yQLiLv/edit?usp=drive_link&ouid=102045217384450827259&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07VCQXSCNL,p2-20-exploring-musical-roots
83,2,4,4,San Francisco,FALSE,Purposeful Play: Evaluation and Co-Design of Casual Music Creation Applications with Children,"The rise of digital technologies has increased interest in democratizing music creation, but current creativity support tools often prioritize literacy and education over meeting children's needs for casual creation. To address this, we conducted Participatory Design sessions with children aged 6-13 to explore their perceptions of casual music creation activities and identify elements of creative applications that support different expressions. Our study aimed to answer two key questions: (1) How do children perceive casual music creation activities and which elements of creative applications facilitate expression? and (2) What insights can inform the design of future casual music creation tools? Our findings indicate that children view casual music creation as involving diverse activities, with visuals aiding in understanding sounds, and engaging in various playful interactions leading to creative experiences. We present design implications based on our findings and introduce casual creation as ""purposeful play"". Furthermore, we discuss its implications for creative MIR.",Michele Newman,Michele Newman (University of Washington)*; Lidia Morris (University of Washington); Jun Kato (National Institute of Advanced Industrial Science and Technology (AIST)); Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST)); Jason Yip (University of Washington); Jin Ha Lee (University of Washington),"Newman, Michele*; Morris, Lidia J; Kato, Jun; Goto, Masataka; Yip, Jason; Lee, Jin Ha",Creativity -> creativity and learning,Creativity -> creative practice involving MIR or generative technology ; Human-centered MIR -> human-computer interaction; Human-centered MIR -> music interfaces and services; Human-centered MIR -> user-centered evaluation,,,,,No,https://drive.google.com/file/d/1Iq4_tkuSSKZdUphnPm-8DtyKE4jFWk2N/view?usp=share_link,https://drive.google.com/file/d/1mP85ID1OC1_QFZreMCh-HxUQOwbNZLQp/view?usp=share_link,https://drive.google.com/file/d/16URMaG7w64HLOG6njAm9UEc1GseMVYX1/view?usp=share_link,https://drive.google.com/file/d/11W7-Grx5vqLGGNfNTkRSlmack3-92kQj/view?usp=share_link,https://drive.google.com/file/d/1qRpPoIdnznWjxZJc28qCpQV89UK5od04/view?usp=share_link,https://ismir2024.slack.com/archives/C07UPU8LG4B,p4-04-purposeful-play-evaluation
84,2,4,9,San Francisco,FALSE,Semi-Supervised Contrastive Learning of Musical Representations,"Despite the success of contrastive learning in Music Information Retrieval, the inherent ambiguity of contrastive self-supervision presents a challenge. Relying solely on augmentation chains and self-supervised positive sampling strategies may lead to a pretraining objective that does not capture key musical information for downstream tasks. We introduce semi-supervised contrastive learning (SemiSupCon), an architecturally simple method for leveraging musically informed supervision signals in the contrastive learning of musical representations. Our approach introduces musically-relevant supervision signals into self-supervised contrastive learning by combining supervised and self-supervised contrastive objectives in a simple framework compared to previous work. This framework improves downstream performance and robustness to audio corruptions on a range of downstream MIR tasks with moderate amounts of labeled data. Our approach enables shaping the learned similarity metric through the choice of labeled data which (1) infuses the representations with musical domain knowledge and (2) improves out-of-domain performance with minimal general downstream performance loss. We show strong transfer learning performance on musically related yet not trivially similar tasks - such as pitch and key estimation. Additionally, our approach shows performance improvement on automatic tagging over self-supervised approaches with only 5% of available labels included in pretraining.",Julien PM Guinot,Julien Guinot (Queen Mary University of London)*; Elio Quinton (Universal Music Group); György Fazekas (QMUL),"Guinot, Julien PM*; Quinton, Elio; Fazekas, George",Knowledge-driven approaches to MIR,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> automatic classification; MIR tasks -> similarity metrics,,,,,No,https://drive.google.com/file/d/1ECakxR9sVf_ZHnob-6l6JbKdrWVgi38g/view?usp=sharing,https://drive.google.com/file/d/19iL-BCJj3nzFS7B-iUXkv0H0J5Fp6N-M/view?usp=sharing,https://drive.google.com/file/d/1X1wQwqw57B8YJmkW-9TNJlcAkbnwneti/view?usp=share_link,https://drive.google.com/file/d/1SEF_iws0abIu6oXPeRGG0DcJNLTtro00/view?usp=share_link,https://drive.google.com/file/d/1bujpntJiewF_aSBUUZLEYt_Qs4Kf43jh/view?usp=share_link,https://ismir2024.slack.com/archives/C07UPU8MTB5,p4-09-semi-supervised-contrastive
86,1,1,4,Remote,FALSE,Harmonic and Transposition Constraints Arising from the Use of the Roland TR-808 Bass Drum,"The study investigates hip-hop music producer Scott Storch's approach to tonality, where the song's key is transposed to fit the Roland TR-808 bass drum instead of tuning the drums to the song’s key. This process, involving the adjustment of all tracks except the bass drum, suggests significant production motives. The primary constraint stems from the limited usable pitch range of the TR-808 bass drum if its characteristic sound is to be preserved. The research examines drum tuning practices, the role of the Roland TR-808 in music, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples reveals their spectral characteristics and their integration into modern genres like trap and hip-hop. The study also considers the impact of loudspeaker frequency response and human ear sensitivity on bass drum perception. The findings suggest that Storch’s method prioritizes the spectral properties of the bass drum over traditional pitch values, enhancing bass response and overall sound quality. The need to maintain the unique sound of the TR-808 bass drum underscores the importance of spectral formants and register in contemporary music production.",Emmanuel Deruty,Emmanuel Deruty (Sony Computer Science Laboratories)*,"Deruty, Emmanuel*",Computational musicology,"Applications -> music composition, performance, and production; Computational musicology -> digital musicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; Musical features and properties -> musical style and genre",,,,,No,https://drive.google.com/file/d/13eZzr4F6FtWq7Kb7YCHicXXkl4N3zA8R/view?usp=share_link,https://drive.google.com/file/d/1-7SRb1F67xRtNMhbnUuc06tTkUX2YJ8N/view?usp=share_link,https://drive.google.com/file/d/1O2570jxaug06VoO0xsDhykKHpqJiMlV4/view?usp=share_link,https://drive.google.com/file/d/1qualw556L5tVvL-2lA3CeS-tWGvHDswT/view?usp=share_link,https://drive.google.com/file/d/182IJrfhDQeOjnudgqPFEV672n0sYgkEh/view?usp=share_link,https://ismir2024.slack.com/archives/C07VCQY1D96,p1-04-harmonic-and-transposition
89,2,3,17,San Francisco,FALSE,End-to-end automatic singing skill evaluation using cross-attention and data augmentation for solo singing and singing with accompaniment,"Automatic singing skill evaluation (ASSE) systems are predominantly designed for solo singing, and the scenario of singing with accompaniment is largely unaddressed. In this paper, we propose an end-to-end ASSE system that effectively processes both solo singing and singing with accompaniment using data augmentation, where a comparative study is conducted on four different data augmentation approaches. Additionally, we incorporate bi-directional cross-attention (BiCA) for feature fusion which, compared to simple concatenation, can better exploit the inter-relationships between different features. Results on the 10KSinging dataset show that data augmentation and BiCA boost performance individually. When combined, they contribute to further significant improvements, with a Pearson correlation coefficient of 0.769 for solo singing and 0.709 for singing with accompaniment. This represents relative improvements of 36.8% and 26.2% compared to the baseline model score of 0.562, respectively.",Yaolong Ju,Yaolong Ju (Huawei)*; Chun Yat Wu (Huawei); Betty Cortiñas Lorenzo  (Huawei); Jing Yang (Huawei 2012 Labs); Jiajun Deng (Huawei); Fan Fan (Huawei); Simon Lui (Huawei),"Ju, Yaolong*; Wu, Chun Yat; Lorenzo , Betty Cortinas; Yang, Jing; Deng, Jiajun; FAN, FAN; Lui, Simon","Musical features and properties -> timbre, instrumentation, and singing voice","Applications -> music composition, performance, and production; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; Musical features and properties -> expression and performative aspects of music",,,,,No,https://drive.google.com/file/d/1iTqz4z5OY5AaRH_0QhYaPILXT5iSZNle/view?usp=share_link,,,,,https://ismir2024.slack.com/archives/C07U9F05UDD,p3-17-end-to-end
90,2,4,19,San Francisco,FALSE,MelodyT5: A Unified Score-to-Score Transformer for Symbolic Music Processing,"In the domain of symbolic music research, the progress of developing scalable systems has been notably hindered by the scarcity of available training data and the demand for models tailored to specific tasks. To address these issues, we propose MelodyT5, a novel unified framework that leverages an encoder-decoder architecture tailored for symbolic music processing in ABC notation. This framework challenges the conventional task-specific approach, considering various symbolic music tasks as score-to-score transformations. Consequently, it integrates seven melody-centric tasks, from generation to harmonization and segmentation, within a single model. Pre-trained on MelodyHub, a newly curated collection featuring over 261K unique melodies encoded in ABC notation and encompassing more than one million task instances, MelodyT5 demonstrates superior performance in symbolic music processing via multi-task transfer learning. Our findings highlight the efficacy of multi-task transfer learning in symbolic music processing, particularly for data-scarce tasks, challenging the prevailing task-specific paradigms and offering a comprehensive dataset and framework for future explorations in this domain.",Maosong Sun,Shangda Wu (Central Conservatory of Music); Yashan Wang (Central Conservatory of Music); Xiaobing Li (Central Conservatory of Music); Feng Yu (Central Conservatory of Music); Maosong Sun (Tsinghua University)*,"Wu, Shangda; Wang, Yashan; Li, Xiaobing; Yu, Feng; Sun, Maosong*",MIR fundamentals and methodology -> symbolic music processing,"Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> music generation; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> melody and motives; Musical features and properties -> structure, segmentation, and form",,,,,No,https://drive.google.com/file/d/1I72iajjKOPsQMmee8CDFiNCSIkQdArvP/view?usp=share_link,https://drive.google.com/file/d/1GaJwHFJhCRNmFLeZGYLMhi9EKvnr7JV6/view?usp=share_link,https://drive.google.com/file/d/1jZrxMhcXPZU7iMAQID3nf_BI246t2GNs/view?usp=share_link,https://drive.google.com/file/d/1KZHABImBHV5T-_TK4OEVb1glgv-Xpbs5/view?usp=share_link,https://drive.google.com/file/d/1aksTuwefarTg6uroZCse-gSPK7_vHCu1/view?usp=share_link,https://ismir2024.slack.com/archives/C07UM5U0SCV,p4-19-melodyt5-a-unified
91,1,2,14,San Francisco,FALSE,MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation,"Existing text-to-music models can produce high-quality audio with great diversity. However, textual prompts alone cannot precisely control temporal musical features such as chords and rhythm of the generated music. To address this challenge, we introduce MusiConGen, a temporally-conditioned Transformer-based text-to-music  model that builds upon the pretrained MusicGen framework. Our innovation lies in an efficient finetuning mechanism, tailored for consumer-grade GPUs, that integrates automatically-extracted chords and rhythm features as the control signal. During inference, the control can either be musical features extracted from a reference audio signal, or be user-defined symbolic chord sequence, BPM, and textual prompts. Our performance evaluation on two datasets---one derived from extracted features and the other from user-created inputs---demonstrates that MusiConGen can generate realistic music that aligns well with the specified temporal control. Sound examples can be found at the supplementary material and the anonymous demo page, \url{https://musicongen.github.io/musicongen_demo/}.",Yun-Han Lan,Yun-Han Lan (Taiwan AI Labs)*; Wen-Yi Hsiao (Taiwan AI Labs); Hao-Chung Cheng (National Taiwan University); Yi-Hsuan Yang (National Taiwan University),"Lan, Yun-Han*; Hsiao, Wen-Yi; Cheng, Hao-Chung; Yang, Yi-Hsuan",MIR tasks -> music generation,"Generative Tasks; MIR fundamentals and methodology -> music signal processing; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> rhythm, beat, tempo",,,,,No,https://drive.google.com/file/d/11kDKpVYCpSs9YI2UGG9QecHPLjwvv2pv/view?usp=share_link,https://drive.google.com/file/d/1vXa5hVkLjiP-AsDcsQmkokGJekFP-6gd/view?usp=share_link,https://drive.google.com/file/d/1uZgfvu9DHuCKXcIOYjnMWWVEwHWVV4Rb/view?usp=share_link,https://drive.google.com/file/d/1RzIDsBLRZwLAWZo6aQsL_5mEw6IdL-B2/view?usp=share_link,https://drive.google.com/file/d/14V0q_J8JYseA8DoNRtNGkekBEWzWMIot/view?usp=share_link,https://ismir2024.slack.com/archives/C07UM5U2FGV,p2-14-musicongen-rhythm-and
96,2,3,7,San Francisco,FALSE,SpecMaskGIT: Masked Generative Modelling of Audio Spectrogram for Efficient Audio Synthesis and Beyond,"Recent advances in generative models that iteratively synthesize audio clips sparked great success in text-to-audio synthesis (TTA), but at the cost of slow synthesis speed and heavy computation. Although there have been attempts to accelerate the iterative procedure, high-quality TTA systems remain inefficient due to the hundreds of iterations required in the inference phase and large amount of model parameters. To address these challenges, we propose SpecMaskGIT, a light-weight, efficient yet effective TTA model based on the masked generative modeling of spectrograms. First, SpecMaskGIT synthesizes a realistic 10 s audio clip in less than 16 iterations, an order of magnitude less than previous iterative TTA methods. As a discrete model, SpecMaskGIT outperforms larger VQ-Diffusion and auto-regressive models in a TTA benchmark, while being real-time with only 4 CPU cores or even 30× faster with a GPU. Next, built upon a latent space of Mel-spectrograms, SpecMaskGIT has a wider range of applications (e.g., zero-shot bandwidth extension) than similar methods built on latent wave domains. Moreover, we interpret SpecMaskGIT as a generative extension to previous discriminative audio masked Transformers, and shed light on its audio representation learning potential. We hope that our work will inspire the exploration of masked audio modeling toward further diverse scenarios.",Zhi Zhong,Marco Comunità (Queen Mary University of London); Zhi Zhong (Sony Group Corporation)*; Akira Takahashi (Sony Group Corporation); Shiqi Yang (Sony); Mengjie Zhao (Sony Group Corporation); Koichi Saito (Sony Gruop Corporation); Yukara Ikemiya (Sony Research); Takashi Shibuya (Sony AI); Shusuke Takahashi (Sony Group Corporation); Yuki Mitsufuji (Sony AI),"Comunita, Marco; Zhong, Zhi*; Takahashi, Akira; Yang, Shiqi; Zhao, Mengjie; Saito, Koichi; Ikemiya, Yukara; Shibuya, Takashi; Takahashi, Shusuke; Mitsufuji, Yuki",Generative Tasks -> music and audio synthesis,Generative Tasks -> interactions; Generative Tasks -> real-time considerations; Generative Tasks -> transformations; MIR tasks -> automatic classification; Musical features and properties -> representations of music,,,,,No,https://drive.google.com/file/d/1Vinmy5AVoQSPJOvA-qYFu32kPhx8_r8x/view?usp=sharing,https://drive.google.com/file/d/1GvfnJaoT73dkCHy0CsV5WcrTSjfomUFe/view?usp=sharing,https://drive.google.com/file/d/1YZkyRX_07zw72sOvTA9eoaEjcDtPfVIm/view?usp=sharing,https://drive.google.com/file/d/1h34o3dFNby363faQnn4IYC8mFwPWu4fc/view?usp=sharing,https://docs.google.com/presentation/d/1gS6pbZypIAgkwD1-Je0v0T4Ecr3eSHGl/edit?usp=share_link&ouid=110993997319166704578&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07U9F0AJ6T,p3-07-specmaskgit-masked-generative
101,2,3,14,San Francisco,FALSE,MMT-BERT: Chord-aware Symbolic Music Generation Based on Multitrack Music Transformer and MusicBERT,"We propose a novel symbolic music representation and Generative Adversarial Network (GAN) framework specially designed for symbolic multitrack music generation. The main theme of symbolic music generation primarily encompasses the preprocessing of raw audio data and the implementation of a deep learning framework. Current techniques dedicated to symbolic music generation generally encounter two significant challenges: training data's lack of information about chords and scales and the requirement of specially designed model architecture adapted to the unique format of symbolic music representation. In this paper, we solve the above problems by introducing new symbolic music representation with MusicLang chord analysis model. We propose our MMT-BERT architecture adapting to the representation. To build a robust multitrack music generator, we fine-tune a pre-trained MusicBERT model to serve as the discriminator, and incorporate relativistic standard loss. This approach, supported by the in-depth understanding of symbolic music encoded within MusicBERT, fortifies the consonance and humanity of music generated by our method. Experimental results demonstrate the effectiveness of our approach which strictly follows the state-of-the-art methods.",Jinlong ZHU,Jinlong ZHU (Hokkaido University)*; Keigo Sakurai (Hokkaido University); Ren Togo (Hokkaido University); Takahiro Ogawa (Hokkaido University); Miki Haseyama (Hokkaido University),"ZHU, Jinlong*; Sakurai, Keigo; Togo, Ren; Ogawa, Takahiro; Haseyama, Miki",MIR tasks -> music generation,"MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> representations of music",,,,,No,https://drive.google.com/file/d/1h2RYen_J2xlE_zcIMKar6f6grNUAsP2C/view?usp=sharing,https://drive.google.com/file/d/1ShFl4bLk6UHjMko6B562YsCrbwWrtQKF/view?usp=sharing,https://drive.google.com/file/d/1V-r2tzbzCUOKtJWrLyoxeOmBnNSKeAB-/view?usp=sharing,https://drive.google.com/file/d/13TVApqN6sCQay-u3RZCWiJK6LzyXrfBW/view?usp=sharing,https://drive.google.com/file/d/1MUXmyFSFeDJFdQPBWVdcJt_1vVGJPY8F/view?usp=share_link,https://ismir2024.slack.com/archives/C07U9F0C4RM,p3-14-mmt-bert-chord
103,2,4,14,San Francisco,FALSE,TheGlueNote: Learned Representations for Robust and Flexible Note Alignment,"Note alignment refers to the task of matching individual notes of two versions of the same symbolically encoded piece. Methods addressing this task commonly rely on sequence alignment algorithms such as Hidden Markov Models or Dynamic Time Warping (DTW) applied directly to note or onset sequences. While successful in many cases, such methods struggle with large mismatches between the versions. In this work, we learn note-wise representations from data augmented with various complex mismatch cases, e.g. repeats, skips, block insertions, and long trills. At the heart of our approach lies a transformer encoder network --- TheGlueNote --- which predicts pairwise note similarities for two 512 note subsequences. We postprocess the predicted similarities using flavors of weightedDTW and pitch-separated onsetDTW to retrieve note matches for two sequences of arbitrary length. Our approach performs on par with the state of the art in terms of note alignment accuracy, is considerably more robust to version mismatches, and works directly on any pair of MIDI files.",Silvan Peter,Silvan David Peter (JKU)*; Gerhard Widmer (Johannes Kepler University),"Peter, Silvan*; Widmer, Gerhard","MIR tasks -> alignment, synchronization, and score following",MIR fundamentals and methodology -> symbolic music processing,,,,,No,https://drive.google.com/file/d/1GHRxv4pvcpny1pO3Lgx1W1u7kbGoUXnn/view?usp=sharing,https://drive.google.com/file/d/12FVcGsJpZaS3qKKGy82vNAFXsaaoTm_9/view?usp=sharing,https://drive.google.com/file/d/1PfmicdgUqlBQIlc7QFRllz3Km0qc3w_D/view?usp=sharing,https://drive.google.com/file/d/1CcnPsI0Fq-S-7zkqwYY6WIRd5K3wrNAn/view?usp=share_link,https://drive.google.com/file/d/1aAlxzvZpTkBq7dEnpPFDXIyUMlq-LFNm/view?usp=share_link,https://ismir2024.slack.com/archives/C07UPU92QGK,p4-14-thegluenote-learned-representations
104,1,2,2,San Francisco,FALSE,Lessons learned from a project to encode Mensural music on a large scale with Optical Music Recognition,"This paper discusses the transcription of a collection of musical works using Optical Music Recognition (OMR) technologies during the implementation of the Spanish PolifonIA project. The project employs a research-oriented OMR application that leverages modern Artificial Intelligence (AI) technology to encode musical works from images into structured formats. The paper outlines the transcription workflow in several phases: selection, preparation, action, and resolution, emphasizing the efficiency of using AI to reduce manual transcription efforts. The tool facilitated various tasks such as document analysis, management of parts, and automatic content recognition, although manual corrections were still indispensable for ensuring accuracy, especially for complex musical notations and layouts. Our study also highlights the iterative process of model training and corrections that gradually improved transcription speed and accuracy. Furthermore, the paper delves into challenges like managing non-musical elements and the limitations of current OMR technologies with early musical notations. Our findings suggest that while automated tools significantly accelerate the transcription process, they require continuous refinement and human oversight to handle diverse and complex musical documents effectively.",David Rizo,David Rizo (University of Alicante. Instituto Superior de Enseñanzas Artísrticas de la Comunidad Valenciana)*; Jorge Calvo-Zaragoza (University of Alicante); Patricia García-Iasci (University of Alicante); Teresa Delgado-Sánchez (Biblioteca Nacional de España),"Rizo, David*; Calvo-Zaragoza, Jorge; Delgado-Sánchez, Teresa; García-Iasci, Patricia",Applications -> digital libraries and archives,MIR tasks -> optical music recognition,,,,,No,https://drive.google.com/file/d/1NLEOmBHoUA5CLQvB9Bzolv9PWAIPySnl/view?usp=share_link,,,,,https://ismir2024.slack.com/archives/C07VCQYBRG8,p2-02-lessons-learned-from
113,2,3,1,Remote,TRUE,Green MIR? Investigating computational cost of recent music-Ai research in ISMIR,"The environmental footprint of Generative AI and other Deep Learning (DL) technologies is increasing. To understand the scale of the problem and to identify solutions for avoiding excessive energy use in DL research at communities such as ISMIR, more knowledge is needed of the current energy cost of the undertaken research. In this paper, we provide a scoping inquiry of how the ISMIR research concerning automatic music generation (AMG) and computing-heavy music analysis currently discloses information related to environmental impact. We present a study based on two corpora that document 1) ISMIR papers published in the years 2017–2023 that introduce an AMG model, and 2) ISMIR papers from the years 2022–2023 that propose music analysis models and include heavy computations with GPUs. Our study demonstrates a lack of transparency in model training documentation. It provides the first estimates of energy consumption related to model training at ISMIR, as a baseline for making more systematic estimates about the energy footprint of the ISMIR conference in relation to other machine learning events. Furthermore, we map the geographical distribution of generative model contributions and discuss the corporate role in the funding and model choices in this body of work.",Andre Holzapfel,"Andre Holzapfel (KTH Royal Institute of Technology in Stockholm)*; Anna-Kaisa Kaila (KTH Royal Institute of Technology, Stockholm); Petra Jääskeläinen (KTH)","Holzapfel, Andre*; Kaila, Anna-Kaisa; Jääskeläinen, Petra",Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies,"Evaluation, datasets, and reproducibility -> reproducibility; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Philosophical and ethical discussions -> legal and societal aspects of MIR",,,,,Yes,https://drive.google.com/file/d/1rAepoJk1U2R4g3S_AcxdqWDGzRGd7xPg/view?usp=sharing,https://drive.google.com/file/d/1FIrhz2X1ejLKxNdDNhi3msM28OhC1MA3/view?usp=share_link,https://drive.google.com/file/d/1cYKFcKBYn5ndyPg95kzhNTzvaUouhemE/view?usp=share_link,https://drive.google.com/file/d/1tPtxxZyBA0nvUGkMXkTlkGIyxkIwzxft/view?usp=share_link,https://drive.google.com/file/d/1O0QwwL0Wl4M2Qq_7MqW3DtwODgviHBsh/view?usp=share_link,https://ismir2024.slack.com/archives/C07UPU957MZ,p3-01-green-mir-investigating
114,2,4,7,San Francisco,FALSE,PiCoGen2: Piano cover generation with transfer learning approach and weakly aligned data,"Piano cover generation aims to create a piano cover from a pop song. Existing approaches mainly employ supervised learning and the training demands strongly-aligned and paired song-to-piano data, which is built by remapping piano notes to song audio. This would, however, result in the loss of piano information and accordingly cause inconsistencies between the original and remapped piano versions. To overcome this limitation, we propose a transfer learning approach that pre-trains our model on piano-only data and fine-tunes it on weakly-aligned paired data constructed without note remapping. During pre-training, to guide the model to learn piano composition concepts instead of merely transcribing audio, we use an existing lead sheet transcription model as the encoder to extract high-level features from the piano recordings. The pre-trained model is then fine-tuned on the paired song-piano data to transfer the learned composition knowledge to the pop song domain. Our evaluation shows that this training strategy enables our model, named PiCoGen2, to attain high-quality results, outperforming baselines on both objective and subjective metrics across five pop genres.",Chih-Pin Tan,Chih-Pin Tan (National Taiwan University)*; Hsin Ai (National Taiwan University); Yi-Hsin Chang (National Taiwan University); Shuen-Huei Guan (KKCompany Techonologies); Yi-Hsuan Yang (National Taiwan University),"Tan, Chih-Pin*; Ai, Hsin; Chang, Yi-Hsin; Guan, Shuen-Huei; Yang, Yi-Hsuan",Generative Tasks,"Applications -> music composition, performance, and production; Generative Tasks -> transformations; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> music generation; Musical features and properties -> musical style and genre",,,,,No,https://drive.google.com/file/d/1ZtVeAwZ4H40Ed2JSPt2uKGBJNjhMvXcP/view?usp=drive_link,https://drive.google.com/file/d/1GN7GywKGpF4j-DxhbAJey0_uLccSOJre/view?usp=drive_link,https://drive.google.com/file/d/1YM75UhJ5geuLqXYH111Zk8vm9diDLPtd/view?usp=drive_link,https://drive.google.com/file/d/1c8Y5WtQiVYHB4GSufL3261GJ2Y00-rWZ/view?usp=drive_link,https://drive.google.com/file/d/1h-Nky-9BXM0U2jBzVyvuYQClaoQ_QGgS/view?usp=drive_link,https://ismir2024.slack.com/archives/C07V2MNBZT3,p4-07-picogen2-piano-cover
119,3,5,14,San Francisco,FALSE,Exploring the inner mechanisms of large generative music models,"Generative models are starting to become very good at generating realistic text, images, and even music. Identifying how exactly these models conceptualize data has become crucial. To date, however, interpretability research has mainly focused on the text and image domain, leaving a gap in the music domain. In this paper, we investigate the transferability of straightforward text-oriented interpretability techniques to the music domain. Specifically, we examine the usability of these techniques for analyzing how the generative music model MusicGen constructs representations of human-interpretable musicological concepts. Using the DecoderLens, we gain insight into how the model gradually composes these concepts, and using interchange interventions, we observe the contributions of individual model components in generating the sound of specific instruments and genres. We also encounter several shortcomings of the interpretability techniques for the music domain, which underscore the complexity of music and need for proper audio-oriented adaptation. Our research marks an initial step toward understanding generative music models, fundamentally, paving the way for future advancements in controlling music generation.",Marcel A Vélez Vásquez,"Marcel A Vélez Vásquez (University of Amsterdam)*; Charlotte Pouw (University of Amsterdam); John Ashley Burgoyne (University of Amsterdam); Willem Zuidema (ILLC, UvA)","Vélez Vásquez, Marcel A*; Pouw, Charlotte; Burgoyne, John Ashley; Zuidema, Willem",MIR tasks -> music generation,"Knowledge-driven approaches to MIR -> representations of music; MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics; MIR tasks -> music synthesis and transformation; Musical features and properties -> musical style and genre; Musical features and properties -> timbre, instrumentation, and singing voice",,,,,No,https://drive.google.com/file/d/1DWHlGw08xGoHtjrDJ8TAcrODnM2Oj23F/view?usp=drive_link,https://drive.google.com/file/d/18eFvercPT7l7NlT6AmTBgswkiGaN3YWk/view?usp=drive_link,https://drive.google.com/file/d/17ku5P9dyPTxyVAiHvOOkdDv9Gx1rt4zx/view?usp=drive_link,https://drive.google.com/file/d/1TyD-z9NevZqChQ5fWC9mxy634k888c2n/view?usp=drive_link,https://docs.google.com/presentation/d/1oSPPot-7f4ZJabWicEN8AVyF1OCs9j-s/edit?usp=drive_link&ouid=114061375603120990288&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07USGF4WSY,p5-14-exploring-the-inner
128,2,3,2,San Francisco,FALSE,Field Study on Children's Home Piano Practice: Developing a Comprehensive System for Enhanced Student-Teacher Engagement,"In piano education for preschool to early elementary school-aged children, both weekly lessons with a teacher and daily home practice supported by parents are fundamental for skill advancement. However, the nature of students' home practice, often unseen by teachers, has been less examined. This paper aims to (1) identify challenges inherent in home practice and (2) develop a system to mitigate these issues, enhancing teachers' understanding and support of students' home practice to improve piano performance skills. Through observing actual lessons and analyzing 177 days of practice videos from 30 students, this study identifies effective strategies, such as assigning appropriately difficult pieces and rewarding with stickers or stamps. It also highlights obstacles, including student tension under parental guidance and repetitive, unproductive practice. The insights from field study suggest the potential of third-party(system) feedback, practice segmentation, reporting practice records to teachers, and rewards for practice sessions. We developed a system incorporating these solutions and examined with 80 children over four months. Results showed increased teacher engagement with students' home practice, improved student motivation and practice duration, and enhanced sight-reading skills, demonstrating the system's effectiveness in supporting piano education.",Seikoh Fukuda,"Seikoh Fukuda (PTNA Research Institute of Music)*; Yuko Fukuda (Kyoritsu Women’s University, To-on Kikaku Company); Ami Motomura (To-on Kikaku Company); Eri Sasao (To-on Kikaku Company); Masamichi Hosoda (NTT East Corporation); Masaki Matsubara (University of Tsukuba); Masahiro Niitsuma (Keio University)","Fukuda, Seikoh*; Fukuda, Yuko; Motomura, Ami; Sasao, Eri; Hosoda, Masamichi; Matsubara, Masaki; Niitsuma, Masahiro",Applications -> music training and education,,,,,,No,https://drive.google.com/file/d/104iLn5nzx0ocx9skVb8iCcF9iaJzQ_Ol/view?usp=drive_link,https://drive.google.com/file/d/1Xt8hHGue5Hq-y___PIdsHzVR7qZoJrBa/view?usp=drive_link,https://drive.google.com/file/d/1YmK5bZ6NGhemaO6_HooMS3V_lyfvXjqi/view?usp=drive_link,https://drive.google.com/file/d/1aQ0OKoof7HABrUyBoPslf8grP5xgd4Av/view?usp=drive_link,https://docs.google.com/presentation/d/1KKlq27UTsCLRqhocrmwxeBA8FZxBfzWJ/edit?usp=drive_link&ouid=114061375603120990288&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07UM5V2669,p3-02-field-study-on
131,3,5,7,San Francisco,FALSE,Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music,"Generative AI models have recently blossomed, significantly impacting artistic and musical traditions. Research investigating how humans interact with and deem these models is therefore crucial. Through a listening and reflection study, we explore participants' perspectives on AI- vs human-generated progressive metal, in symbolic format, using rock music as a control group. AI-generated examples were produced by ProgGP [1], a Transformer-based model. We propose a mixed methods approach to assess the effects of generation type (human vs. AI), genre (progressive metal vs. rock), and curation process (random vs. cherry-picked). This combines quantitative feedback on genre congruence, preference, creativity, consistency, playability, humanness, and repeatability, and qualitative feedback to provide insights into listeners' experiences. A total of 32 progressive metal fans completed the study. Our findings validate the use of fine-tuning to achieve genre-specific specialization in AI music generation, as listeners could distinguish between AI-generated rock and progressive metal. Despite some AI-generated excerpts receiving similar ratings to human music, listeners exhibited a preference for human compositions. Thematic analysis identified key features for genre and AI vs. human distinctions. Finally, we consider the ethical implications of our work in promoting musical data diversity within MIR research by focusing on an under-explored genre.cusing on an under-explored genre.",Jackson J Loth,Pedro Pereira Sarmento (Centre for Digital Music); Jackson J Loth (Queen Mary University of London)*; Mathieu Barthet (Queen Mary University of London),"Sarmento, Pedro Pereira; Loth, Jackson J*; Barthet, Mathieu",Generative Tasks -> qualitative evaluations,Creativity -> human-ai co-creativity; Creativity -> humanistic discussions; Musical features and properties -> musical style and genre; Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies; Philosophical and ethical discussions -> philosophical and methodological foundations,,,,,No,https://drive.google.com/file/d/1YtkoSFwjRZRWBoBSyzxItdPMH0HS2-gj/view?usp=drive_link,https://drive.google.com/file/d/1Gz6u9QdoBF6iREW6k4742hjVStT5sNMH/view?usp=drive_link,https://drive.google.com/file/d/1xI02FsNtZTNZsYLy4xH-1-lK30ok51vy/view?usp=drive_link,https://drive.google.com/file/d/1p4iM4QJ49Hr4TaPrP9qkgUISHLKvhf39/view?usp=drive_link,https://docs.google.com/presentation/d/1R615zKAqpEqsHGLxLFbhUVRzEj5s5GJk/edit?usp=drive_link&ouid=114061375603120990288&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07UPUACTUK,p5-07-between-the-ai
132,2,4,2,San Francisco,FALSE,From Audio Encoders to Piano Judges: Benchmarking Performance Understanding for Solo Piano,"Our study investigates an approach for understanding musical performances through the lens of audio encoding models, focusing on the domain of solo Western classical piano music. Compared to composition-level attribute understanding such as key or genre, we identify a knowledge gap in performance-level music understanding, and address three critical tasks: expertise ranking, difficulty estimation, and piano technique detection, introducing a comprehensive Pianism-Labelling Dataset (PLD) for this purpose.  We leverage pre-trained audio encoders, specifically Jukebox, Audio-MAE, MERT, and DAC, which demonstrate varied capabilities in tackling downstream tasks, to explore whether domain-specific fine-tuning enhances capability in capturing performance nuances.  Our best approach achieved 93.6% accuracy in expertise ranking, 33.7% in difficulty estimation, and 46.7% in technique detection, with Audio-MAE as the overall most effective encoder. Finally, we conducted a case study on Chopin Piano Competition data using trained models for expertise ranking, which highlights the challenge of accurately assessing top-tier performances.",Huan Zhang,Huan Zhang (Queen Mary University of London)*; Jinhua Liang (Queen Mary University of London); Simon Dixon (Queen Mary University of London),"Zhang, Huan*; Liang, Jinhua; Dixon, Simon",Applications -> music training and education,"Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> automatic classification; Musical features and properties -> expression and performative aspects of music",,,,,No,https://drive.google.com/file/d/1rFIbPl_S3DB7RuKDouwM1G93VJs2xNyG/view?usp=drive_link,https://drive.google.com/file/d/1wi4OTC5XBQhQ7qpas0Rn9vleCH3E40W_/view?usp=drive_link,https://drive.google.com/file/d/1vqusYksr6Y8gJrfP-wWV-3TjpQQQAMrZ/view?usp=drive_link,https://drive.google.com/file/d/1GtHVvjc3ynd1kpH5Yi-nMyLSmsxd49FV/view?usp=drive_link,https://drive.google.com/file/d/1VZMqrrOCFwQhjiPM7_MaEEL3WSZ2WgNi/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UHCQD31U,p4-02-from-audio-encoders
142,2,4,20,San Francisco,FALSE,GraphMuse: A Library for Symbolic Music Graph Processing,"Graph Neural Networks (GNNs) have recently gained traction in symbolic music tasks, yet a lack of a unified framework impedes progress. Addressing this gap, we present GraphMuse, a graph processing framework and library that facilitates efficient music graph processing and GNN training for symbolic music tasks. Central to our contribution is a new neighbor sampling technique specifically targeted toward meaningful behavior in musical scores. Additionally, GraphMuse integrates hierarchical modeling elements that augment the expressivity and capabilities of graph networks for musical tasks. Experiments with two specific musical prediction tasks -- pitch spelling and cadence detection -- demonstrate significant performance improvement over previous methods. Our hope is that GraphMuse will lead to a boost in, and standardization of, symbolic music processing based on graph representations. The library is available at https://github.com/manoskary/graphmuse",Emmanouil Karystinaios,Emmanouil Karystinaios (Johannes Kepler University)*; Gerhard Widmer (Johannes Kepler University),"Karystinaios, Emmanouil*; Widmer, Gerhard",MIR fundamentals and methodology -> symbolic music processing,Applications -> digital libraries and archives; Knowledge-driven approaches to MIR -> representations of music,,,,,No,https://drive.google.com/file/d/1v3xnltIXVjl686mPvV34AsC3eN_vNlXr/view?usp=drive_link,https://drive.google.com/file/d/1hsWV5uz5Y3sBhXcOUIYbhFU5CzFdMBdl/view?usp=drive_link,https://drive.google.com/file/d/1OBlsXp-7Cbh0dH5M1H7HEpVbFp0bsDAt/view?usp=drive_link,https://drive.google.com/file/d/1yKYiFaUKBy94b4hVCUKY5VxPMavNYv5w/view?usp=drive_link,https://docs.google.com/presentation/d/1e8Hqm6q6ksb0mU3ZSkPXUblJP9gXXwOE/edit?usp=drive_link&ouid=114061375603120990288&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07UM5W2E0M,p4-20-graphmuse-a-library
143,3,6,14,San Francisco,FALSE,Controlling Surprisal in Music Generation via Information Content Curve Matching,"In recent years, the quality and public interest in music generation systems have grown, encouraging research into various ways to control these systems.  We propose a novel method for controlling surprisal in music generation using sequence models. To achieve this goal, we define a metric called Instantaneous Information Content (IIC). The IIC serves as a proxy function for the perceived musical surprisal (as estimated from a probabilistic model) and can be calculated at any point within a music piece. This enables the comparison of surprisal across different musical content even if the musical events occur in irregular time intervals. We use beam search to generate musical material whose IIC curve closely approximates a given target IIC. We experimentally show that the IIC correlates with harmonic and rhythmic complexity and note density. The correlation decreases with the length of the musical context used for estimating the IIC. Finally, we conduct a qualitative user study to test if human listeners can identify the IIC curves that have been used as targets when generating the respective musical material. We provide code for creating IIC interpolations and IIC visualizations on https://github.com/muthissar/iic.",Mathias Rose Bjare,"Mathias Rose Bjare (Johannes Kepler University Linz)*; Stefan Lattner (Sony Computer Science Laboratories, Paris); Gerhard Widmer (Johannes Kepler University)","Bjare, Mathias Rose*; Lattner, Stefan; Widmer, Gerhard",MIR tasks -> music generation,"Applications -> music composition, performance, and production; Creativity -> computational creativity; Creativity -> human-ai co-creativity; Generative Tasks -> music and audio synthesis; MIR fundamentals and methodology -> symbolic music processing",,,,,No,https://drive.google.com/file/d/1OfyMa9IiQxA-tXZ3Z8mIu3EQosT7b0ke/view?usp=drive_link,https://drive.google.com/file/d/1C5iStI9QSKT53hU1edU3-s5IPpl-WLto/view?usp=drive_link,https://drive.google.com/file/d/1IuShjd5Ve4Rlu8GXnRA-6E5RrjWD3Qxm/view?usp=drive_link,https://drive.google.com/file/d/1peWDFb7ZP5WXiPBECA59VmMawalX4-hC/view?usp=drive_link,https://drive.google.com/file/d/12irAZavfaPfltgQ9QESrL2aGZ10IxpHJ/view?usp=drive_link,https://ismir2024.slack.com/archives/C07VCR0SXSL,p6-14-controlling-surprisal-in
144,4,7,14,San Francisco,FALSE,Towards Musically Informed Evaluation of Piano Transcription Models,"Automatic piano transcription models are typically evaluated using simple frame- or note-wise information retrieval (IR) metrics. Such benchmark metrics do not provide insights into the transcription quality of specific musical aspects such as articulation, dynamics, or rhythmic precision of the output, which are essential in the context of expressive performance analysis. Furthermore, in recent years, MAESTRO has become the de-facto training and evaluation dataset for such models. However, inference performance has been observed to deteriorate substantially when applied on out-of-distribution data, thereby questioning the suitability and reliability of transcribed outputs from such models for specific MIR tasks. In this work, we investigate the performance of three state-of-the-art piano transcription models in two experiments. In the first one, we propose a variety of musically informed evaluation metrics which, in contrast to the IR metrics, offer more detailed insight into the musical quality of the transcriptions. In the second experiment, we compare inference performance on real-world and perturbed audio recordings, and highlight musical dimensions which our metrics can help explain. Our experimental results highlight the weaknesses of existing piano transcription metrics and contribute to a more musically sound error analysis of transcription outputs.",Patricia Hu,Patricia Hu (Johannes Kepler University)*; Lukáš Samuel Marták (Johannes Kepler University Linz); Carlos Eduardo Cancino-Chacón (Johannes Kepler University Linz); Gerhard Widmer (Johannes Kepler University),"Hu, Patricia*; Marták, Lukáš Samuel; Cancino-Chacón, Carlos Eduardo; Widmer, Gerhard",MIR tasks -> music transcription and annotation,"Evaluation, datasets, and reproducibility -> evaluation metrics; Evaluation, datasets, and reproducibility -> reproducibility; MIR fundamentals and methodology -> music signal processing; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> expression and performative aspects of music",,,,,No,https://drive.google.com/file/d/1aMATsrlzx0nQEK39SiEANw0oS1TMtV8P/view?usp=drive_link,https://drive.google.com/file/d/1ePJF7Y-vNl2ByEydncvcK0FzFsVaXONq/view?usp=drive_link,https://drive.google.com/file/d/12ef3ghWqBJuRoYMf0kdy_SISdzbXNUsJ/view?usp=drive_link,https://drive.google.com/file/d/1vItA8zUFd-yhDIhJktFSwkB0OcGmCCtq/view?usp=drive_link,https://drive.google.com/file/d/1cd2eVYQsdCjR9hUHR3nKFTfB2znOQCDD/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UHCRP206,p7-14-towards-musically-informed
146,3,6,7,San Francisco,FALSE,DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation,"Controllable music generation methods are critical for human-centered AI-based music creation, but are currently limited by speed, quality, and control design trade-offs. Diffusion inference-time T-optimization (DITTO), in particular, offers state-of-the-art results, but is over 10x slower than real-time, limiting practical use. We propose Distilled Diffusion Inference-Time T-Optimization (or DITTO-2), a new method to speed up inference-time optimization-based control and unlock faster-than-real-time generation for a wide-variety of applications such as music inpainting, outpainting, intensity, melody, and musical structure control. Our method works by (1) distilling a pre-trained diffusion model for fast sampling via an efficient, modified consistency or consistency trajectory distillation process (2) performing inference-time optimization using our distilled model with one-step sampling as an efficient surrogate optimization task and (3) running a final multi-step sampling generation (decoding) using our estimated noise latents for best-quality, fast, controllable generation. Through thorough evaluation, we find our method not only speeds up generation over 10-20x, but simultaneously improves control adherence and generation quality all at once. Furthermore, we apply our approach to a new application of maximizing text adherence (CLAP score) and show we can convert an unconditional diffusion model without text inputs into a model that yields state-of-the-art text control. Sound examples can be found at https://ditto-music.github.io/ditto2/.",Zachary Novack,Zachary Novack (UC San Diego)*; Julian McAuley (UCSD); Taylor Berg-Kirkpatrick (UCSD); Nicholas J. Bryan (Adobe Research),"Novack, Zachary*; McAuley, Julian; Berg-Kirkpatrick, Taylor; Bryan, Nicholas J.",Generative Tasks -> music and audio synthesis,Generative Tasks -> real-time considerations; MIR tasks -> music generation,,,,,No,https://drive.google.com/file/d/17SwbJ3L5MUNOuP8LEDQim0X_LXGKKAbm/view?usp=drive_link,https://drive.google.com/file/d/1-hSAWIENA4xRie320xWofu0KbbUOfFxr/view?usp=drive_link,https://drive.google.com/file/d/15LEuiK9PlbbtGaccNVc-b0FZW57CbOW1/view?usp=drive_link,https://drive.google.com/file/d/1kPhHdEsYRq0lmiaOiBBKnS5QjGwD8EWP/view?usp=drive_link,https://drive.google.com/file/d/15HjYQPNZZyZP61KO7skeO239mvhGCkbp/view?usp=drive_link,https://ismir2024.slack.com/archives/C07U9F3LCNT,p6-07-ditto-2-distilled
149,3,5,12,Remote,FALSE,A Critical Survey of Research in Music Genre Recognition,"This paper surveys 560 publications about music genre recognition (MGR) published between 2013–2022, complementing the comprehensive survey of [474], which covered the time frame 1995–2012 (467 publications). For each publication we determine its main functions: a review of research, a contribution to evaluation methodology, or an experimental work. For each experimental work we note the data, experimental approach, and figure of merit it applies. We also note the extents to which any publication engages with work critical of MGR as a research problem, as well as genre theory. Our bibliographic analysis shows for MGR research: 1) it typically does not meaningfully engage with any critique of itself; and 2) it typically does not meaningfully engage with work in genre theory.",Owen Green,Owen Green (Max Planck Institute for Empirical Aesthetics)*; Bob L. T.  Sturm (KTH Royal Institute of Technology); Georgina Born (University College London); Melanie Wald-Fuhrmann (Max Planck Institute for Empirical Aesthetics),"Green, Owen*; Sturm, Bob L. T. ; Born, Georgina; Wald-Fuhrmann, Melanie",MIR tasks -> automatic classification,Philosophical and ethical discussions -> philosophical and methodological foundations,,,,,No,https://drive.google.com/file/d/1BSGRQpoOINbaS9DOBRyVzaB4EL2sZe9w/view?usp=drive_link,https://drive.google.com/file/d/1zno_hZyNoFxobuXbAucl2wttAv_UFUdq/view?usp=drive_link,https://drive.google.com/file/d/12jc4wEqTSpiPIoltg1YFjTufK5eOhjYY/view?usp=drive_link,https://drive.google.com/file/d/1ob_iIyuP03H5EDRaqelgeqdo1sQXrJRe/view?usp=drive_link,https://drive.google.com/file/d/10vp0-PIqvg-HOcnPm58s-3wZ7DYX7BfM/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UHCSADAA,p5-12-a-critical-survey
155,1,1,11,San Francisco,FALSE,Harnessing the Power of Distributions: Probabilistic Representation Learning on Hypersphere for Multimodal Music Information Retrieval,"Probabilistic representation learning provides intricate and diverse representations of music content by characterizing the latent features of each content item as a probability distribution within a certain space. However, typical Music Information Retrieval (MIR) methods based on representation learning utilize a feature vector of each content item, thereby missing some details of their distributional properties. In this study, we propose a probabilistic representation learning method for multimodal MIR based on contrastive learning and optimal transport. Our method trains encoders that map each content item to a hypersphere so that the probability distributions of a positive pair of content items become close to each other, while those of an irrelevant pair are far apart. To achieve such training, we design novel loss functions that utilize both probabilistic contrastive learning and spherical sliced-Wasserstein distances. We demonstrate our method's effectiveness on benchmark datasets as well as its suitability for multimodal MIR through both a quantitative evaluation and a qualitative analysis.",Takayuki Nakatsuka,Takayuki Nakatsuka (National Institute of Advanced Industrial Science and Technology (AIST))*; Masahiro Hamasaki (National Institute of Advanced Industrial Science and Technology (AIST)); Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST)),"Nakatsuka, Takayuki*; Hamasaki, Masahiro; Goto, Masataka",MIR fundamentals and methodology -> multimodality,MIR and machine learning for musical acoustics,,,,,No,https://drive.google.com/file/d/179hgCXSyqxXxz1erf68XrB8Qy09KbX_R/view?usp=drive_link,https://drive.google.com/file/d/1C_MVMWoqeur5iulba_NfP21N647EL0W8/view?usp=drive_link,https://drive.google.com/file/d/1xRye0jC3dcCoaocwUr0RzfrK0pcEw4dY/view?usp=drive_link,https://drive.google.com/file/d/1Qv2AWSlvU5TxzQTTBP6tFCISYjdfYNpA/view?usp=drive_link,https://docs.google.com/presentation/d/1cya12goW1W0-BPgXPLBpJxx3JiAuCKAk/edit?usp=drive_link&ouid=114061375603120990288&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07USGJJ8G4,p1-11-harnessing-the-power
158,1,2,15,San Francisco,FALSE,End-to-end Piano Performance-MIDI to Score Conversion with Transformers,"The automated creation of accurate musical notation from an expressive human performance is a fundamental task in computational musicology. To this end, we present an end-to-end deep learning approach that constructs detailed musical scores directly from real-world piano performance-MIDI files.  We introduce a modern transformer-based architecture with a novel tokenized representation for symbolic music data. Framing the task as sequence-to-sequence translation rather than note-wise classification reduces alignment requirements and annotation costs, while allowing the prediction of more concise and accurate notation. To serialize symbolic music data, we design a custom tokenization stage based on compound tokens that carefully quantizes continuous values.  This technique preserves more score information while reducing sequence lengths by 3.5x compared to prior approaches. Using the transformer backbone, our method demonstrates better understanding of note values, rhythmic structure, and details such as staff assignment. When evaluated end-to-end using transcription metrics such as MUSTER, we achieve significant improvements over previous deep learning approaches and complex HMM-based state-of-the-art pipelines. Our method is also the first to directly predict notational details like trill marks or stem direction from performance data. Code and models are available on GitHub.",Tim Beyer,Tim Beyer (Technical University of Munich)*; Angela Dai (Technical University of Munich),"Beyer, Tim*; Dai, Angela",MIR tasks -> music transcription and annotation,"MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music synthesis and transformation; Musical features and properties -> rhythm, beat, tempo; Musical features and properties -> structure, segmentation, and form",,,,,No,https://drive.google.com/file/d/10tX3QWkVV5ALUq3aD0QwXSJ4JP9dmD1B/view?usp=sharing,https://drive.google.com/file/d/11SgGetPXGSIFCwFpgwiUIefA0Hy3jLjk/view?usp=drive_link,https://drive.google.com/file/d/1X8aKfd0wZiXnLgeiCqNYOBWXzxHWIDLY/view?usp=drive_link,https://drive.google.com/file/d/1WEIyNwDfEfHyazJa093LiyjPY5310QN-/view?usp=drive_link,https://drive.google.com/file/d/1pUyUcFoY15nX2VCZt-JRn96v0JjrecFC/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UHCSSRSA,p2-15-end-to-end
160,3,5,9,San Francisco,FALSE,Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox Tewahedo Church Chants,"Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared's establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared's standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.",Mequanent Argaw Muluneh,Mequanent Argaw Muluneh (Academia Sinica; National Chengchi University; Debre Markos University)*; Yan-Tsung Peng (National Chengchi University); Li Su (Academia Sinica),"Muluneh, Mequanent Argaw*; Peng, Yan-Tsung; Su, Li",Knowledge-driven approaches to MIR,Applications -> music heritage and sustainability; Knowledge-driven approaches to MIR -> computational ethnomusicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> automatic classification,,,,,No,https://drive.google.com/file/d/1MEwhva_sHlKS0lC0uCgp-c0NdbR0u0aj/view?usp=sharing,https://drive.google.com/file/d/1MvPSM9M5alKJLAMvY18ECo1FGwOpH1vN/view?usp=drive_link,https://drive.google.com/file/d/1h_N18Rn6r2wvkPRPipYQgxSj-Du2_zBr/view?usp=drive_link,https://drive.google.com/file/d/1VdrhCgcONuBQ_PGqf-4qf3l7isvYMLVN/view?usp=drive_link,https://docs.google.com/presentation/d/1UeQZJTSpW_A_G8IyP67VcTRBZKVTmURQ/edit?usp=drive_link&ouid=100778348236936722074&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07UM5YDGLV,p5-09-computational-analysis-of
163,4,7,7,San Francisco,FALSE,HIERARCHICAL GENERATIVE MODELING OF THE MELODIC VOICE IN HINDUSTANI CLASSICAL MUSIC,"Hindustani music is a performance-driven oral tradition that exhibits the rendition of rich melodic patterns. In this paper, we focus on generative modeling of singers' vocal melodies extracted from audio recordings, as the voice is musically prominent within the tradition. Prior generative work in Hindustani music models melodies as coarse discrete symbols which fails to capture the rich expressive melodic intricacies of singing. Thus, we propose to use a finely quantized pitch contour, as an intermediate representation for hierarchical audio modeling. We propose GaMaDHaNi, a modular two-level hierarchy, consisting of a generative model on pitch contours, and a pitch contour to audio synthesis model. We compare our approach to non-hierarchical audio models and hierarchical models that use a self-supervised intermediate representation, through a listening test and qualitative analysis. We also evaluate audio model's ability to faithfully represent the pitch contour input using Pearson correlation coefficient. By using pitch contours as an intermediate representation, we show that our model may be better equipped to listen and respond to musicians in a human-AI collaborative setting by highlighting two potential interaction use cases (1) primed generation, and (2) coarse pitch conditioning.",Nithya Nadig Shikarpur,"Nithya Nadig Shikarpur (Mila; University of Montreal)*; Krishna Maneesha Dendukuri (Mila); Yusong Wu (Mila, University of Montreal); Antoine CAILLON (IRCAM); Cheng-Zhi Anna Huang (Google Brain)","Shikarpur, Nithya Nadig*; Dendukuri, Krishna Maneesha; Wu, Yusong; CAILLON, Antoine; Huang, Cheng-Zhi Anna",Generative Tasks -> music and audio synthesis,Generative Tasks -> artistically-inspired generative tasks ; MIR tasks -> music generation; Musical features and properties -> expression and performative aspects of music,,,,,No,https://drive.google.com/file/d/1H-vRUco98pLP7fCkW_S2ZuMGROgIzeUn/view?usp=sharing,https://drive.google.com/file/d/1U_ukptuOX3X_RU7MTrWgKKH98jkHs3zv/view?usp=drive_link,https://drive.google.com/file/d/1NOT5skCVZv7xf9Seqp0B2fPNfRuDuCQ1/view?usp=drive_link,https://drive.google.com/file/d/147BlrgUuCe8O0bEWiG_J2xLWQn7RA11G/view?usp=drive_link,https://drive.google.com/file/d/1GGQ9kM6q38IWjaXylisTKTk_jnaCdkwy/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UPUDDVDZ,p7-07-hierarchical-generative-modeling
166,2,3,15,San Francisco,FALSE,Discogs-VI: A Musical Version Identification Dataset Based on Public Editorial Metadata,"Current version identification (VI) datasets often lack suf- ficient size and musical diversity to train robust neural net- works (NNs). Additionally, their non-representative clique size distributions prevent realistic system evaluations. To address these challenges, we explore the untapped poten- tial of the rich editorial metadata in the Discogs music database and create a large dataset of musical versions con- taining about 1,900,000 versions across 348,000 cliques. Utilizing a high-precision search algorithm, we map this dataset to official music uploads on YouTube, resulting in a dataset of approximately 493,000 versions across 98,000 cliques. This dataset offers over nine times the number of cliques and over four times the number of versions than existing datasets. We demonstrate the utility of our dataset by training a baseline NN without extensive model com- plexities or data augmentations, which achieves competi- tive results on the SHS100K and Da-TACOS datasets. Our dataset, along with the tools used for its creation, the ex- tracted audio features, and a trained model, are all publicly available online.",Recep Oguz Araz,Recep Oguz Araz (Universitat Pompeu Fabra)*; Xavier Serra (Universitat Pompeu Fabra ); Dmitry Bogdanov (Universitat Pompeu Fabra),"Araz, Recep Oguz*; Serra, Xavier; Bogdanov, Dmitry",MIR tasks,"Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; MIR tasks -> fingerprinting; MIR tasks -> pattern matching and detection; Musical features and properties -> representations of music",,,,,No,https://drive.google.com/file/d/1ZLoWu-jji5KbPxEsQtWLJIxxYGASZ919/view?usp=sharing,https://drive.google.com/file/d/1S6tj2j1ocTlGctPUCW7SIr6CuhdifpwX/view?usp=drive_link,https://drive.google.com/file/d/11b84MPrhbqy3NfjtwJRNsJnNKlPLExfe/view?usp=drive_link,https://drive.google.com/file/d/1LBJWnWvXdGPgEicEIqG9NF6PkNRNM1cl/view?usp=drive_link,https://drive.google.com/file/d/1IOWKkvQP37WJxMsqoJ-VhSEFlb-3ufA9/view?usp=drive_link,https://ismir2024.slack.com/archives/C07V2MSQ4M7,p3-15-discogs-vi-a
171,2,4,15,San Francisco,FALSE,GAPS: A Large and Diverse Classical Guitar Dataset and Benchmark Transcription Model,"We introduce GAPS (Guitar-Aligned Performance Scores), a new dataset of classical guitar performances, and a benchmark guitar transcription model that achieves state-of-the-art performance on GuitarSet in both supervised and zero-shot settings. GAPS is the largest dataset of real guitar audio, containing 14 hours of freely available audio-score aligned pairs, recorded in diverse conditions by over 200 performers, together with high-resolution note-level MIDI alignments. These enable us to train a state-of-the-art model for automatic transcription of solo guitar recordings which can generalise well to real world audio that is unseen during training. In addition, we propose a novel postprocessing step to estimate note velocities in the absence of ground truth training data.  For each track in the dataset, we provide metadata of the composer and performer, giving dates, nationality, gender and links to IMSLP or Wikipedia. We also analyse guitar-specific features of the dataset, such as the distribution of fret-string combinations and alternate tunings. This dataset has applications to various MIR tasks, including automatic music transcription, score following, performance analysis, generative music modelling and the study of expressive performance timing.",Xavier Riley,Xavier Riley (C4DM)*; Zixun Guo (Singapore University of Technology and Design); Andrew C Edwards (QMUL); Simon Dixon (Queen Mary University of London),"Riley, Xavier*; Guo, Zixun; Edwards, Andrew C; Dixon, Simon",MIR tasks -> music transcription and annotation,"Evaluation, datasets, and reproducibility -> novel datasets and use cases",,,,,No,https://drive.google.com/file/d/1_nkEU4y92n9KsoImukq-hdkQGIF5ayG1/view?usp=sharing,https://drive.google.com/file/d/1pXHGwN_LBlDuo_8h_IuXmJWhH_ARsDqc/view?usp=drive_link,https://drive.google.com/file/d/1jTWK_yvFwyTZ9kZq-aN5mQksmVLpCgFI/view?usp=drive_link,https://drive.google.com/file/d/1gmGyqx4ugWoGoOsRJz1AM4gAhkmHhoGR/view?usp=drive_link,https://docs.google.com/presentation/d/1Bm8F6m_dyU6JbC4_6WmPgP-mEkqz8b2A/edit?usp=drive_link&ouid=100778348236936722074&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07UQ1JTRDG,p4-15-gaps-a-large
172,1,1,8,San Francisco,FALSE,Music2Latent: Consistency Autoencoders for Latent Audio Compression,"Efficient audio representations in a compressed continuous latent space are critical for generative audio modeling and Music Information Retrieval (MIR) tasks. However, some existing audio autoencoders have limitations, such as multi-stage training procedures, slow iterative sampling, or low reconstruction quality. We introduce Music2Latent, an audio autoencoder that overcomes these limitations by leveraging consistency models. Music2Latent encodes samples into a compressed continuous latent space in a single end-to-end training process while enabling high-fidelity single-step reconstruction. Key innovations include conditioning the consistency model on upsampled encoder outputs at all levels through cross connections, using frequency-wise self-attention to capture long-range frequency dependencies, and employing frequency-wise learned scaling to handle varying value distributions across frequencies at different noise levels.    We demonstrate that Music2Latent outperforms existing continuous audio autoencoders in sound quality and reconstruction accuracy while achieving competitive performance on downstream MIR tasks using its latent representations. To our knowledge, this represents the first successful attempt at training an end-to-end consistency autoencoder model.",Marco Pasini,"Marco Pasini (Queen Mary University of London)*; Stefan Lattner (Sony Computer Science Laboratories, Paris); George Fazekas (QMUL)","Pasini, Marco*; Lattner, Stefan; Fazekas, George",Generative Tasks -> music and audio synthesis,"MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics; MIR fundamentals and methodology -> music signal processing; MIR tasks -> music synthesis and transformation; Musical features and properties -> representations of music; Musical features and properties -> timbre, instrumentation, and singing voice",,,,,No,https://drive.google.com/file/d/1r1mR7WET9fAjCfcgA1eg8-5ksrxxYLTO/view?usp=sharing,https://drive.google.com/file/d/1RLeOA3JBDyvLD0YlTzjGZkkc-8bmBZ_y/view?usp=drive_link,https://drive.google.com/file/d/1kvbxJ4jaHuoGgg6G-PogYyFgXFr_H7nf/view?usp=drive_link,https://drive.google.com/file/d/14kuFBPa1kuMBRP_W8qAhKlrNS-Fyeo1y/view?usp=drive_link,https://drive.google.com/file/d/1UA0xJA0UdSH4nVRN0RcjRoQOXf6Sd6pE/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UM5Z46PP,p1-08-music2latent-consistency-autoencoders
173,2,4,6,San Francisco,FALSE,Utilizing Listener-Provided Tags for Music Emotion Recognition: A Data-Driven Approach,"This work introduces a data-driven approach for assigning emotions to music tracks. Consisting of two distinct phases, our framework enables the creation of synthetic emotion-labeled datasets that can serve both Music Emotion Recognition and Auto-Tagging tasks. The first phase presents a versatile method for collecting listener-generated verbal data, such as tags and playlist names, from multiple online sources on a large scale. We compiled a dataset of 5,892 tracks, each associated with textual data from four distinct sources. The second phase leverages Natural Language Processing for representing music-evoked emotions, relying solely on the data acquired during the first phase. By semantically matching user-generated text to a well-known corpus of emotion-labelled English words, we are ultimately able to represent each music track as an 8-dimensional vector that captures the emotions perceived by listeners. Our method departs from conventional labeling techniques: instead of defining emotions as generic ''mood tags'' found on social platforms, we leverage a refined psychological model drawn from Plutchik's theory, which appears more intuitive than the extensively used Valence-Arousal model.",Joanne Affolter,Joanne Affolter (Ecole Polytechnique Fédérale de Lausanne (EPFL))*; Yannis Rammos (EPFL); Martin A Rohrmeier (Ecole Polytechnique Fédérale de Lausanne),"Affolter, Joanne*; Rammos, Yannis; Rohrmeier, Martin A","Musical features and properties -> musical affect, emotion and mood","Applications -> music recommendation and playlist generation; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> lyrics and other textual data; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; MIR fundamentals and methodology -> web mining, and natural language processing",,,,,No,https://drive.google.com/file/d/10-WI067fK768G9WPUCqOtAdDZRE6-Lha/view?usp=sharing,https://drive.google.com/file/d/1guOhHompWVJjf1rj8xCmQ4U5mm0sY-SH/view?usp=sharing,https://drive.google.com/file/d/1UdtX6Cv8KESQAb8MhxvjwMR1pHaP8B0X/view?usp=drive_link,https://drive.google.com/file/d/1kV4-VGsTYzPqBglFwEpIF1A-uYjut1kp/view?usp=drive_link,https://drive.google.com/file/d/1vAOUUB2wk_DH8CGy_aJp5V-67FOmvCET/view?usp=drive_link,https://ismir2024.slack.com/archives/C07USGKV0TE,p4-06-utilizing-listener-provided
175,1,2,11,San Francisco,FALSE,MIDI-to-Tab: Guitar Tablature Inference via Masked Language Modeling,"Guitar tablatures enrich the structure of music notation by assigning each note to a string and fret of a guitar in a particular tuning, defining precisely where to play the note on the instrument. The problem of generating tablature from a symbolic music representation involves inferring this string and fret assignment per note across an entire composition or performance. On the guitar, multiple string-fret assignments are possible for each pitch, which leads to a large combinatorial space that prevents exhaustive search approaches. Most modern methods use constraint-based dynamic programming approaches to minimize some cost function (e.g. hand position movement). In this work, we introduce a novel deep learning solution to symbolic guitar tablature estimation. We train an encoder-decoder Transformer model in a masked language modeling paradigm to assign notes to strings. The model is first pre-trained on DadaGP, a dataset of over 25K tablatures, and then fine-tuned on a curated set of professionally transcribed guitar performances. Given the subjective nature of assessing tablature quality, we conduct a user study amongst guitarists, wherein we ask participants to rate the playability of multiple versions of tablature for the same four-bar excerpt. The results indicate our system significantly outperforms competing algorithms.",Andrew C Edwards,Andrew C Edwards (QMUL)*; Xavier Riley (C4DM); Pedro Pereira Sarmento (Centre for Digital Music); Simon Dixon (Queen Mary University of London),"Edwards, Andrew C*; Riley, Xavier; Sarmento, Pedro Pereira; Dixon, Simon",MIR fundamentals and methodology -> symbolic music processing,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> music transcription and annotation; Musical features and properties -> representations of music,,,,,No,https://drive.google.com/file/d/1dA3AfAQem0BFPcvtKOIvP1gPe1LLphtb/view?usp=sharing,https://drive.google.com/file/d/1eKOb0fTCyui8Hwi6WBoAsZZqbSC-2g5G/view?usp=drive_link,https://drive.google.com/file/d/1wNvHcqpvSlqibYDq1Ww4DIltYIjRvpXW/view?usp=drive_link,https://drive.google.com/file/d/1_0gL0ENfndsCMpHoxvaVD8r6lmOOz4xA/view?usp=drive_link,https://drive.google.com/file/d/1CrZYixT3O0wIX604yI-ZrPkHlLnsfKz0/view?usp=drive_link,https://ismir2024.slack.com/archives/C07USGL5AN8,p2-11-midi-to-tab
181,3,5,15,San Francisco,FALSE,Quantitative Analysis of Melodic Similarity in Music Copyright Infringement Cases,"This study aims to measure the similarity of melodies objectively using natural language processing (NLP) techniques. We utilize Mel2word which is a melody tokenization method based on byte-pair encoding to facilitate the semantic analysis of melodies. In addition, we apply two word weighting methods: the modified Tversky measure for word salience and the TF-IDF method for word importance and uniqueness, to better understand the characteristics of each melodic element. We validate our approach by comparing song vectors calculated from an average of Mel2Word vectors to the ground truth in 108 cases of music copyright infringement, sourced from an extensive review of legal documents from law archives. The results demonstrate that the proposed approach is more in accordance with court rulings and perceptual similarity.",Saebyul Park,Saebyul Park (KAIST)*; Halla Kim (KAIST); Jiye Jung (Heinrich Heine University Düsseldorf); Juyong Park (KAIST); Jeounghoon Kim (KAIST); Juhan Nam (KAIST),"Park, Saebyul*; Kim, Halla; Jung, Jiye; Park, Juyong; Kim, Jeounghoon; Nam, Juhan",MIR tasks -> similarity metrics,"Evaluation, datasets, and reproducibility -> evaluation metrics; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> representations of music; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> melody and motives",,,,,No,https://drive.google.com/file/d/1h2RYen_J2xlE_zcIMKar6f6grNUAsP2C/view?usp=sharing,https://drive.google.com/file/d/1BEklT7FMPCCtK9rbU55EzcjJoXVm3Zd-/view?usp=drive_link,https://drive.google.com/file/d/1KUZ1jwX_Upxe_npkfZz_2_URZ-AnXUtP/view?usp=drive_link,https://drive.google.com/file/d/1EQoIOAOjkXA20GzKP0ScroOHIFp75ZmN/view?usp=drive_link,https://drive.google.com/file/d/1RyanJd_89zIQ1FgzonHmgWJz2GTy9qJ0/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UHCU8PJS,p5-15-quantitative-analysis-of
184,2,3,11,San Francisco,FALSE,Towards Zero-Shot Amplifier Modeling: One-to-Many Amplifier Modeling via Tone Embedding Control,"The pursuit of replicating analog device circuits through neural audio effect modeling has garnered increasing interest in recent years. Existing work has predominantly focused on a one-to-one emulation strategy, modeling specific devices individually. However, the potential for a one-to-many emulation strategy remains an avenue yet to be explored. This paper presents such an attempt that utilizes conditioning mechanisms to emulate multiple guitar amplifiers through a single neural model. For condition representation, we use contrastive learning to build a tone embedding encoder designed to distill and encode the distinctive style-related features of various amplifiers, leveraging a dataset of comprehensive amplifier settings. Targeting zero-shot application scenarios, we also examine various strategies for tone embedding representation, evaluating referenced tone embedding against two retrieval-based embedding methods for amplifiers unseen in the training time. Our findings showcase the efficacy and potential of the proposed methods in achieving versatile one-to-many amplifier modeling,  contributing a foundational step towards zero-shot audio modeling applications.",Yu-Hua Chen,Yu-Hua Chen (NTU)*; Yen-Tung Yeh (National Taiwan University); Yuan-Chiao Cheng  (Positive Grid); Jui-Te Wu (Positive Grid);  Yu-Hsiang Ho (Positive Grid ); Jyh-Shing Roger Jang (National Taiwan University); Yi-Hsuan Yang (National Taiwan University),"Chen, Yu-Hua*; Yeh, Yen-Tung; Cheng , Yuan-Chiao; Wu, Jui-Te; Ho,  Yu-Hsiang; Jang, Jyh-Shing Roger; Yang, Yi-Hsuan",MIR fundamentals and methodology -> music signal processing,"Applications -> music composition, performance, and production; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> music synthesis and transformation; Musical features and properties -> timbre, instrumentation, and singing voice",,,,,No,https://drive.google.com/file/d/1LB9waLGDAqxl6v7OeYOtljs-yG4VwW0B/view?usp=sharing,https://drive.google.com/file/d/1ylPmxWdUuuKhEQn34Y79rLP3ET2SUQcr/view?usp=drive_link,https://drive.google.com/file/d/1nL1IHXeDaJOGw2_R-N-6JIyCDSoW0pjs/view?usp=drive_link,https://drive.google.com/file/d/1NP9o0FXcdpRSEbAPJP8hIPj-kIrqvLl4/view?usp=drive_link,https://drive.google.com/file/d/14CrDEhMQK0_5fQ69l35jpO6Mu8NORrEH/view?usp=drive_link,https://ismir2024.slack.com/archives/C07V2MTPMMF,p3-11-towards-zero-shot
189,3,5,3,San Francisco,FALSE,Do Music Generation Models Encode Music Theory?,"Music foundation models possess impressive music generation capabilities. When people compose music, they may infuse their understanding of music into their work, by using notes and intervals to craft melodies, chords to build progressions, and tempo to create a rhythmic feel. To what extent is this true of music generation models? More specifically, are fundamental Western music theory concepts observable within the ""inner workings"" of these models? Recent work proposed leveraging latent audio representations from music generation models towards music information retrieval tasks (e.g. genre classification, emotion recognition), which suggests that high-level musical characteristics are encoded within these models. However, probing individual music theory concepts (e.g. tempo, pitch class, chord quality) remains under-explored. Thus, we introduce SynTheory, a synthetic MIDI and audio music theory dataset, consisting of tempos, time signatures, notes, intervals, scales, chords, and chord progressions concepts. We then propose a framework to probe for these music theory concepts in music foundation models (Jukebox and MusicGen) and assess how strongly they encode these concepts within their internal representations. Our findings suggest that music theory concepts are discernible within foundation models and that the degree to which they are detectable varies by model size and layer.",Megan Wei,Megan Wei (Brown University)*; Michael Freeman (Brown University); Chris Donahue (Carnegie Mellon University); Chen Sun (Brown University),"Wei, Megan*; Freeman, Michael; Donahue, Chris; Sun, Chen",Musical features and properties,"Knowledge-driven approaches to MIR -> computational music theory and musicology; Musical features and properties -> representations of music; Musical features and properties -> rhythm, beat, tempo",,,,,No,https://drive.google.com/file/d/1R2hwTNW-EQT5xNODQKtDdxot3yFrTpAE/view?usp=sharing,https://drive.google.com/file/d/1kB-DGyxmkJIC_aZ1oqnbhqQBlyjlZPGm/view?usp=drive_link,https://drive.google.com/file/d/1MATeD7zabPqC5O0vgoKabKJZjuKhOpHH/view?usp=drive_link,https://drive.google.com/file/d/16sNrWnQaBXKcd2X3bjhM0XRUHzA0FA8w/view?usp=sharing,https://drive.google.com/file/d/1_rsqoSkGyWDpp3sb8knCUVqJWXLk9C-0/view?usp=drive_link,https://ismir2024.slack.com/archives/C07U9F62P9V,p5-03-do-music-generation
193,3,6,15,San Francisco,FALSE,Toward a More Complete OMR Solution,"Optical music recognition (OMR) aims to convert music notation into digital formats. One approach to tackle OMR is through a multi-stage pipeline, where the model first detects visual music notation elements in the image (object detection) and then assembles them into a music notation (notation assembly). In this paper, we focus on the MUSCIMA++ v2.0 dataset. It represents musical notation as a graph, where the pairwise relationships among detected music objects are predicted. Most previous work on notation assembly unrealistically assumes perfect object detection. In this study, we consider both stages together. First, we introduce a music object detector based on YOLOv8, which improves detection performance. Second, we introduce a supervised training pipeline that completes the notation assembly stage based on detection output. We find that this model is able to outperform existing models trained on perfect detection output, showing the benefit of considering the detection and assembly stage in a more holistic way. These findings are an important step toward a more complete OMR solution.",Guang Yang,Guang Yang (University of Washington)*; Muru Zhang (University of Washington); Lin Qiu (University of Washington); Yanming Wan (University of Washington); Noah A Smith (University of Washington and Allen Institute for AI),"Yang, Guang*; Zhang, Muru; Qiu, Lin; Wan, Yanming; Smith, Noah A",MIR tasks -> optical music recognition,"Evaluation, datasets, and reproducibility -> evaluation metrics; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",,,,,No,https://drive.google.com/file/d/1ymF8sx8o5Krxh8EGMA2UwmhRX0ov0YkR/view?usp=sharing,https://drive.google.com/file/d/161aBb4xSrTI7akF0_bTI2SPwTzI4zunK/view?usp=drive_link,https://drive.google.com/file/d/1S0-dUNVs10dALNj86ORBmU5EaH9IV-79/view?usp=drive_link,https://drive.google.com/file/d/1py7A8U7NlmRV53-cKnyykr626eHooiwL/view?usp=drive_link,https://drive.google.com/file/d/1Vzp703cjlBywTrZz_PSw3wayOGtqz01Y/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UM5ZULS1,p6-15-toward-a-more
198,3,6,5,San Francisco,FALSE,MidiCaps: A Large-scale MIDI Dataset with Text Captions,"Generative models guided by text prompts are increasingly becoming more popular. However, no text-to-MIDI models currently exist due to the lack of a captioned MIDI dataset. This work aims to enable research that combines LLMs with symbolic music by presenting MidiCaps, the first openly available large-scale MIDI dataset with text captions. MIDI (Musical Instrument Digital Interface) files are widely used for encoding musical information and can capture the nuances of musical composition. They are widely used by music producers, composers, musicologists, and performers alike. Inspired by recent advancements in captioning techniques, we present a curated dataset of over 168k MIDI files with textual descriptions. Each MIDI caption describes the musical content, including tempo, chord progression, time signature, instruments, genre, and mood, thus facilitating multi-modal exploration and analysis. The dataset encompasses various genres, styles, and complexities, offering a rich data source for training and evaluating models for tasks such as music information retrieval, music understanding, and cross-modal translation. We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study. We anticipate that this resource will stimulate further research at the intersection of music and natural language processing, fostering advancements in both fields.",Abhinaba Roy,Jan Melechovsky (Singapore University of Technology and Design); Abhinaba Roy (SUTD)*; Dorien Herremans (Singapore University of Technology and Design),"Melechovsky, Jan; Roy, Abhinaba*; Herremans, Dorien","Evaluation, datasets, and reproducibility -> novel datasets and use cases","Generative Tasks -> music and audio synthesis; MIR fundamentals and methodology -> lyrics and other textual data; MIR fundamentals and methodology -> symbolic music processing; MIR fundamentals and methodology -> web mining, and natural language processing; Musical features and properties -> representations of music",,,,,No,https://drive.google.com/file/d/18ltPsN6Gp-anmdtvXVkfsuFWG-tTAvmF/view?usp=sharing,https://drive.google.com/file/d/1qIrcmvqJRIO-Ov3kpzGRrxay8Mh_jzom/view?usp=drive_link,https://drive.google.com/file/d/16b061WIusDZCvVrtgezh5AiyCE9ElkdT/view?usp=drive_link,https://drive.google.com/file/d/1oM1Zo-J2dI_yHQeh3Op6J7Bc3eC4oS6W/view?usp=drive_link,https://docs.google.com/presentation/d/1U8-zNyvoh2XQzxW63IP0gEs5Agsz9mcR/edit?usp=drive_link&ouid=100778348236936722074&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07UPUEUDV1,p6-05-midicaps-a-large
205,4,7,15,San Francisco,FALSE,Using Item Response Theory to Aggregate Music Annotation Results of Multiple Annotators,"Human music annotation is one of the most important tasks in music information retrieval (MIR) research. Results of labeling, tagging, assessment, and evaluation can be used as training data for machine learning models that estimate them automatically. For such machine learning purposes, a single target (e.g., song) is usually annotated by multiple human annotators, and the results are aggregated by majority voting or averaging. Majority voting, however, requires the number of annotators to be an odd number, which is not always possible. And averaging is sensitive to differences in the judgmental characteristics of each annotator and cannot be used for ordinal scales. This paper therefore proposes that the item response theory (IRT) be used to aggregate the music annotation results of multiple annotators. IRT-based models can jointly estimate annotators' characteristics and latent scores (i.e., aggregations of annotation results) of the targets, and they are also applicable to ordinal scales. We evaluated the IRT-based models in two actual cases of music annotation --- semantic tagging of music and Likert scale-based evaluation of singing skill --- and compared those models with their simplified models that do not consider the characteristics of each annotator.",Tomoyasu Nakano,Tomoyasu Nakano (National Institute of Advanced Industrial Science and Technology (AIST))*; Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST)),"Nakano, Tomoyasu*; Goto, Masataka",MIR tasks -> music transcription and annotation,,,,,,No,https://drive.google.com/file/d/1BVN3Qgw_tfZC3iUYrjK2mBdM4exBcZhl/view?usp=sharing,https://drive.google.com/file/d/17c0w9DoxLbJZUjA6tUriZUgnE54eOgqb/view?usp=drive_link,https://drive.google.com/file/d/1s7Hc5Jmx_LyU5wfSknSFiNyXYGg6xxgT/view?usp=drive_link,https://drive.google.com/file/d/14BG6zTeWW-X8EG95y9GnlrTbIKM-esJo/view?usp=drive_link,https://drive.google.com/file/d/1Gtsgw0oUsPWNgawHzNeDSgBmsJLasdnk/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UHCURWMC,p7-15-using-item-response
207,1,1,1,Remote,TRUE,Formal Modeling of Structural Repetition using Tree Compression,"Repetition is central to musical structure as it gives rise both to piece-wise and stylistic coherence. Identifying repetitions in music is computationally not trivial, especially when they are varied or deeply hidden within tree-like structures. Rather than focusing on repetitions of musical events, we propose to pursue repeated structural relations between events. More specifically, given a context-free grammar that describes a tonal structure, we aim to computationally identify such relational repetitions within the derivation tree of the grammar. To this end, we first introduce the template, a grammar-generic structure for generating trees that contain structural repetitions. We then approach the discovery of structural repetitions as a search for optimally compressible templates that describe a corpus of pieces in the form of production-rule-labeled trees. To make it tractable, we develop a heuristic, inspired by tree compression algorithms, to approximate the optimally compressible templates of the corpus. After implementing the algorithm in Haskell, we apply it to a corpus of jazz harmony trees, where we assess its performance based on the compressibility of the resulting templates and the music-theoretical relevance of the identified repetitions.",Zeng Ren,Zeng Ren (École Polytechnique Fédérale de Lausanne)*; Yannis Rammos (EPFL); Martin A Rohrmeier (Ecole Polytechnique Fédérale de Lausanne),"Ren, Zeng*; Rammos, Yannis; Rohrmeier, Martin A",Computational musicology,"Computational musicology -> mathematical music theory; Knowledge-driven approaches to MIR -> computational music theory and musicology; Knowledge-driven approaches to MIR -> representations of music; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> structure, segmentation, and form",,,,,Yes,https://drive.google.com/file/d/1mL_I29Y9OXfstFXm0C2EVB8xozviBLhL/view?usp=sharing,https://drive.google.com/file/d/1bXs3DMiL--96DfbHYoJfdmp4cQe4bIEr/view?usp=drive_link,https://drive.google.com/file/d/1g-xHPS7iaP2x5Gbq3DlGXcaiNs5NAXRC/view?usp=drive_link,https://drive.google.com/file/d/14P2-03D3SKJsWef0kto2uDsHpqCh4-3h/view?usp=drive_link,https://drive.google.com/file/d/1HiVtUezFuXJ3e7h_9MsAc18nYJwVFyeH/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UPUF5AKD,p1-01-formal-modeling-of
212,2,3,16,Remote,FALSE,WHO'S AFRAID OF THE `ARTYFYSHALL BYRD'? HISTORICAL NOTIONS AND CURRENT CHALLENGES OF MUSICAL ARTIFICIALITY,"The meteoric surge of AI-generated music has prompted significant concerns among artists and publishers alike. Some fear that the adoption of AI is poised to result in massive job destruction; others sense it will jeopardize and eventually upend all legal frameworks of intellectual property. AI, however, is not the first instance where humanity has confronted the prospect of machines emulating musical creativity. Already in the Baroque, various modes of musical artificiality were explored, ranging from automata and organ stops mimicking human performance and natural sounds, up to devices for mechanized composition (e.g., Athanasius Kircher, Johann Philip Kirnberger, C.P.E. Bach, Antonio Calegari and Diederich Nickolaus Winkel).   Valuable insights emerge from the reconsideration—and digital implementation—of these curiosities through the lens of present-day generative models. It can be argued that the very notion of ‘artificiality’ has presented humanity with long-standing philosophical dilemmas, in addressing the debate on the role of art as a substitute of (divine) nature. By digitally implementing and formalizing some pioneering instances of  algorithmically-generated music we wish to illustrate how mechanical devices have played a role in human art and entertainment prior to our digital era.",Nicholas Cornia,Nicholas Cornia (Orpheus Instituut)*; Bruno Forment (Orpheus Instituut),"Cornia, Nicholas*; Forment, Bruno",Philosophical and ethical discussions -> philosophical and methodological foundations,Creativity -> computational creativity; Creativity -> humanistic discussions; MIR tasks -> optical music recognition; Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies; Philosophical and ethical discussions -> legal and societal aspects of MIR,,,,,No,https://drive.google.com/file/d/1Wd35AEbNSSAsSknVhi-6OXNFaTHJdnKa/view?usp=sharing,https://drive.google.com/file/d/1oN4xp6Z0jJeApFTE6fruD1CBM_2IjaBQ/view?usp=drive_link,https://drive.google.com/file/d/1tShkawH3_Fc0_5EAGZp7ZfCG9nfmnRfO/view?usp=drive_link,https://drive.google.com/file/d/1OSpNzx0I9AaqIsnNCK-qLTBMmtpAhHGQ/view?usp=drive_link,https://drive.google.com/file/d/1k9Vy-A43gkcP4cm_nvITibfQC_Pz1nMS/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UHELJVTQ,p3-16-who-s-afraid
214,1,1,16,San Francisco,FALSE,Semi-Supervised Piano Transcription Using Pseudo-Labeling Techniques,"Automatic piano transcription (APT) transforms piano recordings into symbolic note events. In recent years, APT has relied on supervised deep learning, which demands a large amount of labeled data that is often limited. This paper introduces a semi-supervised approach to APT, leveraging unlabeled data with techniques originally introduced in computer vision (CV): pseudo-labeling, consistency regularization, and distribution matching. The idea of pseudo-labeling is to use the current model for producing artificial labels for unlabeled data, and consistency regularization makes the model's predictions for unlabeled data robust to augmentations. Finally, distribution matching ensures that the pseudo-labels follow the same marginal distribution as the reference labels, adding an extra layer of robustness. Our method, tested on three piano datasets, shows improvements over purely supervised methods and performs comparably to existing semi-supervised approaches. Conceptually, this work illustrates that semi-supervised learning techniques from CV can be effectively transferred to the music domain, considerably reducing the dependence on large annotated datasets.",Sebastian Strahl,Sebastian Strahl (International Audio Laboratories Erlangen)*; Meinard Müller (International Audio Laboratories Erlangen),"Strahl, Sebastian*; Müller, Meinard",MIR tasks -> music transcription and annotation,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music,,,,,No,https://drive.google.com/file/d/1CHhf2YqFLE4yhviOEnoiWPkquE-MqLBy/view?usp=sharing,https://drive.google.com/file/d/1wLdS2VgE1tyDmEQWvyI0OaVOt-ZAJvL3/view?usp=drive_link,https://drive.google.com/file/d/1qYZX0b77rH_vb-cNHkkztkV7ggpFtA3e/view?usp=drive_link,https://drive.google.com/file/d/1hvjjP5pOg0rHA1HL7ua4uD2K767PJ1_m/view?usp=drive_link,https://drive.google.com/file/d/1DhAU3B4pLBPavXvgRt-b770pUt6p_g49/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UQ1LBAUA,p1-16-semi-supervised-piano
216,4,7,5,San Francisco,FALSE,Towards Assessing Data Replication in Music Generation with Music Similarity Metrics on Raw Audio,"Recent advancements in music generation are raising multiple concerns about the implications of AI in creative music processes, current business models and impacts related to intellectual property management. A relevant discussion and related technical challenge is the potential replication and plagiarism of the training set in AI-generated music, which could lead to misuse of data and intellectual property rights violations. To tackle this issue, we present the Music Replication Assessment (MiRA) tool: a model-independent open evaluation method based on diverse audio music similarity metrics to assess data replication. We evaluate the ability of five metrics to identify exact replication by conducting a controlled replication experiment in different music genres using synthetic samples. Our results show that the proposed methodology can estimate exact data replication with a proportion higher than 10%. By introducing the MiRA tool, we intend to encourage the open evaluation of music-generative models by researchers, developers, and users concerning data replication, highlighting the importance of the ethical, social, legal, and economic consequences. Code and examples are available for reproducibility purposes.",Roser Batlle-Roca,"Roser Batlle-Roca (Universitat Pompeu Fabra)*; Wei-Hsiang Liao (Sony Group Corporation); Xavier Serra (Universitat Pompeu Fabra ); Yuki Mitsufuji (Sony AI); Emilia Gomez (Joint Research Centre, European Commission & Universitat Pompeu Fabra)","Batlle-Roca, Roser*; Liao, Wei-Hsiang; Serra, Xavier; Mitsufuji, Yuki; Gomez, Emilia","Evaluation, datasets, and reproducibility -> evaluation methodology","Evaluation, datasets, and reproducibility -> evaluation metrics; Generative Tasks -> evaluation metrics; MIR tasks -> music generation; MIR tasks -> similarity metrics",,,,,No,https://drive.google.com/file/d/1VxJq1lT9F1gJ64Cvb10en0WeVNUeV0hk/view?usp=sharing,https://drive.google.com/file/d/1TWr7oLMJb2Q7ENmhfPF6rpZeDhwSDqMI/view?usp=sharing,https://drive.google.com/file/d/1WbKAV-UY56QfYuDJhsB4Vzm6A7oUvrp9/view?usp=sharing,https://drive.google.com/file/d/1yVUzCS6SITQJSFhIqk1PjpCsML0cAtko/view?usp=sharing,https://drive.google.com/file/d/1vYgaCU99Gc2jiaA8CW2XRgc09pnag9EI/view?usp=sharing,https://ismir2024.slack.com/archives/C07UHCV5PLN,p7-05-towards-assessing-data
218,3,5,4,San Francisco,FALSE,PolySinger: Singing-Voice to Singing-Voice Translation from English to Japanese,"The speech domain prevails in the spotlight for several natural language processing (NLP) tasks while the singing domain remains less explored. The culmination of NLP is the speech-to-speech translation (S2ST) task, referring to translation and synthesis of human speech. A disparity between S2ST and the possible adaptation to the singing domain, which we describe as singing-voice to singing-voice translation (SV2SVT), is becoming prominent as the former is progressing ever faster, while the latter is at a standstill. Singing-voice synthesis systems are overcoming the barrier of multi-lingual synthesis, despite limited attention has been paid to multi-lingual songwriting and song translation. This paper endeavors to determine what is required for successful SV2SVT and proposes PolySinger (Polyglot Singer): the first system for SV2SVT, performing lyrics translation from English to Japanese. A cascaded approach is proposed to establish a framework with a high degree of control which can potentially diminish the disparity between SV2SVT and S2ST. The performance of PolySinger is evaluated by a mean opinion score test with native Japanese speakers. Results and in-depth discussions with test subjects suggest a solid foundation for SV2SVT, but several shortcomings must be overcome, which are discussed for the future of SV2SVT.",Silas Antonisen,Silas Antonisen (University of Granada)*; Iván López-Espejo (University of Granada),"Antonisen, Silas*; López-Espejo, Iván",Creativity -> tools for artists,"Applications -> music composition, performance, and production; Human-centered MIR -> human-computer interaction; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",,,,,No,https://drive.google.com/file/d/1FHMaLM9kP_efWjgEbKSpw992jVUIfaF6/view?usp=sharing,https://drive.google.com/file/d/19Zc4xVJS-BOs4ZNWClZ5Pd2Z-lCAyiPt/view?usp=sharing,https://drive.google.com/file/d/1zaKJIe_X3jNlitv-0i71A42s2-yyIcN4/view?usp=sharing,https://drive.google.com/file/d/1z7L7IDjLpgsOxxR6AsxUalDx43dLPUDY/view?usp=sharing,https://drive.google.com/file/d/1qb0fegUsc-EIrDKStBAIXaM7VxfTAHge/view?usp=sharing,https://ismir2024.slack.com/archives/C07USGM7B8C,p5-04-polysinger-singing-voice
225,1,2,8,San Francisco,FALSE,DIFF-A-RIFF: MUSICAL ACCOMPANIMENT CO-CREATION VIA LATENT DIFFUSION MODELS,"Recent advancements in deep generative models present new opportunities for music production but also pose challenges, such as high computational demands and limited audio quality. Moreover, current systems frequently rely solely on text input and typically focus on producing complete musical pieces, which is incompatible with existing workflows in music production. To address these issues, we introduce Diff-A-Riff, a Latent Diffusion Model designed to generate high-quality instrumental accompaniments adaptable to any musical context. This model offers control through either audio references, text prompts, or both, and produces 48kHz pseudo-stereo audio while significantly reducing inference time and memory usage. We demonstrate the model's capabilities through objective metrics and subjective listening tests, with extensive examples available on the accompanying website.",Javier Nistal,"Javier Nistal (Sony CSL)*; Marco Pasini (Queen Mary University of London); Cyran Aouameur  (Sony CSL); Stefan Lattner (Sony Computer Science Laboratories, Paris); Maarten Grachten (Machine Learning Consultant)","Nistal, Javier*; Pasini, Marco; Aouameur , Cyran; Lattner, Stefan; Grachten, Maarten",Generative Tasks -> music and audio synthesis,"Applications -> music composition, performance, and production; Creativity -> human-ai co-creativity; Creativity -> tools for artists; Human-centered MIR -> human-computer interaction; MIR tasks -> music generation",,,,,No,https://drive.google.com/file/d/17kD9fcVGogcbAwqArNmF6T8HuJF_OU_5/view?usp=sharing,https://drive.google.com/file/d/1dNMZ76lzlZFS_feWfNRjIv5Vkfx55749/view?usp=sharing,https://drive.google.com/file/d/1QBTWFHw78yg2xAQcWLAIyXrpiDC2L7Rg/view?usp=sharing,https://drive.google.com/file/d/1uMTTqCZzqQnVJHHJGHm0EEFkEUzd60hu/view?usp=sharing,https://drive.google.com/file/d/1mNei697xb4cLMF__7BVSgOl_RRIwWb34/view?usp=sharing,https://ismir2024.slack.com/archives/C07V2MUP4JV,p2-08-diff-a-riff
231,3,6,9,San Francisco,FALSE,DEEP RECOMBINANT TRANSFORMER: ENHANCING LOOP COMPATIBILITY IN DIGITAL MUSIC PRODUCTION,"The widespread availability of music loops has revolutionized music production. However, combining loops requires a nuanced understanding of musical compatibility that can be difficult to learn and time-consuming. This study concentrates on the 'vertical problem' of music loop compatibility, which pertains to layering different loops to create a harmonious blend. The main limitation to applying deep learning in this domain is the absence of a large, high-quality, labeled dataset containing both positive and negative pairs. To address this, we synthesize high-quality audio from multi-track MIDI datasets containing independent instrument stems, and then extract loops to serve as positive pairs. This provides models with instrument-level information when learning compatibility. Moreover, we improve the generation of negative examples by matching the key and tempo of candidate loops, and then employing AutoMashUpper to identify incompatible loops. Creating a large dataset allows us to introduce and examine the application of Transformer architectures for addressing vertical loop compatibility. Experimental results show that our method outperforms the previous state-of-the-art, achieving an 18.6\% higher accuracy across multiple genres. Subjective assessments rate our model higher in seamlessly and creatively combining loops, underscoring our method's effectiveness. We name our approach the Deep Recombinant Transformer and provide audio samples available at: https://conference-demo-2024.github.io/demo/",Muhammad Taimoor Haseeb,Muhammad Taimoor Haseeb (Mohamed bin Zayed University of Artificial Intelligence)*; Ahmad Hammoudeh (Mohamed bin Zayed University of Artificial Intelligence); Gus Xia (Mohamed bin Zayed University of Artificial Intelligence),"Haseeb, Muhammad Taimoor*; Hammoudeh, Ahmad; Xia, Gus",Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,Applications -> music retrieval systems; Creativity -> human-ai co-creativity; Creativity -> tools for artists; MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics; MIR tasks -> automatic classification,,,,,No,https://drive.google.com/file/d/1V7_pi-xl8w6NaSXtx19TnfXuNi70nsF-/view?usp=sharing,https://drive.google.com/file/d/15TzFNZPcOT9P5p_Kqe1lpvfH_j5duHDh/view?usp=sharing,https://drive.google.com/file/d/17LX_eqTjhyJwTVsoLxihZWu02qHnwpnY/view?usp=sharing,https://drive.google.com/file/d/1M3c7JEvlngUOURE-mbKB3vSuUBxqHUE3/view?usp=sharing,https://drive.google.com/file/d/18M1PNiqU2QWLPVl7FznuwZQ9CWSNypW8/view?usp=sharing,https://ismir2024.slack.com/archives/C07U9F71DDM,p6-09-deep-recombinant-transformer
237,3,5,2,San Francisco,FALSE,ComposerX: Multi-Agent Music Generation with LLMs,"Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. Current LLMs often struggle with this task, sometimes generating poorly written music even when equipped with modern techniques like In- Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs’ potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX , an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.",Ruibin Yuan,Qixin Deng (University of Rochester); Qikai Yang (University of Illinois at Urbana-Champaign); Ruibin Yuan (CMU)*; Yipeng Huang  (Multimodal Art Projection Research Community); Yi Wang (CMU); Xubo Liu (University of Surrey); Zeyue Tian (Hong Kong University of Science and Technology); Jiahao Pan (The Hong Kong University of Science and Technology); Ge Zhang (University of Michigan); Hanfeng Lin (Multimodal Art Projection Research Community); Yizhi Li (The University  of Sheffield); Yinghao MA (Queen Mary University of London); Jie Fu (HKUST); Chenghua Lin (University of Manchester); Emmanouil Benetos (Queen Mary University of London); Wenwu  Wang (University of Surrey); Guangyu Xia (NYU Shanghai); Wei Xue (The Hong Kong University of Science and Technology); Yike Guo (Hong Kong University of Science and Technology),"Deng, Qixin; Yang, Qikai; Yuan, Ruibin*; Huang , Yipeng; Wang, Yi; Liu, Xubo; Tian, Zeyue; Pan, Jiahao; Zhang, Ge; Lin, Hanfeng; Li, Yizhi; MA, Yinghao; Fu, Jie; Lin, Chenghua; Benetos, Emmanouil; Wang, Wenwu ; Xia, Guangyu; Xue, Wei; Guo, Yike","Applications -> music composition, performance, and production",Creativity -> computational creativity; Creativity -> human-ai co-creativity,,,,,No,https://drive.google.com/file/d/1d9YrO-EbGtMbuDhXaOVwzRV8Ba-XnfO7/view?usp=sharing,https://drive.google.com/file/d/1APy8gF-5PJjQHv2uslozLB_80fP25UBO/view?usp=sharing,https://drive.google.com/file/d/1EJZNth3SLMRutsH9WVrPghGvzJk21EAK/view?usp=sharing,https://drive.google.com/file/d/1P-CPFLWiu4vrHifYxTl3deiqOazzTsKs/view?usp=sharing,https://drive.google.com/file/d/1CkBxm_QAOYeeLrMlF5ZcLieczShCfu_8/view?usp=share_link,https://ismir2024.slack.com/archives/C07UM60SQ3F,p5-02-composerx-multi-agent
249,3,5,1,San Francisco,TRUE,ST-ITO: Controlling audio effects for style transfer with inference-time optimization,"Audio production style transfer is the task of processing an input recording to impart the stylistic elements from a reference recording. Existing approaches for this task often train a neural network to estimate control parameters for a set of audio effects. However, generalization of these systems is limited due to their reliance on synthetic training data and differentiable audio effects. In this work, we introduce ST-ITO, Style Transfer with Inference-Time Optimization, an approach that instead searches the parameter space of an audio effect chain at inference. This method enables control of arbitrary audio effect chains, including unseen and non-differentiable effects. Our approach employs a learned metric of audio production style, which we train through a simple and scalable self-supervised pretraining strategy, along with a gradient-free optimizer. Due to the limited existing evaluation methods for audio production style transfer, we introduce a four-part benchmark to comprehensively evaluate both audio production style metrics and style transfer systems. This evaluation demonstrates that our approach enables more expressive style transfer and improved generalization, highlighting the limitations of synthetic training data and differentiable audio effects.",Christian J. Steinmetz,Christian J. Steinmetz (Queen Mary University of London)*; Shubhr singh (Queen Mary University of London); Marco Comunita (Queen Mary University of London); Ilias Ibnyahya (Queen Mary University of London); Shanxin Yuan (Queen Mary University of London); Emmanouil Benetos (Queen Mary University of London); Joshua D. Reiss (Queen Mary University of London),"Steinmetz, Christian J.*; singh, Shubhr; Comunita, Marco; Ibnyahya, Ilias; Yuan, Shanxin; Benetos, Emmanouil; Reiss, Joshua D.",MIR fundamentals and methodology -> music signal processing,"Generative Tasks -> transformations; MIR tasks -> music synthesis and transformation; MIR tasks -> similarity metrics; Musical features and properties -> timbre, instrumentation, and singing voice",,,,,Yes,https://drive.google.com/file/d/1UVtuCnE7K0tByHHGlVoMRKldDgf7me1Y/view?usp=sharing,https://drive.google.com/file/d/1LgitojSd5dpzY2_71D0hoErCGiugaeo_/view?usp=sharing,https://drive.google.com/file/d/1cT7XVw1JPxlybJYhaApaFAebYeY3u4l5/view?usp=sharing,https://drive.google.com/file/d/1kTHqdjx8t_lvCG8cGeKZCIZWKenGOl-s/view?usp=sharing,https://drive.google.com/file/d/1HAWETOF_UdkUSe-unEIMhLfok1kgZm_h/view?usp=share_link,https://ismir2024.slack.com/archives/C07V2MV1GN5,p5-01-st-ito-controlling
251,2,3,3,Remote,FALSE,Inner Metric Analysis as a Measure of Rhythmic Syncopation,"Inner Metric Analysis (IMA) is a method for symbolic music analysis that identifies strong and weak metrical positions according to coinciding periodicities within note onsets. These periodicities are visualized with bar graphs known as metric weight and spectral weight profiles. Analyzing these profiles for the presence of syncopation has thus far required manual inspection. In this paper, we propose a simple measure using chi-squared distance for quantifying the level of syncopation found in IMA weight profiles by considering each as a distribution to be compared against (1) a uniform distribution 'nominal' weight profile, and (2) a non-uniform distribution based on beat strength. We apply this measure to the task of predicting perceptual ratings of syncopation using the Song (2014) dataset of 111 single-bar rhythmic patterns and compare its performance to seven existing models of syncopation/complexity. Our results indicate that the proposed measure based on (1) achieves a moderately high Spearman rank correlation (r_s=0.80) to all ratings and is the only single measure that reportedly works across all categories. For so-called polyrhythms in 4/4, the measure based on (2) surpasses all other models and further outperforms five models for monorhythms in 6/8 and three models for monorhythms in 4/4.",Brian Bemman,Brian Bemman (Durham University)*; Justin Christensen (The University of Sheffield),"Bemman, Brian*; Christensen, Justin",Computational musicology,"MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> rhythm, beat, tempo",,,,,No,https://drive.google.com/file/d/1yiNllHxKAmqCiJvBI-FrQHrt_nRxyMno/view?usp=sharing,https://drive.google.com/file/d/1ynvu1NYUiKg5yi1GwiZtL4tpdbDhneeD/view?usp=drive_link,https://drive.google.com/file/d/1H5nfBlCS0ILmBV-1MTpzqmPvfKScKJSW/view?usp=drive_link,https://drive.google.com/file/d/1RryR5iVASqN_UL-UFLDmAS-I8OTV6dQI/view?usp=drive_link,https://drive.google.com/file/d/1xBU1uQWavrsnDfSTheY2I-DBb5Bk5dO3/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UQ1M20JW,p3-03-inner-metric-analysis
254,4,7,9,San Francisco,FALSE,Unsupervised Composable Representations for Audio,"Current generative models are able to generate high-quality artefacts but have been shown to struggle with compositional reasoning, which can be defined as the ability to generate complex structures from simpler elements. In this paper, we focus on the problem of compositional representation learning for music data, specifically targeting the fully-unsupervised setting. We propose a simple and extensible framework that leverages an explicit compositional inductive bias, defined by a flexible auto-encoding objective that can leverage any of the current state-of-art generative models. We demonstrate that our framework, used with diffusion models, naturally addresses the task of unsupervised audio source separation, showing that our model is able to perform high-quality separation. Our findings reveal that our proposal achieves comparable or superior performance with respect to other blind source separation methods and, furthermore, it even surpasses current state-of-art supervised baselines on signal-to-interference ratio metrics. Additionally, by learning an a-posteriori masking diffusion model in the space of composable representations, we achieve a system capable of seamlessly performing unsupervised source separation, unconditional generation, and variation generation. Finally, as our proposal works in the latent space of pre-trained neural audio codecs, it also provides a lower computational cost with respect to other neural baselines.",Giovanni Bindi,Giovanni Bindi (IRCAM)*; Philippe Esling,"Bindi, Giovanni*; Esling, Philippe",Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,Knowledge-driven approaches to MIR -> representations of music; MIR tasks -> music generation; MIR tasks -> music synthesis and transformation; MIR tasks -> sound source separation,,,,,No,https://drive.google.com/file/d/1ztKyMPIcPJo0E72ednIy3TDfApTNd_ag/view?usp=sharing,https://drive.google.com/file/d/1-COJpeJIhvMJlZi-x-_KVgI1j8745Tui/view?usp=sharing,https://drive.google.com/file/d/1DLqRUQZ_YjKq-obUOgLO4XaZ37s1cAGq/view?usp=sharing,https://drive.google.com/file/d/1o1nPt9tNpHYfLZPIl7sndn6JIQLO3Hzd/view?usp=sharing,https://drive.google.com/file/d/1Ok4Wnx9LnuVL0615GNGcaix9_hcwomIt/view?usp=sharing,https://ismir2024.slack.com/archives/C07UQ1M4G2E,p7-09-unsupervised-composable-representations
258,2,3,8,San Francisco,FALSE,Long-form music generation with latent diffusion,"Audio-based generative models for music have seen great strides recently, but so far have not managed to produce full-length music tracks with coherent musical structure from text prompts. We show that by training a generative model on long temporal contexts it is possible to produce long-form music of up to 4m45s. Our model consists of a diffusion-transformer operating on a highly downsampled continuous latent representation (latent rate of 21.5Hz). It obtains state-of-the-art generations according to metrics on audio quality and prompt alignment, and subjective tests reveal that it produces full-length music with coherent structure.",Julian D Parker,Zach Evans (Stability AI); Julian D Parker (Stability AI)*; CJ Carr (Stability AI); Zachary Zuckowski (Stability AI); Josiah Taylor (Stability AI); Jordi Pons (Stability AI),"Evans, Zach; Parker, Julian D*; Carr, CJ; Zuckowski, Zachary; Taylor, Josiah; Pons, Jordi",Generative Tasks -> music and audio synthesis,"Applications -> music composition, performance, and production",,,,,No,https://drive.google.com/file/d/1XSTppMn8KRzwDzMMuHS_K4kW66TswNNt/view?usp=sharing,https://drive.google.com/file/d/1X2HlAu0ftKr0tp09ETAwQScU8pau8mME/view?usp=sharing,https://drive.google.com/file/d/1yrCSDe2DdYEVK95jQy5tNf_4EUX6J6ns/view?usp=sharing,https://drive.google.com/file/d/1H5kCABRvG2SjS2lmwcwqICsJIfE_UlGr/view?usp=sharing,https://drive.google.com/file/d/1y7s0zF1Llt6OAPL1Lab8uvIahq0-kwu_/view?usp=share_link,https://ismir2024.slack.com/archives/C07U9F7GLPR,p3-08-long-form-music
260,3,6,18,San Francisco,FALSE,STONE: Self-supervised tonality estimator,"Although deep neural networks can estimate the key of a musical piece, their supervision incurs a massive annotation effort. Against this shortcoming, we present STONE, the first self-supervised tonality estimator. The architecture behind STONE, named ChromaNet, is a convnet with octave equivalence which outputs a ``key signature profile'' (KSP) of 12 structured logits. First, we train ChromaNet to regress artificial pitch transpositions between any two unlabeled musical excerpts from the same audio track, as measured as cross-power spectral density (CPSD) within the circle of fifths (CoF). We observe that this self-supervised pretext task leads KSP to correlate with tonal key signature. Based on this observation, we extend STONE to output a structured KSP of 24 logits, and introduce supervision so as to disambiguate major versus minor keys sharing the same key signature. Applying different amounts of supervision yields semi-supervised and fully supervised tonality estimators: i.e., Semi-TONEs and Sup-TONEs.  We evaluate these estimators on FMAK, a new dataset of 5489 real-world musical recordings with expert annotation of 24 major and minor keys. We find that Semi-TONE matches the classification accuracy of Sup-TONE with reduced supervision and outperforms it with equal supervision. We observe that this self-supervised pretext task leads PCP to correlate with tonal key signature. Based on this observation, we extend STONE to output a structured PCP of 24 logits, and introduce supervision so as to disambiguate major versus minor keys sharing the same key signature. Applying different amounts of supervision yields semi-supervised and fully supervised tonality estimators: i.e., Semi-TONEs and Sup-TONEs. We evaluate these estimators on FMAK, a new dataset of 5489 real-world musical recordings with expert annotation of 24 major and minor keys. We find that Semi-TONE matches the classification accuracy of Sup-TONE with reduced supervision and outperforms it with equal supervision.",Yuexuan KONG,"Yuexuan KONG (Deezer)*; Vincent Lostanlen (LS2N, CNRS); Gabriel Meseguer Brocal (Deezer); Stella Wong (Columbia University); Mathieu Lagrange (LS2N); Romain Hennequin (Deezer Research)","KONG, Yuexuan*; Lostanlen, Vincent; Meseguer Brocal, Gabriel; Wong, Stella; Lagrange, Mathieu; Hennequin, Romain","Musical features and properties -> harmony, chords and tonality","Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> automatic classification",,,,,No,https://drive.google.com/file/d/1Nhy-9MFVDrdEab7l0P7VndGYameYnDp6/view?usp=sharing,https://drive.google.com/file/d/129ZAwEILwjS7R7PiOMPtnUo7PpjW1Wz3/view?usp=sharing,https://drive.google.com/file/d/1omJMBuixqU0dfrKeN0E2Uev99spaQy1f/view?usp=sharing,https://drive.google.com/file/d/1VoXdy9aSzBbiKFSnFIOe4btBxx1GXjma/view?usp=sharing,https://drive.google.com/file/d/1D5rP1ZGBIiDKSgtOomqCkW5kbFqMpboa/view?usp=sharing,https://ismir2024.slack.com/archives/C07U9F7L28P,p6-18-stone-self-supervised
261,3,6,4,San Francisco,FALSE,Mosaikbox: Improving Fully Automatic DJ Mixing Through Rule-based Stem Modification And Precise Beat-Grid Estimation,"We present a novel system for automatic music mixing combining diverse music information retrieval (MIR) techniques and sources for song selection and transitioning. Specifically, we explore how music source separation and stem analysis can contribute to the task of music similarity calculation by modifying incompatible stems using a rule-based approach and investigate how audio-based similarity measures can be supplemented by lyrics as contextual information to capture more aspects of music. Additionally, we propose a novel approach for tempo detection, outperforming state-of-the-art techniques in low error-tolerance windows. We evaluate our approaches using a listening experiment and compare them to a state-of-the-art model as a baseline. The results show that our approach to automatic song selection and automated music mixing significantly outperforms the baseline and that our rule-based stem removal approach significantly enhances the perceived quality of a mix. No improvement can be observed for the inclusion of contextual information, i.e., mood information derived from lyrics, into the music similarity measure.",Robert Sowula,Robert Sowula (TU Wien)*; Peter Knees (TU Wien),"Sowula, Robert*; Knees, Peter",Creativity -> tools for artists,"Applications -> music recommendation and playlist generation; Creativity -> creative practice involving MIR or generative technology ; Human-centered MIR -> music interfaces and services; MIR tasks -> similarity metrics; Musical features and properties -> rhythm, beat, tempo",,,,,No,https://drive.google.com/file/d/1c_OY_5sI5pR0HLwObXqRUuE8RYipUOpU/view?usp=drive_link,https://drive.google.com/file/d/1GZUMFnXUsYbkjjFZsAJkMCERw70Ra7wJ/view?usp=drive_link,https://drive.google.com/file/d/14PHc1b8L-XG5xKNVewIRd7dXPUUp78Rw/view?usp=drive_link,https://drive.google.com/file/d/1JtyOe4i6Vkr8-5X1FAVecxObcdRjA_8N/view?usp=drive_link,https://drive.google.com/file/d/1LSEw_6G_btzgYHt-UIyVks3V4BZsqy96/view?usp=drive_link,https://ismir2024.slack.com/archives/C07V2MVH8TT,p6-04-mosaikbox-improving-fully
262,1,1,6,San Francisco,FALSE,Classical Guitar Duet Separation using GuitarDuets - a Dataset of Real and Synthesized Guitar Recordings,"Recent advancements in music source separation (MSS) have focused in the multi-timbral case, with existing architectures tailored for the separation of distinct instruments, overlooking thus the challenge of separating instruments with similar timbral characteristics. Addressing this gap, our work focuses on monotimbral MSS, specifically within the context of classical guitar duets. To this end, we introduce the GuitarDuets dataset, featuring a combined total of approximately 3 hours of real and synthesized classical guitar duet recordings, as well as note-level annotations of the synthesized duets. We perform an extensive cross-dataset evaluation by adapting Demucs, a state-of-the-art MSS architecture, to monotimbral source separation. Furthermore, we develop a joint permutation-invariant transcription and separation framework, to exploit note event predictions as auxiliary information. Our results indicate that utilizing both the real and synthesized subsets of GuitarDuets leads to improved separation performance in an independently recorded test set compared to existing multi-track guitar datasets. We also find that while the availability of ground-truth note labels greatly helps the performance of the separation network, the predicted note estimates result only in marginal improvement. Finally, we discuss the suitability of commonly utilized metrics, such as SDR and SI-SDR, in the context of monotimbral MSS.",Marios Glytsos,Marios Glytsos (National Technical University of Athens)*; Christos Garoufis (Athena Research Center); Athanasia Zlatintsi (Athena Research Center); Petros Maragos (National Technical University of Athens),"Glytsos, Marios*; Garoufis, Christos; Zlatintsi, Athanasia; Maragos, Petros","Evaluation, datasets, and reproducibility -> novel datasets and use cases","Evaluation, datasets, and reproducibility -> evaluation metrics; MIR tasks -> music transcription and annotation; MIR tasks -> sound source separation",,,,,No,https://drive.google.com/file/d/1hmHE0nv8wZsj51ajCsSdN_UehrQKrMDt/view?usp=drive_link,https://drive.google.com/file/d/13Iw_NtHQmtN_Z4iend_Py8r4_fUG9F2x/view?usp=drive_link,https://drive.google.com/file/d/1wq-3SzXdmDQmELX5hNjycKnF-RSLLc2_/view?usp=drive_link,https://drive.google.com/file/d/1tT0xYZ7MVSJG6-Dj4asIYA6Ewm_d1oIz/view?usp=drive_link,https://drive.google.com/file/d/1RE4gKJAJxUv6ZmhV-wUQprtQ7AJByOX2/view?usp=drive_link,https://ismir2024.slack.com/archives/C07V2MVK7TK,p1-06-classical-guitar-duet
271,1,2,16,San Francisco,FALSE,From Real to Cloned Singer Identification,"Cloned voices of popular singers sound increasingly realistic and have gained popularity over the past few years. They however pose a threat to the industry due to personality rights concerns. As such, methods to identify the original singer in synthetic voices are needed. In this paper, we investigate how singer identification methods could be used for such a task. We present three embedding models that are trained using a singer-level contrastive learning scheme, where positive pairs consist of segments with vocals from the same singers. These segments can be mixtures for the first model, vocals for the second, and both for the third. We demonstrate that all three models are highly capable of identifying real singers. However, their performance deteriorates when classifying cloned versions of singers in our evaluation set. This is especially true for models that use mixtures as an input. These findings highlight the need to understand the biases that exist within singer identification systems, and how they can influence the identification of voice deepfakes in music.",Dorian Desblancs,Dorian Desblancs (Deezer Research)*; Gabriel Meseguer Brocal (Deezer); Romain Hennequin (Deezer Research); Manuel Moussallam (Deezer),"Desblancs, Dorian*; Meseguer Brocal, Gabriel; Hennequin, Romain; Moussallam, Manuel",MIR tasks -> automatic classification,"Generative Tasks -> music and audio synthesis; MIR fundamentals and methodology -> music signal processing; MIR tasks -> sound source separation; Musical features and properties -> representations of music; Musical features and properties -> timbre, instrumentation, and singing voice",,,,,No,https://drive.google.com/file/d/1okGYXLSh31trEZ8_jDqM_lODG0_TzDNK/view?usp=drive_link,https://drive.google.com/file/d/1JbDn3mE6q9S8gzy0-xgnIrTPbZZ7jPuz/view?usp=drive_link,https://drive.google.com/file/d/1NScXZ7Ut0OslxqE4kqxtDEqZMU6qYr9T/view?usp=drive_link,https://drive.google.com/file/d/1QDJhWyLpD5Oheyeq0z7cFInOtFksVAxr/view?usp=drive_link,https://drive.google.com/file/d/1YhlxNlMwLJEc2m4h41HvHZ6Y_9vR-d2x/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UQ1MJEBC,p2-16-from-real-to
272,3,5,11,San Francisco,FALSE,Lyrics Transcription for Humans: A Readability-Aware Benchmark,"Writing down lyrics for human consumption involves not only accurately capturing word sequences, but also incorporating punctuation and formatting for clarity and to convey contextual information. This includes song structure, emotional emphasis, and contrast between lead and background vocals. While automatic lyrics transcription (ALT) systems have advanced beyond producing unstructured strings of words and are able to draw on wider context, ALT benchmarks have not kept pace and continue to focus exclusively on words. To address this gap, we introduce Jam-ALT, a comprehensive lyrics transcription benchmark. The benchmark features a complete revision of the JamendoLyrics dataset, in adherence to industry standards for lyrics transcription and formatting, along with evaluation metrics designed to capture and assess the lyric-specific nuances, laying the foundation for improving the readability of lyrics. We apply the benchmark to recent transcription systems and present additional error analysis, as well as an experimental comparison with a classical music dataset.",Ondřej Cífka,Ondřej Cífka (AudioShake)*; Hendrik Schreiber (AudioShake); Luke Miner (AudioShake); Fabian-Robert Stöter (AudioShake),"Cífka, Ondřej*; Schreiber, Hendrik; Miner, Luke; Stöter, Fabian-Robert",MIR fundamentals and methodology -> lyrics and other textual data,"Evaluation, datasets, and reproducibility; Evaluation, datasets, and reproducibility -> evaluation metrics; Evaluation, datasets, and reproducibility -> novel datasets and use cases",,,,,No,https://drive.google.com/file/d/18DaiaPQ46l9FPfs_IX5hI12y0WiaaZg0/view?usp=drive_link,https://drive.google.com/file/d/1aO3TyMqL8iw7MRan4-Z4PEMnpLMu7YOy/view?usp=drive_link,https://drive.google.com/file/d/1GbM--7-vFptu10l8mbUFdaf2aYAsuYSy/view?usp=drive_link,https://drive.google.com/file/d/1KeD6qyfmisYMpKuxn_HOed85pqGOg-J2/view?usp=drive_link,https://drive.google.com/file/d/1ById8hvXgqLcK_pK9T63cY_aQpabP0qF/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UQ1MM13L,p5-11-lyrics-transcription-for
273,3,6,2,San Francisco,FALSE,Human Pose Estimation for Expressive Movement Descriptors in Vocal Musical Performance,"Vocal concerts in Indian music are invariably associated with the   performers’ hand gesticulations that are believed to convey emotion, music semantics as well as the individual style of the   performers. Video recordings, with one or more cameras, along with markerless human pose estimation algorithms can be employed to capture such movements, and thus potentially solve music information retrieval (MIR) queries.  Nevertheless, off-the-shelf algorithms are built for the most part for upright human configurations contrasting with seated positions in Indian vocal concerts and the upper body movements in the context of performing music.  Current state-of-the-art algorithms are black box neural network based and this calls for an investigation of the components of such algorithms.  Key decisions involve the choice of one or more cameras, the choice of 2D or 3D features, and relevant parameters. such as confidence thresholds in common machine learning methods. In this paper, we quantify the increase in the performance with 3 cameras on two music information retrieval tasks. We offer insights for single and multi-view processing of videos.",Sujoy Roychowdhury,Sujoy Roychowdhury (Indian Institute of Technology Bombay)*; Preeti Rao (Indian Institute of Technology  Bombay); Sharat Chandran (IIT Bombay),"Roychowdhury, Sujoy*; Rao, Preeti; Chandran, Sharat","Applications -> music videos, multimodal music systems",MIR fundamentals and methodology -> multimodality,,,,,No,https://drive.google.com/file/d/1OXrs5ZPmgM95rN72ylqc5FbHJPEXtAsH/view?usp=drive_link,https://drive.google.com/file/d/1D4GZP-2qjiJbvs8So2KYz19bxlBP0rJy/view?usp=drive_link,https://docs.google.com/presentation/d/1GEkVj9WJaooeTeIAOfutmOe7hqlRTwfk/edit?usp=drive_link&ouid=114902126096780857875&rtpof=true&sd=true,https://drive.google.com/file/d/1u2zPhPPa1LwJJDKbwOmEYtN2ajYkjzak/view?usp=drive_link,https://docs.google.com/presentation/d/1LmjJ_RMIbQYKq0yxjT8iopHT3uqxW5QK/edit?usp=drive_link&ouid=114902126096780857875&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07USGNDRUL,p6-02-human-pose-estimation
275,3,6,11,San Francisco,FALSE,I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition,"Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument recognition. We present an evaluation and analysis of two-tower systems for zero-shot instrument recognition, and a detailed analysis of the properties of the pre-projected and joint embedding spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an existing instrument ontology is proposed. This method reveals deficiencies in the systems' instrumental knowledge and provides evidence of the need for fine-tuning text encoders on musical data.",Yannis Vasilakis,Yannis Vasilakis (Queen Mary University of London)*; Rachel Bittner (Spotify); Johan Pauwels (Queen Mary University of London),"Vasilakis, Yannis*; Bittner, Rachel; Pauwels, Johan",MIR fundamentals and methodology -> multimodality,"Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; MIR tasks -> automatic classification",,,,,No,https://drive.google.com/file/d/1vHktHxDFIuRjWpEIZHFEaWsNpVNc1Ox_/view?usp=drive_link,https://drive.google.com/file/d/17p5K8VOFeqe5La36MwY4hC87sLFS2089/view?usp=drive_link,https://drive.google.com/file/d/1ShVguNWg8DDYlHO5NZP6wZb4i2O-9U6G/view?usp=drive_link,https://drive.google.com/file/d/1Ku9p2tOaHugEDAx1tH0ervgBcpre2E4M/view?usp=drive_link,https://drive.google.com/file/d/174XlTrh62UtEsTEAAqaU58lyZ38Bx4Tw/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UM61SQ85,p6-11-i-can-listen
278,2,4,18,San Francisco,FALSE,Audio Prompt Adapter: Unleashing Music Editing Abilities for Text-to-Music with Lightweight Finetuning,"Recent text-to-music models have enabled users to generate realistic audio music with a simple command. However, editing music audios remains challenging due to conflicting desiderata: performing fine-grained alterations on the audio while maintaining a simplistic user interface. To address this challenge, we propose Audio Prompt Adapter (or AP Adapter), a lightweight addition to pretrained text-to-music models. We utilize AudioMAE to extract features from the input audio, and construct attention-based adapters to feed these features into the internal layers of AudioLDM2, a diffusion text-to-music model. With only 22M trainable parameters, AP Adapter empowers users to harness both global (e.g., style and timbre) and local (e.g., melody) aspects of music, using the original audio and a short text as inputs. Through objective and subjective studies, we evaluate AP Adapter on three tasks: timbre transfer, style transfer, and accompaniment generation. Additionally, we demonstrate its effectiveness on out-of-domain audios containing unseen instruments during training.",Fang Duo Tsai,"Fang Duo Tsai (National Taiwan University)*; Shih-Lun Wu (Carnegie Mellon University); Haven Kim (University of California San Diego); Bo-Yu Chen (National Taiwan University, Rhythm Culture Corporation); Hao-Chung Cheng (National Taiwan University); Yi-Hsuan Yang (National Taiwan University)","Tsai, Fang Duo*; Wu, Shih-Lun; Kim, Haven; Chen, Bo-Yu; Cheng, Hao-Chung; Yang, Yi-Hsuan",MIR tasks -> music synthesis and transformation,Creativity -> computational creativity; Generative Tasks -> music and audio synthesis; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR and machine learning for musical acoustics; MIR tasks -> music generation,,,,,No,https://drive.google.com/file/d/1dDdDgd5TVYl0UorNvzj4f__AftQ9RJRw/view?usp=drive_link,https://drive.google.com/file/d/1PkwkRsbws4zUxxy_vp0pMi6LWueUjzTY/view?usp=drive_link,https://drive.google.com/file/d/13QtDnOhEOCD40rDFaCuChfIwGpIuCqys/view?usp=drive_link,https://drive.google.com/file/d/1QvWRJ4PyIWLrEsovRVVVOJyKHTSIkvWw/view?usp=drive_link,https://drive.google.com/file/d/1oEvkb5vMISIEpt9qz83bL_-02gTSAHoZ/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UM61UR5K,p4-18-audio-prompt-adapter
280,4,7,3,San Francisco,FALSE,Looking for Tactus in All the Wrong Places: Statistical Inference of Metric Alignment in Rap Flow,"Musical rhythm and meter are characterized by simple proportional relationships between event durations within pieces, making comparison of rhythms between different musical pieces a nebulous practice, especially at different tempos. Though the ""main tempo,"" or tactus, of a piece serves as an important cognitive reference point, it is difficult to identify objectively. In this paper, I investigate how statistical regularities in rhythmic patterns can be used to determine how to compare pieces at different tempos, speculating that these regularities could relate to the perception of tactus. Using a Bayesian statistical approach, I model first-order (two-gram) rhythmic event transitions in a symbolic dataset of rap transcriptions (MCFlow), allowing the model to renotate the rhythmic values of each transcription as needed to optimize fit. The resulting model predicts makes ""renotations"" which match a priori predictions from the original dataset's transcriber. I then demonstrate that the model can be used to rhythmically align new data, giving an objective basis for rhythmic annotation decisions.",Nathaniel Condit-Schultz,Nathaniel Condit-Schultz (Georgia Institute of Technology)*,"Condit-Schultz, Nathaniel*",Computational musicology,"Evaluation, datasets, and reproducibility -> annotation protocols; Knowledge-driven approaches to MIR -> cognitive MIR; Knowledge-driven approaches to MIR -> computational music theory and musicology; MIR tasks -> music transcription and annotation; Musical features and properties -> rhythm, beat, tempo",,,,,No,https://drive.google.com/file/d/15s3hLyFhSpdkSLDpThGQ5teU1LuPotu_/view?usp=drive_link,https://drive.google.com/file/d/1SdhM--el63CU_awSBoQ1atUIbIxzplxz/view?usp=drive_link,https://drive.google.com/file/d/1Nrh_OGw3wV0r-3FnXVEaoTD_PFEQXib4/view?usp=drive_link,https://drive.google.com/file/d/12WK4b1sZvF38l3zGomcJAZIixSt8sPvH/view?usp=drive_link,https://drive.google.com/file/d/1P5wiIPAQI9wKRg9ba-3nYxK6ILkZ9POn/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UHCWLQG6,p7-03-looking-for-tactus
283,1,1,19,San Francisco,FALSE,A Contrastive Self-Supervised Learning scheme for beat tracking amenable to few-shot learning,"In this paper, we propose a novel Self-Supervised-Learning scheme to train rhythm analysis systems and instantiate it for few-shot beat tracking.   Taking inspiration from the Contrastive Predictive Coding paradigm, we propose to train a Log-Mel-Spectrogram-Transformer-encoder to contrast observations at times separated by hypothesized beat intervals from those that are not.   We do this without the knowledge of ground-truth tempo or beat positions, as we rely on the local maxima of a Predominant Local Pulse function, considered as a proxy for Tatum positions, to define candidate anchors, candidate positives (located at a distance of a power of two from the anchor) and negatives (remaining time positions).   We show that a model pre-trained using this approach on the unlabeled FMA, MTT and MTG-Jamendo datasets can successfully be fine-tuned in the few-shot regime, i.e. with just a few annotated examples to get a competitive beat-tracking performance.",Antonin Gagneré,"Antonin Gagneré (LTCI - Télécom Paris, IP Paris)*; Slim Essid (  LTCI - Télécom Paris, IP Paris); Geoffroy Peeters (LTCI - Télécom Paris, IP Paris)","Gagneré, Antonin*; Essid, Slim; Peeters, Geoffroy","Musical features and properties -> rhythm, beat, tempo",Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,,,,,No,https://drive.google.com/file/d/1ZCbNqs3QV7vhfH-HGAD6iaFzzZRvch_g/view?usp=drive_link,https://drive.google.com/file/d/1dBlcd_jNwcnBDoKOC1YZOkYYb90I1aSE/view?usp=drive_link,https://drive.google.com/file/d/1RWkcjSZM-yUW2tDEPU8_q42I7t6igv2Z/view?usp=drive_link,https://drive.google.com/file/d/1BPmJ_BnweNHfHsrmFpmCzPF3Q7SdoMh9/view?usp=drive_link,https://drive.google.com/file/d/1CyiIu_sjkW3QO5Hpx3uPd2qiEF0MRVa1/view?usp=drive_link,https://ismir2024.slack.com/archives/C07USGNN5C4,p1-19-a-contrastive-self
293,2,4,11,San Francisco,FALSE,Improved symbolic drum style classification with grammar-based hierarchical representations,"Deep learning models have become a critical tool for analysis and classification of musical data. These models operate either on the audio signal, e.g. waveform or spectrogram, or on a symbolic representation, such as MIDI. In the latter, musical information is often reduced to basic features, i.e. durations, pitches and velocities. Most existing works then rely on generic tokenization strategies from classical natural language processing, or matrix representations, e.g. piano roll. In this work, we evaluate how enriched representations of symbolic data can impact deep models, i.e. Transformers and RNN, for music style classification. In particular, we examine representations that explicitly incorporate musical information implicitly present in MIDI-like encodings, such as rhythmic organization, and show that they outperform generic tokenization strategies. We introduce a new tree-based representation of MIDI data built upon a context-free musical grammar. We show that this grammar representation accurately encodes high-level rhythmic information and outperforms existing encodings on the GrooveMIDI Dataset for drumming style classification, while being more compact and parameter-efficient.",Léo Géré,Léo Géré (Cnam)*; Nicolas Audebert (IGN); Philippe Rigaux (Cnam),"Géré, Léo*; Audebert, Nicolas; Rigaux, Philippe",MIR tasks -> automatic classification,Knowledge-driven approaches to MIR; Knowledge-driven approaches to MIR -> representations of music; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> representations of music,,,,,No,https://drive.google.com/file/d/13ylnqtKDbEBOQxFsCRlpaGPuK-c3RAPH/view?usp=drive_link,https://drive.google.com/file/d/1D8zWhXRLWjWukMzDXT6DLO8UKfg8IaO4/view?usp=drive_link,https://drive.google.com/file/d/1d-G3JwglvXOhAmH8jcKVcKVuC6FRoSND/view?usp=drive_link,https://drive.google.com/file/d/14RvMqnf57mymZFZ5vRsTRTR4aCVFpJ5Z/view?usp=drive_link,https://drive.google.com/file/d/1Mu5h0nYN0QFGlmtpty8vMscvMIXyRI5T/view?usp=drive_link,https://ismir2024.slack.com/archives/C07U9F8BYB1,p4-11-improved-symbolic-drum
296,1,2,6,San Francisco,FALSE,Automatic Estimation of Singing Voice Musical Dynamics,"Musical dynamics form a core part of expressive singing voice performances. However, automatic analysis of musical dynamics for singing voice has received limited attention partly due to the scarcity of suitable datasets and a lack of clear evaluation frameworks. To address this challenge, we propose a methodology for dataset curation. Employing the proposed methodology, we compile a dataset comprising 509 musical dynamics annotated singing voice  performances, aligned with 163 score files, leveraging state-of-the-art source separation and alignment techniques. The scores are sourced from the OpenScore Lieder corpus of romantic-era compositions, widely known for its wealth of expressive annotations. Utilizing the curated dataset, we train a multi-head attention based CNN model with varying window sizes to evaluate the effectiveness of estimating musical dynamics. We explored two distinct perceptually motivated input representations for the model training: log-Mel spectrum and bark-scale based features. For testing, we manually curate another dataset of 25 musical dynamics annotated performances in collaboration with a professional vocalist. We conclude through our experiments that bark-scale based features outperform log-Mel-features for the task of singing voice dynamics prediction. The dataset along with the code is shared publicly for further research on the topic.",Jyoti Narang,Jyoti Narang (Student)*; Nazif Can Tamer (Universitat Pompeu Fabra); Viviana De La Vega (Escola Superior de Música de Catalunya); Xavier Serra (Universitat Pompeu Fabra ),"Narang, Jyoti*; Tamer, Nazif Can; De La Vega, Viviana; Serra, Xavier","Evaluation, datasets, and reproducibility -> annotation protocols",Applications -> music training and education; Knowledge-driven approaches to MIR -> representations of music; MIR tasks -> automatic classification; MIR tasks -> music transcription and annotation; Musical features and properties -> representations of music,,,,,No,https://drive.google.com/file/d/1z976v11q-NVjF0m2I5cFTHqTVSwpA2mb/view?usp=drive_link,https://drive.google.com/file/d/1Rcn5m3PbMitV5QXrqKU9jmk9TG15t9gb/view?usp=drive_link,https://drive.google.com/file/d/191TJ7easeXGMU9D4CsMKxHf4ENkISQfi/view?usp=drive_link,https://drive.google.com/file/d/1tdp3X2iMEOhLyebL7aH4akayiPcMKB9T/view?usp=drive_link,https://docs.google.com/presentation/d/1jVC1MTJWUpRqoY7Y5zt9A04MuIg_gGIC/edit?usp=drive_web&ouid=102045217384450827259&rtpof=true,https://ismir2024.slack.com/archives/C07V2MW9G57,p2-06-automatic-estimation-of
299,2,4,8,San Francisco,FALSE,Diff-MST: Differentiable Mixing Style Transfer,"Mixing style transfer automates the generation of a multitrack mix for a given set of tracks by inferring production attributes from a reference song. However, existing systems for mixing style transfer are limited in that they often operate only on a fixed number of tracks, introduce artifacts, and produce mixes in an end-to-end fashion, without grounding in traditional audio effects, prohibiting interpretability and controllability. To overcome these challenges, we introduce Diff-MST, a framework comprising a differentiable mixing console, a transformer controller, and an audio production style loss function. By inputting raw tracks and a reference song, our model estimates control parameters for audio effects within a differentiable mixing console, producing high-quality mixes and enabling post-hoc adjustments. Moreover, our architecture supports an arbitrary number of input tracks without source labelling, enabling real-world applications. We evaluate our model's performance against robust baselines and showcase the effectiveness of our approach, architectural design, tailored audio production style loss, and innovative training methodology for the given task. We provide code, pre-trained models, and listening examples online.",Soumya Sai Vanka,Soumya Sai Vanka (QMUL)*; Christian J. Steinmetz (Queen Mary University of London); Jean-Baptiste Rolland (Steinberg Media Technologies GmbH); Joshua D. Reiss (Queen Mary University of London); George Fazekas (QMUL),"Vanka, Soumya Sai*; Steinmetz, Christian J.; Rolland, Jean-Baptiste; Reiss, Joshua D.; Fazekas, George",Generative Tasks -> music and audio synthesis,"Applications -> music composition, performance, and production; Generative Tasks -> artistically-inspired generative tasks ; MIR fundamentals and methodology -> music signal processing",,,,,No,https://drive.google.com/file/d/16yvnUj2NZx2-4Dw9HX6OqBemRkT_-NBj/view?usp=drive_link,https://drive.google.com/file/d/1tUSUuB2osgTX5-ME8kf_5SmE-s43mr25/view?usp=drive_link,https://drive.google.com/file/d/1NJEN9xBHhcLPUYWEGRV8LTxD-1p6DzYY/view?usp=drive_link,https://drive.google.com/file/d/14G-uNVzQEu1aG6MEMjq2Kj9DzXXTfka8/view?usp=drive_link,https://drive.google.com/file/d/1NxEVn4ZJsZo-eyzZwbFv8PChPoih99_F/view?usp=drive_link,https://ismir2024.slack.com/archives/C07VCR6DKK2,p4-08-diff-mst-differentiable
304,3,5,16,San Francisco,FALSE,Robust lossy audio compression identification,"Previous research contributions on blind lossy compression identification report near perfect performance metrics on their test set, across a variety of codecs and bit rates. However, we show that such results can be deceptive and may not accurately represent true ability of the system to tackle the task at hand. In this article, we present an investigation into the robustness and generalisation capability of a lossy audio identification model. Our contributions are as follows. (1) We show the lack of robustness to codec parameter variations of a model equivalent to prior art. In particular, when naively training a lossy compression detection model on a dataset of music recordings processed with a range of codecs and their lossless counterparts, we obtain near perfect performance metrics on the held-out test set, but severely degraded performance on lossy tracks produced with codec parameters not seen in training. (2) We propose and show the effectiveness of an improved training strategy to significantly increase the robustness and generalisation capability of the model beyond codec configurations seen during training. Namely we apply a random mask to the input spectrogram to encourage the model not to rely solely on the training set's codec cutoff frequency.",Hendrik Vincent Koops,Hendrik Vincent Koops (Universal Music Group)*; Gianluca Micchi (Universal Music Group); Elio Quinton (Universal Music Group),"Koops, Hendrik Vincent*; Micchi, Gianluca; Quinton, Elio",MIR tasks,MIR tasks -> automatic classification,,,,,No,https://drive.google.com/file/d/1a7Gf7efa-MPguct1Cfnz6Dnqr_adqbMH/view?usp=drive_link,https://drive.google.com/file/d/1X8L6NyjPn0YTHVnGCtWLxm4UCZHDXQlU/view?usp=drive_link,https://drive.google.com/file/d/1azt9YjZHFULXu2BWk6FQ1413hSq3EIIu/view?usp=drive_link,https://drive.google.com/file/d/10BWHAZtK81FDktXcLFQLNUXNaBiBla4J/view?usp=drive_link,https://docs.google.com/presentation/d/1GzoWexmEogTZNgAoxDj6mWmXGlhp6Y6W/edit?usp=drive_link&ouid=102045217384450827259&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07UHEM5UCE,p5-16-robust-lossy-audio
305,1,2,19,San Francisco,FALSE,Which audio features can predict the dynamic musical emotions of both composers and listeners?,"Are composers’ emotional intentions conveyed to listeners through audio features? In the field of Music Emotion Recognition (MER), recent efforts have been made to predict listeners' time-varying perceived emotions using machine-learning models. However, interpreting these models has been challenging due to their black-box nature. To increase the explainability of models for subjective emotional experiences, we focus on composers’ emotional intentions. Our study aims to determine which audio features effectively predict both composers' time-varying emotions and listeners' perceived emotions. Seven composers performed 18 piano improvisations expressing three types of emotions (joy/happiness, sadness, and anger), which were then listened to by 36 participants in a laboratory setting. Both composers and listeners continuously assessed the emotional valence of the music clips on a 9-point scale (1: 'very negative' to 9: 'very positive'). Linear mixed-effect models analysis revealed that listeners significantly perceived the composers' intended emotions. Regarding audio features, the RMS was found to modulate the degree to which the listener's perceived emotion resembled the composer's emotion across all emotions. Moreover, the significant audio features that influenced this relationship varied depending on the emotion type. We propose that audio features related to the emotional responses of composers-listeners can be considered key factors in predicting listeners' emotional responses.",Kyung Myun Lee,Eun Ji Oh (KAIST); Hyunjae Kim (KAIST); Kyung Myun Lee (KAIST)*,"Oh, Eun Ji; Kim, Hyunjae; Lee, Kyung Myun*","Musical features and properties -> musical affect, emotion and mood","Human-centered MIR -> user behavior analysis and mining, user modeling; Human-centered MIR -> user-centered evaluation; Musical features and properties -> expression and performative aspects of music",,,,,No,https://drive.google.com/file/d/1H6zjSdJ59mbyeEbN_4pd_4532fUJzNIU/view?usp=drive_link,https://drive.google.com/file/d/1lBkImXL2vrWGy3IQCwU5_f-SbAoJF3vs/view?usp=sharing,https://drive.google.com/file/d/17rWRaBd_DBuKdxFvvyaH0f9ILzAhRcgv/view?usp=drive_link,https://drive.google.com/file/d/19BjCq9cRrQD_HWM3q-gzMv4wxHcY07r4/view?usp=drive_link,https://drive.google.com/file/d/1E8gdjZPBTeIBCg_auFZAuO5snGzMw23X/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UQ3CHBQA,p2-19-which-audio-features
306,2,4,17,San Francisco,FALSE,Stem-JEPA: A Joint-Embedding Predictive Architecture for Musical Stem Compatibility Estimation,"This paper explores the automated process of determining stem compatibility by identifying audio recordings of single instruments that blend well with a given musical context. To tackle this challenge, we present Stem-JEPA, a novel Joint-Embedding Predictive Architecture (JEPA) trained on a multi-track dataset using a self-supervised learning approach.  Our model comprises two networks: an encoder and a predictor, which are jointly trained to predict the embeddings of compatible stems from the embeddings of a given context, typically a mix of several instruments. Training a model in this manner allows its use in estimating stem compatibility—retrieving, aligning, or generating a stem to match a given mix—or for downstream tasks such as genre or key estimation, as the training paradigm requires the model to learn information related to timbre, harmony, and rhythm.  We evaluate our model’s performance on a retrieval task on the MUSDB18 dataset, testing its ability to find the missing stem from a mix and through a subjective user study. We also show that the learned embeddings capture temporal alignment information and, finally, evaluate the representations learned by our model on several downstream tasks, highlighting that they effectively capture meaningful musical features.",Gaëtan Hadjeres,"Alain Riou (Sony CSL Paris); Stefan Lattner (Sony Computer Science Laboratories, Paris); Gaëtan Hadjeres (Sony CSL)*; Michael Anslow (Sony Computer Science Laboratories, Paris); Geoffroy Peeters (LTCI - Télécom Paris, IP Paris)","Riou, Alain; Lattner, Stefan; Hadjeres, Gaëtan*; Anslow, Michael; Peeters, Geoffroy",Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,Knowledge-driven approaches to MIR -> representations of music; Musical features and properties; Musical features and properties -> representations of music,,,,,No,https://drive.google.com/file/d/1nlqY1ruyjVURT8cD0cH5Avzhdz5JDGQO/view?usp=drive_link,https://drive.google.com/file/d/13Tz5oxvfM9iJaOQQyYS64smicMkURT3e/view?usp=sharing,https://drive.google.com/file/d/1WSTUYMmEIA5COiQycZITf_3HGiPp85Fp/view?usp=drive_link,https://drive.google.com/file/d/1Y6n_osKu4o_VOQizLTcrCaOc-bk_TJbT/view?usp=drive_link,https://docs.google.com/presentation/d/17buqdziLcdsasUgAUhRhJN1aIHpUA0I-i5kGw47npTo/edit?usp=sharing,https://ismir2024.slack.com/archives/C07USGP2VN0,p4-17-stem-jepa-a
315,4,7,2,San Francisco,FALSE,CADENZA: A Generative Framework for Expressive Musical Ideas and Variations,"We introduce Cadenza, a new multi-stage generative framework for predicting expressive variations of symbolic musical ideas as well as unconditional generations. To accomplish this we introduce a novel MIDI encoding method, PerTok (Performance Tokenizer) that captures minute expressive details whilst maintaining short sequence length and vocabulary sizes for polyphonic, monophonic and rhythmic tasks. The proposed framework comprises two sequential stages: 1) Composer and 2) Performer. The Composer model is a transformer-based Variational Autoencoder (VAE), with Rotary Positional Embeddings (RoPE) and an autoregressive decoder modified to more effectively integrate the latent codes of the input musical idea. The Performer model is a bidirectional transformer encoder that is separately trained to predict velocities and microtimings on MIDI sequences. Extensive human evaluations demonstrate Cadenza's versatile capabilities in both meeting and surpassing the musical quality of other state-of-the-art symbolic models in unconditional generation, and secondly, composing new, expressive ideas that are both stylistically related to the input whilst providing novel ideas to the user. Our framework is designed, researched and implemented with the objective of ethically providing inspiration for musicians.",Anirudh Mani,Julian Lenz (Lemonaide ); Anirudh Mani (Lemonaide)*,"Lenz, Julian; Mani, Anirudh*","Applications -> music composition, performance, and production",Creativity -> human-ai co-creativity; Creativity -> tools for artists; Generative Tasks -> artistically-inspired generative tasks ; Generative Tasks -> transformations; Musical features and properties -> representations of music,,,,,No,https://drive.google.com/file/d/1Eg3tB-462vsXvSvZHSwJiBZBjiFeNofw/view?usp=drive_link,https://drive.google.com/file/d/1FWq2xYed6xqkDLhIsrTFfQvf6E2YhRol/view?usp=sharing,https://drive.google.com/file/d/1H-1dKOrp8XcAPWIJTjL0uNCZ7UtzVEOA/view?usp=drive_link,https://drive.google.com/file/d/1cEmfh9XOnZbv6zmv-30c5wj9eqsT_n9c/view?usp=drive_link,https://drive.google.com/file/d/1SQQWcdsjGlY7J2ExkmqTUAOdJvdWleIE/view?usp=drive_link,https://ismir2024.slack.com/archives/C07USJDDWHJ,p7-02-cadenza-a-generative
316,3,5,8,San Francisco,FALSE,Combining audio control and style transfer using latent diffusion,"Deep generative models are now able to synthesize high-quality audio signals, shifting the critical aspect in their development from audio quality to control capabilities. Although text-to-music generation is getting largely adopted by the general public, explicit control and example-based style transfer are more adequate modalities to capture the intents of artists and musicians.   In this paper, we aim to unify explicit control and style transfer within a single model by separating local and global information to capture musical structure and timbre respectively. To do so, we leverage the capabilities of diffusion autoencoders to extract semantic features, in order to build two representation spaces. We enforce disentanglement between those spaces using an adversarial criterion and a two-stage training strategy. Our resulting model can generate audio matching a timbre target, while specifying structure either with explicit controls or through another audio example. We evaluate our model on one-shot timbre transfer and MIDI-to-audio tasks on instrumental recordings and show that we outperform existing baselines in terms of audio quality and target fidelity. Furthermore, we show that our method can generate cover versions of complete musical pieces by transferring rhythmic and melodic content to the style of a target audio in a different genre.",Nils Demerlé,Nils Demerlé (IRCAM)*; Philippe  Esling (IRCAM); Guillaume Doras (Ircam); David Genova (Ircam),"Demerlé, Nils*; Esling, Philippe ; Doras, Guillaume; Genova, David",Generative Tasks -> music and audio synthesis,"Applications -> music composition, performance, and production; Creativity -> tools for artists; Generative Tasks -> artistically-inspired generative tasks ; Generative Tasks -> interactions; Generative Tasks -> transformations",,,,,No,https://drive.google.com/file/d/14OoxZjA_1WmLAWntlWCimXWXb87q8lU_/view?usp=drive_link,https://drive.google.com/file/d/1f9uMz_tRnnTpj-_TFScB7IMZIw4J-4R4/view?usp=drive_link,https://drive.google.com/file/d/1t8X5IzAD2SbYWxEnY8AFEWvUR_rJ7mEy/view?usp=drive_link,https://drive.google.com/file/d/1L4upF6s4rMeCJ-NfTBB5KZ1d3jmsNSQG/view?usp=drive_link,https://docs.google.com/presentation/d/1dThpwBYPxR7kyQICcDmqrZzuckKntKoh/edit?usp=drive_link&ouid=102045217384450827259&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07USGP5JMA,p5-08-combining-audio-control
317,2,3,6,San Francisco,FALSE,The ListenBrainz Listens Dataset,"The ListenBrainz listens dataset is a continually evolv- ing repository of music listening history events submitted by all ListenBrainz users. Currently totalling over 800 million entries, each datum within the dataset encapsu- lates a timestamp, a pseudonymous user identifier, track metadata, and optionally MusicBrainz identifiers facilitat- ing seamless linkage to external resources and datasets. This paper discusses the process of raw data acquisition, the subsequent steps of data synthesis and cleaning, the comprehensive contents of the refined dataset, and the di- verse potential applications of this invaluable resource. Al- though not the largest dataset in terms of music listening events (yet), its distinctiveness lies in its perpetual evolu- tion, with users contributing data daily. This paper under- scores the significance of the ListenBrainz listens dataset as a significant asset for researchers and practitioners alike, offering insights into music consumption patterns, user preferences, and avenues for further exploration in the fields of music information retrieval and recommendation systems.",Kartik Ohri,Kartik Ohri (MetaBrainz Foundation Inc.)*; Robert Kaye (MetaBrainz Foundation Inc.),"Ohri, Kartik*; Kaye, Robert","Evaluation, datasets, and reproducibility -> novel datasets and use cases","Applications -> digital libraries and archives; Applications -> music recommendation and playlist generation; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",,,,,No,https://drive.google.com/file/d/1Dng1gCcWZzhNoohvmfIeV0pjkcpcXvV3/view?usp=drive_link,https://drive.google.com/file/d/1n4It45Un21eMOFehfIeOYa-CPUpipUYH/view?usp=drive_link,https://drive.google.com/file/d/13M9Rm068n5C025eqBMzGE2lj360mL6ur/view?usp=drive_link,https://drive.google.com/file/d/1X4n3yRyuXSuY15IwllmlsgtdrMKk7xIm/view?usp=drive_link,https://drive.google.com/file/d/1d-ot4CXsh94NIqKdz_-QM1Xvm7Augymp/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UPUHDYF5,p3-06-the-listenbrainz-listens
322,3,5,17,San Francisco,FALSE,RNBert: Fine-Tuning a Masked Language Model for Roman Numeral Analysis,"Music is plentiful, but labeled data for music theory tasks like roman numeral analysis is scarce. Self-supervised pretraining on unlabeled data is therefore a promising means of improving performance on these tasks, especially because, during pretraining, a model may be expected to acquire latent representations of musical abstractions like keys and chords. However, existing deep learning models for roman numeral analysis have not used pretraining, instead training from scratch on labeled data, while conversely, pretrained models for music understanding have generally been applied to sequence-level tasks not involving explicit music theory, like composer or genre classification. In contrast, this paper applies pretraining methods to a music theory task by fine-tuning a masked language model, MusicBERT, for roman numeral analysis. We apply token classification to predict labels for each note, then aggregate the predictions of simultaneous notes to obtain a single label at each time step. Conditioning the chord predictions on key predictions gives more coherent labels. The resulting model outperforms previous roman numeral analysis models by a substantial margin.",Malcolm Sailor,Malcolm Sailor (Yale University)*,"Sailor, Malcolm*",Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,"Computational musicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> automatic classification; Musical features and properties -> harmony, chords and tonality",,,,,No,https://drive.google.com/file/d/1Gqyk2QoOrzJXOPd_LBgchAsnBDJxG6cz/view?usp=drive_link,https://drive.google.com/file/d/101uolZhDyW2dpOhoPqlgfgy_uM_kyBIu/view?usp=drive_link,https://drive.google.com/file/d/1shpkSE2PSIr-u-vcBxTryredYcCwPBRu/view?usp=drive_link,https://drive.google.com/file/d/1wLkwNpEi0tli-WbJLSHEaU8B3vOOW9zn/view?usp=drive_link,https://drive.google.com/file/d/1eQ98oLd0j47TFttCKB-4fd2oN6erU1AQ/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UHCXAFE2,p5-17-rnbert-fine-tuning
326,1,1,15,San Francisco,FALSE,Automatic Detection of Moral Values in Music Lyrics,"Moral values play a fundamental role in how we evaluate information, make decisions, and form judgements around important social issues. The possibility to extract morality rapidly from lyrics enables a deeper understanding of our music-listening behaviours. Building on the Moral Foundations Theory (MFT), we tasked a set of transformer-based language models (BERT) fine-tuned on 2,721 synthetic lyrics generated by a large language model (GPT-4) to detect moral values in 200 real music lyrics annotated by two experts. We evaluate their predictive capabilities against a series of baselines including out-of-domain (BERT fine-tuned on MFT-annotated social media texts) and zero-shot (GPT-4) classification. The proposed models yielded the best accuracy across experiments, with an average F1 weighted score of 0.8. This performance is, on average, 5% higher than out-of-domain and zero-shot models. When examining precision in binary classification, the proposed models perform on average 12% higher than the baselines. Our approach contributes to annotation-free and effective lyrics morality learning, and provides useful insights into the knowledge distillation of LLMs regarding moral expression in music, and the potential impact of these technologies on the creative industries and musical culture.",Vjosa Preniqi,Vjosa Preniqi (Queen Mary University of London)*; Iacopo Ghinassi (Queen Mary University of London); Julia Ive (Queen Mary University of London); Kyriaki Kalimeri (ISI Foundation); Charalampos Saitis (Queen Mary University of London),"Preniqi, Vjosa*; Ghinassi, Iacopo; Ive, Julia; Kalimeri, Kyriaki; Saitis, Charalampos",MIR fundamentals and methodology -> lyrics and other textual data,"Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> web mining, and natural language processing",,,,,No,https://drive.google.com/file/d/1nTVfYNUTeLcioLm6T1-A3cCtG9ElEAVF/view?usp=drive_link,https://drive.google.com/file/d/1EfbWaqUcxnOqSdVEvL2HZbeoWe2FN7sA/view?usp=drive_link,https://drive.google.com/file/d/1p0hDd03EtOchCgOQqB92j0bjd7Jy64Zy/view?usp=drive_link,https://drive.google.com/file/d/1Eiz0NkthEE7ejBxQ_Z6CA558SfE8LCbr/view?usp=drive_link,https://docs.google.com/presentation/d/14v-IGWiLgrQDmUQTbK2q1mGxBTdK0SjO/edit?usp=drive_link&ouid=102045217384450827259&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07UHCXC8T0,p1-15-automatic-detection-of
328,1,1,3,San Francisco,FALSE,X-Cover: Better music version identification system by integrating pretrained ASR model,"Methods based on deep learning have emerged as a dominant approach for cover song identification (CSI) literature over the past years, among which ByteCover systems have consistently delivered state-of-the-art performance across major CSI datasets in the field. Despite its steady improvements along previous generations from audio feature dimensionality reduction to short query identification, the system is found to be vulnerable to audios with noise and ambiguous melody when extracting musical information from constant-Q transformation (CQT) spectrograms. Although some recent studies suggest that incorporating lyric-related features can enhance the overall performance of CSI systems, this approach typically requires training a separate automatic lyric recognition (ALR) model to extract lyric-related features from music recordings. In this work, we introduce X-Cover, the latest CSI system that incorporates a pre-trained automatic speech recognition (ASR) module, Whisper, to extract and integrate lyrics-related features into modelling. Specifically, we jointly fine-tune the ASR block and the previous ByteCover3 system in a parameter-efficient fashion, which largely reduces the cost of using lyric information compared to training a new ALR model from scratch. In addition, a bag of tricks is further applied to the training of this new generation, assisting X-Cover to achieve strong performance across various datasets.",Xingjian Du,Xingjian Du (University of Rochester)*; Zou Pei (ByteDance); Mingyu Liu (ByteDance); Xia Liang (Bytedance); Huidong Liang (University of Oxford); Minghang Chu (Bytedance); Zijie Wang (ByteDance); Bilei Zhu (ByteDance AI Lab),"Du, Xingjian*; Pei, Zou; Liu, Mingyu; Liang, Xia; Liang, Huidong; Chu, Minghang; Wang, Zijie; Zhu, Bilei",Applications -> music retrieval systems,MIR tasks -> indexing and querying,,,,,No,https://drive.google.com/file/d/1OYqi9OjSIDEdoCORyvwZtlLMoR06XX7v/view?usp=drive_link,https://drive.google.com/file/d/1qQLxjsDFUM2eXJwZ6G5xujt-sOOJyZds/view?usp=drive_link,https://drive.google.com/file/d/1jFLymUg1dk9CMr4TBpvXjKe-6a_mF6nk/view?usp=drive_link,https://drive.google.com/file/d/1WQoRrfp2BSluPa2zVtN00s9RH1W91rz0/view?usp=drive_link,https://drive.google.com/file/d/1YnJAkXMbqFrAjWsxTwjl2bHm7ga_su6H/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UM62QLTX,p1-03-x-cover-better
333,3,6,1,San Francisco,TRUE,MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models,"Multimodal models that jointly process audio and language hold great promise in audio understanding and are increasingly being adopted in the music domain. By allowing users to query via text and obtain information about a given audio input, these models have the potential to enable a variety of music understanding tasks via language-based interfaces. However, their evaluation poses considerable challenges, and it remains unclear how to effectively assess their ability to correctly interpret music-related inputs with current methods. Motivated by this, we introduce MuChoMusic, a benchmark for evaluating music understanding in multimodal language models focused on audio. MuChoMusic comprises 1,187 multiple-choice questions, all validated by human annotators, on 644 music tracks sourced from two publicly available music datasets, and covering a wide variety of genres. Questions in the benchmark are crafted to assess knowledge and reasoning abilities across several dimensions that cover fundamental musical concepts and their relation to cultural and functional contexts. Through the holistic analysis afforded by the benchmark, we evaluate five open-source models and identify several pitfalls, including an over-reliance on the language modality, pointing to a need for better multimodal integration. Data and code are open-sourced.",Benno Weck,"Benno Weck (Music Technology Group, Universitat Pompeu Fabra (UPF))*; Ilaria Manco (Queen Mary University of London); Emmanouil Benetos (Queen Mary University of London); Elio Quinton (Universal Music Group); George Fazekas (QMUL); Dmitry Bogdanov (Universitat Pompeu Fabra)","Weck, Benno*; Manco, Ilaria; Benetos, Emmanouil; Quinton, Elio; Fazekas, George; Bogdanov, Dmitry","Evaluation, datasets, and reproducibility -> novel datasets and use cases","MIR fundamentals and methodology -> lyrics and other textual data; MIR fundamentals and methodology -> multimodality; MIR fundamentals and methodology -> web mining, and natural language processing",,,,,Yes,https://drive.google.com/file/d/18Sd4DvX2-GSRybRgjk5jRdHNTixovPlX/view?usp=sharing,https://drive.google.com/file/d/1_mWEFUkb6g2TTWt6ZKNBk9KW0dQeIV2C/view?usp=drive_link,https://drive.google.com/file/d/1Au9_Bg1lF0KETm2fsN7GLRHfOStXgpkJ/view?usp=drive_link,https://drive.google.com/file/d/15IJY9NC5AT1ZXABUF19nPDH21FVRClQJ/view?usp=drive_link,https://drive.google.com/file/d/1h7Xi18AmfnJ-B6fCPoLChti337L_JLZG/view?usp=drive_link,https://ismir2024.slack.com/archives/C07V2PM1SP3,p6-01-muchomusic-evaluating-music
334,1,2,9,San Francisco,FALSE,Exploring Internet Radio Across the Globe with the MIRAGE Online Dashboard,"This study presents the Music Informatics for Radio Across the GlobE (MIRAGE) online dashboard, which allows users to access, interact with, and export metadata (e.g., artist name, song title) and musicological features (e.g., instrument list, voice type, key/mode) for 1 million songs streaming on 10,000 internet radio stations across the globe. Users can search for stations or songs according to several criteria, display, analyze, and listen to the selected station/song lists using interactive visualizations that include embedded links to streaming services, and finally export relevant metadata and visualizations for further study.",David Sears,"Ngan V.T. Nguyen (University of Science, Vietnam Nation University Ho Chi Minh City); Elizabeth A.M. Acosta (Texas Tech University); Tommy Dang (Texas Tech University); David R.W. Sears (Texas Tech University)*","Nguyen, Ngan V.T.; Acosta, Elizabeth; Dang, Tommy; Sears, David*",Human-centered MIR -> music interfaces and services,"Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> cognitive MIR; Knowledge-driven approaches to MIR -> computational ethnomusicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies",,,,,No,https://drive.google.com/file/d/1zyarHqwIq80cXUHwfW76uEwoJQEFKOnL/view?usp=sharing,https://drive.google.com/file/d/1l7YN-x0PXcnUosh5EZCv1x4PdzLjDOm6/view?usp=sharing,https://drive.google.com/file/d/1FTAAUOg4remtlKF9jW9kjJfsjP4opVeI/view?usp=sharing,https://drive.google.com/file/d/1aFqRSXlY6EzHKr0Cq_2YpTg7ztRCnA0X/view?usp=sharing,https://drive.google.com/file/d/1o1Vahu6E6_6KpH2icFoYCpR7bQrXW5r8/view?usp=share_link,https://ismir2024.slack.com/archives/C07UQ1NQZMY,p2-09-exploring-internet-radio
335,1,1,12,San Francisco,FALSE,Towards Automated Personal Value Estimation in Song Lyrics,"Most music widely consumed in Western Countries contains song lyrics, with U.S. samples reporting almost all of their song libraries contain lyrics. In parallel, social science theory suggests that personal values - the abstract goals that guide our decisions and behaviors - play an important role in communication: we share what is important to us to coordinate efforts, solve problems and meet challenges. Thus, the values communicated in song lyrics may be similar or different to those of the listener, and by extension affect the listener's reaction to the song. This suggests that working towards automated estimation of values in lyrics may assist in downstream MIR tasks, in particular, personlization. However, as highly subjective text, song lyrics present a challenge in terms of sampling songs to be annotated, annotation methods, and in choosing a method for aggregation. In this project, we take a perspectivist approach, guided by social science theory, to gathering annotations, estimating their quality, and aggregating them. We then compare aggregated ratings to estimates based on pre-trained sentence/word embedding models by employing a validated value dictionary. We discuss conceptually 'fuzzy' solutions to sampling and annotation challenges, promising initial results in annotation quality and in automated estimations, and future directions.",Andrew M. Demetriou,Andrew M. Demetriou (Delft University of Technology)*; Jaehun Kim (Pandora / SiriusXM); Cynthia Liem (Delft University of Technology),"Demetriou, Andrew M.*; Kim, Jaehun; Liem, Cynthia",MIR fundamentals and methodology -> lyrics and other textual data,"Evaluation, datasets, and reproducibility -> novel datasets and use cases",,,,,No,https://drive.google.com/file/d/1UNtJ-Tf6HmkqHZxoN178HyZbGWMMiRLH/view?usp=sharing,https://drive.google.com/file/d/10QODdTfivKPKUHNKsqQHtqYetjjVPpdg/view?usp=share_link,https://drive.google.com/file/d/1RzPyaKKiOooQhJgskFcMbTapg0OzSOeU/view?usp=share_link,https://drive.google.com/file/d/1ZOuRfzixPfpDKnYsXJl4XuO8GV7NeSsh/view?usp=share_link,https://drive.google.com/file/d/1UCq2oLD7HNDWMfAtKGigHhkuNj77vyAh/view?usp=share_link,https://ismir2024.slack.com/archives/C07UHCXJX0E,p1-12-towards-automated-personal
336,3,6,16,San Francisco,FALSE,"Augment, Drop & Swap: Improving Diversity in LLM Captions for Efficient Music-Text Representation Learning","Audio-text contrastive models have become a powerful approach in music representation learning. Despite their empirical success, however, little is known about the influence of key design choices on the quality of music-text representations learnt through this framework. In this work, we expose these design choices within the constraints of limited data and computation budgets, and establish a more solid understanding of their impact grounded in empirical observations along three axes: the choice of base encoders, the level of curation in training data, and the use of text augmentation. We find that data curation is the single most important factor for music-text contrastive training in resource-constrained scenarios. Motivated by this insight, we introduce two novel techniques, Augmented View Dropout and TextSwap, which increase the diversity and descriptiveness of text inputs seen in training. Through our experiments we demonstrate that these are effective at boosting performance across different pre-training regimes, model architectures, and downstream data distributions, without incurring higher computational costs or requiring additional training data.",Ilaria Manco,Ilaria Manco (Queen Mary University of London)*; Justin Salamon (Adobe); Oriol Nieto (Adobe),"Manco, Ilaria*; Salamon, Justin; Nieto, Oriol",Knowledge-driven approaches to MIR -> representations of music,"Applications -> music retrieval systems; Applications -> music videos, multimodal music systems; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> multimodality; MIR fundamentals and methodology -> web mining, and natural language processing",,,,,No,https://drive.google.com/file/d/1QU9KGnGv96vGy6Jvh-OgdfcY2fPMdXHT/view?usp=sharing,https://drive.google.com/file/d/1lqBFpk6ik1wOBxaBmTtFuFqfSnooydtj/view?usp=sharing,https://drive.google.com/file/d/14QtD9aUQH8b5oBxNlgMwsBMZZnfVywFD/view?usp=sharing,https://drive.google.com/file/d/1-ozbdUylrOzfa6KpGiSK5JzLVYCzkie1/view?usp=sharing,https://drive.google.com/file/d/1TTGK2MXwG6RaIMrqkuzohMiodiJHO4Zp/view?usp=sharing,https://ismir2024.slack.com/archives/C07UQ0TV4HH,p6-16-augment-drop-swap
340,3,6,8,San Francisco,FALSE,The Concatenator: A Bayesian Approach To Real Time Concatenative Musaicing,"We present ``The Concatenator,'' a real time system for audio-guided concatenative synthesis. Similarly to Driedger et al.'s ``musaicing'' (or ``audio mosaicing'') technique, we concatenate a set number of windows within a corpus of audio to re-create the harmonic and percussive aspects of a target audio stream. Unlike Driedger's NMF-based technique, however, we instead use an explicitly Bayesian point of view, where corpus window indices are hidden states and the target audio stream is an observation. We use a particle filter to infer the best hidden corpus states in real-time. Our transition model includes a tunable parameter to control the time-continuity of corpus grains, and our observation model allows users to prioritize how quickly windows change to match the target. Because the computational complexity of the system is independent of the corpus size, our system scales to corpora that are hours long, which is an important feature in the age of vast audio data collections. Within The Concatenator module itself, composers can vary grain length, fit to target, and pitch shift in real time while reacting to the sounds they hear, enabling them to rapidly iterate ideas. To conclude our work, we evaluate our system with extensive quantitative tests of the effects of parameters, as well as a qualitative evaluation with artistic insights. Based on the quality of the results, we believe the real-time capability unlocks new avenues for musical expression and control, suitable for live performance and modular synthesis integration, which furthermore represents an essential breakthrough in concatenative synthesis technology.",Christopher J Tralie,Christopher J. Tralie (Ursinus College)*; Ben Cantil (DataMind Audio),"Tralie, Christopher J*; Cantil, Ben",Generative Tasks -> real-time considerations,Creativity -> tools for artists; Generative Tasks -> artistically-inspired generative tasks ; Generative Tasks -> interactions; Generative Tasks -> qualitative evaluations; Generative Tasks -> transformations,,,,,No,https://drive.google.com/file/d/1NZam-5Qy57d8hFXBd-OEOvSP4N9mgsCz/view?usp=sharing,https://drive.google.com/file/d/1Vu1SncA4mhAmcPPw9KQQ-IEsPtSbF-A9/view?usp=sharing,https://drive.google.com/file/d/1ndrkyCDu26e6CgWB--S80RU0a1lj1jsv/view?usp=sharing,https://drive.google.com/file/d/1ArR5u9D4MHZvWG5mFmKd9CtiGnNTchSD/view?usp=sharing,https://drive.google.com/file/d/1xNRQz8Kb5i_OLuStqxrp5WDnyE91lZaI/view?usp=sharing,https://ismir2024.slack.com/archives/C07UQ3D59GS,p6-08-the-concatenator-a
345,4,7,4,San Francisco,FALSE,Exploring GPT's Ability as a Judge in Music Understanding,"Recent progress in text-based Large Language Models (LLMs) and their extended ability to process multi-modal sensory data have led us to explore their applicability in addressing music information retrieval (MIR) challenges. In this paper, we use a systematic prompt engineering approach for LLMs to solve MIR problems. We convert the music data to symbolic inputs and evaluate LLMs' ability in detecting annotation errors in three key MIR tasks: beat tracking, chord extraction, and key estimation. A concept augmentation method is proposed to evaluate LLMs' music reasoning consistency with the provided music concepts in the prompts. Our experiments tested the MIR capabilities of Generative Pre-trained Transformers (GPT). Results show that GPT has an error detection accuracy of 65.20%, 64.80%, and 59.72% in beat tracking, chord extraction, and key estimation tasks, respectively, all exceeding the random baseline. Moreover, we observe a positive correlation between GPT's error finding accuracy and the amount of concept information provided. The current findings based on symbolic music input provide a solid ground for future LLM-based MIR research.",Kun Fang,Kun Fang (McGill University)*; Ziyu Wang (NYU Shanghai); Gus Xia (New York University Shanghai); Ichiro Fujinaga (McGill University),"Fang, Kun*; Wang, Ziyu; Xia, Gus; Fujinaga, Ichiro",Creativity -> creative practice involving MIR or generative technology,"Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> rhythm, beat, tempo",,,,,No,https://drive.google.com/file/d/1faSCXbSbmPlzCZxPF14QMZT8EALvlXf4/view?usp=sharing,https://drive.google.com/file/d/1HUb7E9n3UhZXhSkVTqGJ6sIgazG9fxxv/view?usp=sharing,https://drive.google.com/file/d/1aCrLu7Ribt416pix7FTkXvv5OuzEzgXr/view?usp=sharing,https://drive.google.com/file/d/1vlIVKi9CQqPQwyrkA7BXPjpj-CebbHA7/view?usp=sharing,https://drive.google.com/file/d/1CCEtuzQuuQAtzti_fQaFcaTC_W6_QkoX/view?usp=sharing,https://ismir2024.slack.com/archives/C07V2MX6L3T,p7-04-exploring-gpt-s
347,2,4,16,Remote,FALSE,A Kalman Filter model for synchronization in musical ensembles,"The synchronization of motor responses to rhythmic auditory cues is a fundamental biological phenomenon observed across various species. While the importance of temporal alignment varies across different contexts, achieving precise temporal synchronization is a prominent goal in musical performances. Musicians often incorporate expressive timing variations, which require precise control over timing and synchronization, particularly in ensemble performance. This is crucial because both deliberate expressive nuances and accidental timing deviations can affect the overall timing of a performance. This discussion prompts the question of how musicians adjust their temporal dynamics to achieve synchronization within an ensemble. This paper introduces a novel feedback correction model based on the Kalman Filter, aimed at improving the understanding of interpersonal timing in ensemble music performances. The proposed model performs similarly to other linear correction models in the literature, with the advantage of low computational cost and good performance even in scenarios where the underlying tempo varies.",Hugo T Carvalho,Hugo T. Carvalho (Federal University of Rio de Janeiro)*; Min S. Li (University of Birmingham); Massimiliano Di Luca (University of Birmingham); Alan M. Wing (University of Birmingham),"Carvalho, Hugo T*; Li, Min Susan; Di Luca, Massimiliano; Wing, Alan M.",Musical features and properties,"MIR tasks -> alignment, synchronization, and score following; Musical features and properties -> expression and performative aspects of music; Musical features and properties -> rhythm, beat, tempo",,,,,No,https://drive.google.com/file/d/1xbuQojY3c7U5aBISEmYQbcgdSrHE7IMA/view?usp=sharing,https://drive.google.com/file/d/1YKoc_qGkB-_kV3iYWly-RBG5QGwl3cnE/view?usp=sharing,https://drive.google.com/file/d/1PaMyuIzsGJRpviccxxaaSnHvOku7Wjg3/view?usp=sharing,https://drive.google.com/file/d/1Q_1EwrIcYXlMDQR39wgg7zMQEd4ECInT/view?usp=sharing,https://drive.google.com/file/d/1ZR-utQveISM4ESccDSJah4MYeSL6s4cH/view?usp=sharing,https://ismir2024.slack.com/archives/C07U9F9FQLF,p4-16-a-kalman-filter
352,1,1,2,San Francisco,FALSE,Saraga Audiovisual: a large multimodal open data collection for the analysis of Carnatic Music,"Carnatic music is a style of South Indian art music whose analysis using computational methods is an active area of research in Music Information Research (MIR). A core, open dataset for such analysis is the Saraga dataset, which includes multi-stem audio, expert annotations, and accompanying metadata. However, it has been noted that there are several limitations to the Saraga collections, and that additional relevant aspects of the tradition still need to be covered to facilitate musicologically important research lines. In this work, we present Saraga Audiovisual, a dataset that includes new and more diverse renditions of Carnatic vocal performances, totalling 42 concerts and more than 60 hours of music. A major contribution of this dataset is the inclusion of video recordings for all concerts, allowing for a wide range of multimodal analyses. We also provide high-quality human pose estimation data of the musicians extracted from the video footage, and perform benchmarking experiments for the different modalities to validate the utility of the novel collection. Saraga Audiovisual, along with access tools and results of our experiments, is made available for research purposes.",Adithi Shankar Sivasankar,"Adithi Shankar (Music Technology Group- Universitat Pompeu Fabra)*; Genís Plaja-Roglans (Music Technology Group); Thomas Nuttall (Universitat Pompeu Fabra, Barcelona); Martín Rocamora (Universitat Pompeu Fabra); Xavier Serra (Universitat Pompeu Fabra )","Sivasankar, Adithi Shankar*; Plaja-Roglans, Genís; Nuttall, Thomas; Rocamora, Martín; Serra, Xavier",Computational musicology,"Applications -> music videos, multimodal music systems; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> computational ethnomusicology; MIR tasks -> pattern matching and detection; MIR tasks -> sound source separation",,,,,No,https://drive.google.com/file/d/1EQXLNuxVj3e60R2Eovj8U4saxajWlXac/view?usp=sharing,https://drive.google.com/file/d/1Hn3rqQxlbgnb69IGcw8hZj53CRzNeqYm/view?usp=share_link,https://drive.google.com/file/d/1dSina5Drt9bNlrD4KEr4FXYmpbSTOkyh/view?usp=sharing,https://drive.google.com/file/d/1WLVY2eot7RHjB-PrJXrM5Q-JEQ2kt0uB/view?usp=share_link,https://drive.google.com/file/d/1DNPj6by0BRnlh5Ar6UhcyCuSu7805xwe/view?usp=sharing,https://ismir2024.slack.com/archives/C07V2MXCN65,p1-02-saraga-audiovisual-a
354,3,5,6,San Francisco,FALSE,Sanidha: A Studio Quality Multi-Modal Dataset for Carnatic Music,"Music source separation demixes a piece of music into its individual sound sources (vocals, percussion, melodic instruments, etc.), a task with no simple mathematical solution. It requires deep learning methods involving training on large datasets of isolated music stems. The most commonly available datasets are made from commercial Western music, limiting the models' applications to non-Western genres like Carnatic music. Carnatic music is a live tradition, with the available multi-track recordings containing overlapping sounds and bleeds between the sources. This poses a challenge to commercially available source separation models like Spleeter and Hybrid Demucs. In this work, we introduce Sanidha, the first open-source novel dataset for Carnatic music, offering studio-quality, multi-track recordings with minimal to no overlap or bleed. Along with the audio files, we provide high-definition videos of the artists' performances. Additionally, we fine-tuned Spleeter, one of the most commonly used source separation models, on our dataset and observed improved SDR performance compared to fine-tuning on a pre-existing Carnatic multi-track dataset. The outputs of the fine-tuned model with Sanidha are evaluated through a listening study.",Venkatakrishnan Vaidyanathapuram Krishnan,Venkatakrishnan Vaidyanathapuram Krishnan (Georgia Institute of Technology)*; Noel Alben (Georgia Institute Of Technology); Anish Nair (Georgia Institute of Technology); Nathaniel Condit-Schultz (Georgia Institute of Technology),"Vaidyanathapuram Krishnan, Venkatakrishnan*; Alben, Noel; Nair , Anish  A; Condit-Schultz, Nathaniel","Evaluation, datasets, and reproducibility -> novel datasets and use cases","Evaluation, datasets, and reproducibility -> reproducibility; Knowledge-driven approaches to MIR -> computational ethnomusicology; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> multimodality; MIR tasks -> sound source separation",,,,,No,https://drive.google.com/file/d/1xxZ4RvR9q_4mIj3QBAs3Ey32bl6g2Bqg/view?usp=sharing,https://drive.google.com/file/d/1I5hfnKGshAW6HBgw2YkUG2a9yttdxHN8/view?usp=sharing,https://drive.google.com/file/d/1JuGwSJvFXexbfTXoILktGru4ABzJctxR/view?usp=sharing,https://drive.google.com/file/d/1cs7FlC0Wky9WGM0FiKQaryvD70Q5qXAX/view?usp=sharing,https://drive.google.com/file/d/1cs7FlC0Wky9WGM0FiKQaryvD70Q5qXAX/view?usp=sharing,https://ismir2024.slack.com/archives/C07V2MXEVLH,p5-06-sanidha-a-studio
357,4,7,16,San Francisco,FALSE,Just Label the Repeats for In-The-Wild Audio-to-Score Alignment,"We propose an efficient workflow for high-quality offline alignment of in-the-wild performance audio and corresponding sheet music scans (images). Recent work on audio-to-score alignment extends dynamic time warping (DTW) to be theoretically able to handle jumps in sheet music induced by repeat signs—this method requires no human annotations, but we show that it often yields low-quality alignments. As an alternative, we propose a workflow and interface that allows users to quickly annotate jumps (by clicking on repeat signs), requiring a small amount of human supervision but yielding much higher quality alignments on average. Additionally, we refine audio and score feature representations to improve alignment quality by: (1) integrating measure detection into the score feature representation, and (2) using raw onset prediction probabilities from a music transcription model instead of piano roll. We propose an evaluation protocol for audio-to-score alignment that computes the distance between the estimated and ground truth alignment in units of measures. Under this evaluation, we find that our proposed jump annotation workflow and improved feature representations together improve alignment accuracy by 150% relative to prior work (33% → 82%).",Irmak Bukey,Irmak Bukey (Carnegie Mellon University)*; Michael Feffer (Carnegie Mellon University); Chris Donahue (CMU),"Bukey, Irmak*; Feffer, Michael; Donahue, Chris","MIR tasks -> alignment, synchronization, and score following",MIR fundamentals and methodology -> multimodality; MIR tasks -> optical music recognition; MIR tasks -> pattern matching and detection,,,,,No,https://drive.google.com/file/d/1UvKKzTuptPSWcUG8eLtoRAW27nLrJMCU/view?usp=sharing,https://drive.google.com/file/d/1w4trgsz-MIdKaRspq3X0EEUYFMWbYweP/view?usp=sharing,https://drive.google.com/file/d/10SNBKkgmZ93IS5WNXThWLvpxRXXQpTk_/view?usp=sharing,https://drive.google.com/file/d/11zO68fEWu30iyUTulZET7OrLb0i5Aa32/view?usp=sharing,https://drive.google.com/file/d/1u5GFS8Cpj2YIi7_gpfc6wtmqEshEGfjc/view?usp=sharing,https://ismir2024.slack.com/archives/C07UHEN7T2S,p7-16-just-label-the
360,4,7,17,San Francisco,FALSE,Investigating Time-Line-Based Music Traditions with Field Recordings: A Case Study of Candomblé Bell Patterns,"We introduce a series of transdisciplinary corpus studies aimed at investigating cross-cultural trends in time-line-based music traditions. Our analyses concentrate on a compilation of field recordings from the Centre de Recherche en Ethnomusicologie (CREM) sound archive. To demonstrate the value of an interdisciplinary approach combining ethnomusicology and music information research to rhythmic analysis, we propose a case study on the bell patterns used in the musical practices of Candomblé, an Afro-Brazilian religion. After removing vocals from the recordings with a deep learning source separation technique, we further process the instrumental segments using non-negative matrix factorization and select the bell components. Then, we compute a tempo-agnostic rhythmic feature from the bell track and use it to cluster the data. Finally, we use synthesized patterns from the musicological literature about Candomblé as references to propagate labels to the rhythmic clusters in our data. This semi-supervised approach to pattern analysis precludes the need for downbeat and cycle annotations, making it particularly suited for extensive archive investigations. Lastly, by comparing bell patterns in Candomblé and a West African music tradition, we lay the foundation for our future cross-cultural research and observe the potential application of this methodology to other time-line-based music.",Lucas S Maia,Lucas S Maia (Universidade Federal do Rio de Janeiro)*; Richa Namballa (New York University); Martín Rocamora (Universidad de la República); Magdalena Fuentes (New York University); Carlos Guedes (NYU Abu Dhabi),"Maia, Lucas S*; Namballa, Richa; Rocamora, Martín; Fuentes, Magdalena; Guedes, Carlos",Knowledge-driven approaches to MIR -> computational ethnomusicology,Applications -> music heritage and sustainability; Creativity -> humanistic discussions; MIR tasks -> pattern matching and detection; Musical features and properties -> representations of music; Philosophical and ethical discussions -> legal and societal aspects of MIR,,,,,No,https://drive.google.com/file/d/1FEBBNQF6eV4wxgOTzXmIcyEYtQ9PR0UQ/view?usp=sharing,https://drive.google.com/file/d/1N-NXiV5hUmDU4GbbMMXyN8f7T89Tw8Cv/view?usp=sharing,https://drive.google.com/file/d/1_OYp2YWSaG6tqPTJAoKcLsZSz6QX_VxM/view?usp=sharing,https://drive.google.com/file/d/158UbPnkqeYdglOF6iEf3aoz93GZgaJMq/view?usp=sharing,https://drive.google.com/file/d/1d1BQIVSEZL9v5YxSiOLrksEB_RiP-WO-/view?usp=sharing,https://ismir2024.slack.com/archives/C07VCSXQQTS,p7-17-investigating-time-line
364,1,1,17,San Francisco,FALSE,Note-Level Transcription of Choral Music,"Choral music is a musical activity with one of the largest participant bases, yet it has drawn little attention from automatic music transcription research. The main reasons we argue are due to the lack of data and technical difficulties arise from diverse acoustic conditions and unique properties of choral singing. To address these challenges, in this paper we introduce YouChorale, a novel choral music dataset in a cappella setting curated from the Internet. YouChorale contains 496 real-world recordings in diverse acoustic configurations of choral music from over 100 composers as well as their MIDI scores. In this paper we also propose a Transformer-based framework for note-level transcription of choral music. This framework bypasses the frame-level processing and directly produces a sequence of notes with associated timestamps. Trained on YouChorale, our proposed model achieves state-of-the-art performance in choral music transcription, marking a significant advancement in the field.",Huiran Yu,Huiran Yu (University of Rochester)*; Zhiyao Duan (University of Rochester),"Yu, Huiran*; Duan, Zhiyao",MIR tasks -> music transcription and annotation,"Applications -> music retrieval systems; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",,,,,No,https://drive.google.com/file/d/1Guv-I2ac5iDOWntRs6M5MdZs2FiPLK48/view?usp=drive_link,https://drive.google.com/file/d/1tcFjNk0p0z14T5IvU-1h-kK3Ms3LR3Wq/view?usp=drive_link,https://drive.google.com/file/d/1eKvu4ZSNFuwX4KW91A-KyGFHRCYjjZrt/view?usp=drive_link,https://drive.google.com/file/d/1sTC_5dpgBbtQ6UWcsolKyXFrZwIQlJSp/view?usp=drive_link,https://drive.google.com/file/d/1gEqtWRzJ2Tr4spcHzCNWeT85VDUfKGSO/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UPUJCBEX,p1-17-note-level-transcription
365,1,2,12,San Francisco,FALSE,Transcription-based lyrics embeddings: simple extraction of effective lyrics embeddings from audio,"The majority of Western popular music contains lyrics. Previous studies have shown that lyrics are a rich source of information and are complementary to other information sources, such as audio. One factor that hinders the research and application of lyrics on a large scale is their availability. To mitigate this, we propose the use of transcription-based lyrics embeddings (TLE). These estimate `ground-truth' lyrics embeddings given only audio as input. Central to this approach is the use of transcripts derived from an automatic lyrics transcription (ALT) system instead of human-transcribed, `ground-truth' lyrics, making them substantially more accessible. We conduct an experiment to assess the effectiveness of TLEs across various music information retrieval (MIR) tasks. Our results indicate that TLEs can improve the performance of audio embeddings alone, especially when combined, closing the gap with cases where ground-truth lyrics information is available.",Jaehun Kim,Jaehun Kim (Pandora / SiriusXM)*; Florian Henkel (SiriusXM + Pandora); Camilo Landau (Pandora / SiriusXM); Samuel E. Sandberg (SiriusXM + Pandora); Andreas F. Ehmann (SiriusXM + Pandora),"Kim, Jaehun*; Henkel, Florian; Landau, Camilo; Sandberg, Samuel E.; Ehmann, Andreas F.",MIR fundamentals and methodology -> lyrics and other textual data,Applications -> music recommendation and playlist generation; MIR fundamentals and methodology -> multimodality; MIR tasks -> automatic classification; Musical features and properties -> representations of music,,,,,No,https://drive.google.com/file/d/13bjL3Kt-vHf_PaWqoTbIxu6DNnD7W3PK/view?usp=drive_link,https://drive.google.com/file/d/1hX06JcKiPBAFV2-1O6VsUpYluNbqE6WP/view?usp=drive_link,https://drive.google.com/file/d/1x7bDU2EIR_cbGPvGTegxp6deDMf933BJ/view?usp=drive_link,https://drive.google.com/file/d/1javSHd7p_gz4nV107bjWb4al7WrnltSH/view?usp=drive_link,https://drive.google.com/file/d/1tEjjGQeU7NH_a0gT7Ld5sF1sNTuYdKvK/view?usp=drive_link,https://ismir2024.slack.com/archives/C07USJEDVGC,p2-12-transcription-based-lyrics
366,4,7,8,San Francisco,FALSE,SymPAC: Scalable Symbolic Music Generation With Prompts And Constraints,"Progress in the task of symbolic music generation may be lagging behind other tasks like audio and text generation, in part because of the scarcity of symbolic training data. In this paper, we leverage the greater scale of audio music data by applying pre-trained MIR models (transcription, beat tracking, structure analysis, etc.) to extract symbolic events and encode them into token sequences. To the best of our knowledge, this work is the first to demonstrate the feasibility of training symbolic generation models solely from extensive transcribed audio data. Furthermore, to enhance the controllability for the trained model, we introduce SymPAC (Symbolic Music Language Model with Prompting And Constrained Generation), which is a combination of (a) prompt bars in encoding and (b) a technique called Constrained Generation via Finite State Machines (FSMs) during inference time. We show the flexibility and controllability that this approach affords, which may be critical in making music AI useful to creators and users.",Haonan Chen,Haonan Chen (Bytedance Inc.)*; Jordan B. L. Smith (TikTok); Janne Spijkervet (University of Amsterdam); Ju-Chiang Wang (ByteDance); Pei Zou (Bytedance Inc.); Bochen Li (University of Rochester); Qiuqiang Kong (Byte Dance); Xingjian Du (University of Rochester),"Chen, Haonan*; Smith, Jordan B. L.; Spijkervet, Janne; Wang, Ju-Chiang; Zou, Pei; Li, Bochen; Kong, Qiuqiang; Du, Xingjian",Generative Tasks -> artistically-inspired generative tasks,Creativity -> creative practice involving MIR or generative technology ; Generative Tasks -> evaluation metrics; Generative Tasks -> interactions; Musical features and properties -> representations of music,,,,,No,https://drive.google.com/file/d/1ga7Z_lhyOq_3rckSOfKyss1qmcBSEjfK/view?usp=drive_link,https://drive.google.com/file/d/1F4TgiHVhcLfouWgDset3dnXn8ouKlk6K/view?usp=drive_link,https://drive.google.com/file/d/1eIjaFHViy2t8lRVANMAxMuKMvgJP5TPv/view?usp=drive_link,https://drive.google.com/file/d/1SiKyNUXHZvFjZYzhpmwX1v2gdA_OdOf0/view?usp=drive_link,https://drive.google.com/file/d/1w64Ctpn0OwFb8KVG-Bvk5nwJGbNRjimw/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UM63JPRT,p7-08-sympac-scalable-symbolic
367,3,6,6,San Francisco,FALSE,"A New Dataset, Notation Software, and Representation for Computational Schenkerian Analysis","Schenkerian Analysis (SchA) is a uniquely expressive method of music analysis, combining elements of melody, harmony, counterpoint, and form to describe the hierarchical structure supporting a work of music. However, despite its powerful analytical utility and potential to improve music understanding and generation, SchA has rarely been utilized by the computer music community. This is in large part due to the paucity of available high-quality data in a computer-readable format. With a larger corpus of Schenkerian data, it may be possible to infuse machine learning models with a deeper understanding of musical structure, leading to more ""human"" results.  To encourage further research in Schenkerian analysis and its potential benefits for music informatics and generation, this paper presents three main contributions: 1) a new and growing dataset of SchAs, the largest in human- and computer-readable formats to date (>140 excerpts), 2) a novel software for visualization and collection of SchA data, and 3) a novel, flexible representation of SchA as a heterogeneous-edge graph data structure.",Stephen Hahn,Stephen Hahn (Duke)*; Weihan Xu (duke); Zirui Yin (Duke University); Rico Zhu (Duke University); Simon Mak (Duke University); Yue Jiang (Duke University); Cynthia Rudin (Duke),"Hahn, Stephen*; Xu, Weihan; Yin, Zirui; Zhu, Rico; Mak, Simon; Jiang, Yue; Rudin, Cynthia","Evaluation, datasets, and reproducibility -> novel datasets and use cases","Human-centered MIR -> music interfaces and services; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> structure, segmentation, and form",,,,,No,https://drive.google.com/file/d/19RtCI-wJZy7RKj7oEz7gD-94TMkdRPIb/view?usp=drive_link,https://drive.google.com/file/d/1mx1xF6y1fZBStu2DGS_5gp6RSQ9qIIUX/view?usp=drive_link,https://drive.google.com/file/d/1ME_Waz-jgKrb5fOavvJYK97jwteeVLYf/view?usp=drive_link,https://drive.google.com/file/d/18VRICoSc5LG6v_g1NaXDOciRyR9IBUa0/view?usp=drive_link,https://drive.google.com/file/d/1vM09esMw8Abv2UXOTDjog0wHD81FdoR-/view?usp=drive_link,https://ismir2024.slack.com/archives/C07VCR7PY6L,p6-06-a-new-dataset
373,3,6,3,San Francisco,FALSE,Enhancing predictive models of music familiarity with EEG: Insights from fans and non-fans of K-pop group NCT127,"Predicting a listener’s experience of music based solely on audio features has its limitations due to the individual variability in responses to the same music. This study examines the effectiveness of electroencephalogram (EEG) in predicting the subjective experiences while listening to music, including arousal, valence, familiarity, and preference. We collected EEG data alongside subjective ratings of arousal, valence, familiarity, and preference from both fans (N=20) and non-fans (N=34) of the K-pop idol group, NCT127 to investigate response variability to the same NCT127 music. Our analysis focused on determining whether the inclusion of EEG alongside audio features could enhance the predictive power of linear mixed-effects models for these subjective ratings. Specifically, we employed stimulus-response correlation (SRC), a recent approach in neuroscience correlating stimulus features with EEG responses to the ecologically valid stimuli. The results showed that familiarity and preference was significantly higher in the fan group. Furthermore, the inclusion of SRC significantly enhanced the prediction of familiarity compared to models based solely on audio features. However, the impact of SRC on predictions of arousal and valence exhibited variation depending on the correlated audio features, with certain SRCs improving predictions while others diminished them. For preference, only a few SRCs negatively affected model performance. These results suggest that correlations of EEG responses and audio features can provide information of individual listeners’ subjective responses, particularly in predicting familiarity.",Kyung Myun Lee,Seokbeom Park (KAIST); Hyunjae Kim (KAIST); Kyung Myun Lee (KAIST)*,"Park, Seokbeom; Kim, Hyunjae; Lee, Kyung Myun*",Musical features and properties,"Human-centered MIR -> user behavior analysis and mining, user modeling; Musical features and properties -> musical affect, emotion and mood",,,,,No,https://drive.google.com/file/d/1If3mdYW1r3a--u0ZgAKO4ztpln3Ob5Wy/view?usp=drive_link,https://drive.google.com/file/d/1qcY7f82eJ1PSS8UMGiZYx1joANjzfeXS/view?usp=sharing,https://drive.google.com/file/d/1jE0qVn5E-CRo04kFo1oEt8aqvN64Alex/view?usp=drive_link,https://drive.google.com/file/d/1tyyYBiGqaYePfWS9SDsJ66i-tYDGNgC4/view?usp=drive_link,https://drive.google.com/file/d/11UhMhN2qgEJTTVNFRDhsTn9XMRCKJnLo/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UHENKTPG,p6-03-enhancing-predictive-models
374,2,3,12,San Francisco,FALSE,Mel-RoFormer for Vocal Separation and Vocal Melody Transcription,"Developing a versatile deep neural network to model music audio is crucial in MIR. This task is challenging due to the intricate spectral variations inherent in music signals, which convey melody, harmonics, and timbres of diverse instruments. In this paper, we introduce Mel-RoFormer, a spectrogram-based model featuring two key designs: a novel Mel-band Projection module at the front-end to enhance the model's capability to capture informative features across multiple frequency bands, and interleaved RoPE Transformers to explicitly model the frequency and time dimensions as two separate sequences. We apply Mel-RoFormer to tackle two essential MIR tasks: vocal separation and vocal melody transcription, aimed at isolating singing voices from audio mixtures and transcribing their lead melodies, respectively. Despite their shared focus on singing signals, these tasks possess distinct optimization objectives. Instead of training a unified model, we adopt a two-step approach. Initially, we train a vocal separation model, which subsequently serves as a foundation model for fine-tuning for vocal melody transcription. Through extensive experiments conducted on benchmark datasets, we showcase that our models achieve state-of-the-art performance in both vocal separation and melody transcription tasks, underscoring the efficacy and versatility of Mel-RoFormer in modeling complex music audio signals.",Ju-Chiang Wang,Ju-Chiang Wang (ByteDance)*; Wei-Tsung Lu (New Your University); Jitong Chen (ByteDance),"Wang, Ju-Chiang*; Lu, Wei-Tsung; Chen, Jitong",MIR fundamentals and methodology -> music signal processing,MIR tasks -> music transcription and annotation; MIR tasks -> sound source separation,,,,,No,https://drive.google.com/file/d/1XEo3yfUAx6ISDCYN67ygpMc6zSu4FY8S/view?usp=drive_link,https://drive.google.com/file/d/1gARjs4Ww0qBo9NH-c1o0Ue9jT70g1rya/view?usp=drive_link,https://drive.google.com/file/d/1ni7Grl9p8OQYLbsJHJJk3R-nOcURBOR0/view?usp=drive_link,https://drive.google.com/file/d/1_qodZGdIbsylkBF8QUBS4Lpbb9UbF8DV/view?usp=drive_link,https://drive.google.com/file/d/15c4-i63qh70vDjfnqBCyWbm8ezH4q9nV/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UM63P4TF,p3-12-mel-roformer-for
376,1,2,17,San Francisco,FALSE,Emotion-driven Piano Music Generation via Two-stage Disentanglement and Functional Representation,"Managing the emotional aspect remains a challenge in automatic music generation. Prior works aim to learn various emotions at once, leading to inadequate modeling. This paper explores the disentanglement of emotions in piano performance generation through a two-stage framework. The first stage focuses on valence modeling of lead sheet, and the second stage addresses arousal modeling by introducing performance-level attributes. To further capture features that shape  valence, an aspect less explored by previous approaches, we introduce a novel functional representation of symbolic music. This representation aims to capture the emotional impact of major-minor tonality, as well as the interactions among notes, chords, and key signatures. Objective and subjective experiments validate the effectiveness of our framework in both emotional valence and arousal modeling. We further leverage our framework in a novel application of emotional controls, showing a broad potential in emotion-driven music generation.",Jingyue Huang,Jingyue Huang (New York University)*; Ke Chen (University of California San Diego); Yi-Hsuan Yang (National Taiwan University),"Huang, Jingyue*; Chen, Ke; Yang, Yi-Hsuan",MIR tasks -> music generation,"Musical features and properties -> expression and performative aspects of music; Musical features and properties -> musical affect, emotion and mood",,,,,No,https://drive.google.com/file/d/1DD8pEMU4ladZZ6I-uhE5aa7qkGikQNqf/view?usp=sharing,https://drive.google.com/file/d/17ZHRJcK-nu5-nYVTEHhnQhYz70-up6uC/view?usp=sharing,https://drive.google.com/file/d/1u_bBXAgQDTiTWfHiEhRCyaPDXZk7R4iM/view?usp=drive_link,https://drive.google.com/file/d/1QYtTDuGUT2ZvswQDMTF-TTzoxf9HuYIy/view?usp=drive_link,https://drive.google.com/file/d/1tzunUpQS5oKD6-ed2-zM5umz3uwYolfY/view?usp=drive_link,https://ismir2024.slack.com/archives/C07V2MXSA49,p2-17-emotion-driven-piano
382,4,7,1,San Francisco,TRUE,Scoring Time Intervals Using Non-Hierarchical Transformer for Automatic Piano Transcription,"The neural semi-Markov Conditional Random Field (semi-CRF) framework has demonstrated promise for event-based piano transcription. In this framework, all events (notes or pedals) are represented as closed time intervals tied to specific event types. The neural semi-CRF approach requires an interval scoring matrix that assigns a score for every candidate interval. However, designing an efficient and expressive architecture for scoring intervals is not trivial. This paper introduces a simple method for scoring intervals using scaled inner product operations that resemble how attention scoring is done in transformers. We show theoretically that, due to the special structure from encoding the non-overlapping intervals, under a mild condition, the inner product operations are expressive enough to represent an ideal scoring matrix that can yield the correct transcription result. We then demonstrate that an encoder-only structured non-hierarchical transformer backbone, operating only on a low-time-resolution feature map, is capable of transcribing piano notes and pedals with high accuracy and time precision. The experiment shows that our approach achieves the new state-of-the-art performance across all subtasks in terms of the F1 measure on the Maestro dataset.",Yujia Yan,Yujia Yan (University of Rochester)*; Zhiyao Duan (University of Rochester),"Yan, Yujia*; Duan, Zhiyao",MIR tasks -> music transcription and annotation,MIR fundamentals and methodology -> music signal processing,,,,,Yes,https://drive.google.com/file/d/1gMc5ubAqdEcvUbcer9u5d59u_yi014t3/view?usp=sharing,https://drive.google.com/file/d/11_86V7lHMEzbzDg9dBcqwO5L4DNtMv6D/view?usp=drive_link,https://drive.google.com/file/d/1ArJhcjqNBA7D00oz8_Hs7DX0h0o9BNsB/view?usp=drive_link,https://drive.google.com/file/d/1EnZC4AvxXCxtBRES8CQ_H1OUJCyng-Bk/view?usp=drive_link,https://drive.google.com/file/d/1QcH4JRA3OK9Rk0aRoPN6whKUCGUyvoEh/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UM63SRNH,p7-01-scoring-time-intervals
396,3,6,17,San Francisco,FALSE,Music Discovery Dialogue Generation Using Human Intent Analysis and Large Language Model,"A conversational music retrieval system can help users discover music that matches their preferences through dialogue. To achieve this, a conversational music retrieval system should seamlessly engage in multi-turn conversation by 1) understanding user queries and 2) responding with natural language and retrieved music. A straightforward solution would be a data-driven approach utilizing such conversation logs. However, few datasets are available for the research and are limited in terms of volume and quality. In this paper, we present a data generation framework for rich music discovery dialogue using a large language model (LLM) and user intents, system actions, and musical attributes. This is done by i) dialogue intent analysis using grounded theory, ii) generating attribute sequences via cascading database filtering, and iii) generating utterances using large language models. By applying this framework to the Million Song dataset, we create -- LP-MusicDialog, a Large Language Model based Pseudo Music Dialogue dataset, containing over 288k music conversations using more than 319k music items. Our evaluation shows that the synthetic dataset is competitive with an existing, small human dialogue dataset in terms of dialogue consistency, item relevance, and naturalness. Furthermore, using the dataset, we train a conversational music retrieval model and show promising results",Seungheon Doh,Seungheon Doh (KAIST)*; Keunwoo Choi (Genentech); Daeyong Kwon (KAIST); Taesoo Kim (KAIST); Juhan Nam (KAIST),"Doh, Seungheon*; Choi, Keunwoo; Kwon, Daeyong; Kim, Taesoo; Nam, Juhan",MIR tasks -> indexing and querying,"MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",,,,,No,https://drive.google.com/file/d/11H7CUJNQ0ZKToLGjuvm46d9q0aGfxQyq/view?usp=sharing,https://drive.google.com/file/d/10AwIg1xgUeN5UzeT-69VoKQdXn-ZLa5l/view?usp=sharing,https://drive.google.com/file/d/1gF09iN985cGJMCuVSXhldyBnfCqODT_B/view?usp=drive_link,https://drive.google.com/file/d/1giCjNyrAIR16lhNMkcIREsY5JB3qf3xA/view?usp=drive_link,https://drive.google.com/file/d/1OyXuGRDLjmx6__EEPAsKY8Gl9bnaAhFV/view?usp=drive_link,https://ismir2024.slack.com/archives/C07U9FA5DMM,p6-17-music-discovery-dialogue
405,1,1,20,San Francisco,FALSE,Using Pairwise Link Prediction and Graph Attention Networks for Music Structure Analysis,"The task of music structure analysis has been mostly addressed as a sequential problem, by relying on the internal homogeneity of musical sections or their repetitions. In this work, we instead regard it as a pairwise link prediction task. If for any pair of time instants in a track, one can successfully predict whether they belong to the same structural entity or not, then the underlying structure can be easily recovered. Building upon this assumption, we propose a method that first learns to classify pairwise links between time frames as belonging to the same section (or segment) or not. The resulting link features, along with node-specific information, are combined through a graph attention network. The latter is regularized with a graph partitioning training objective and outputs boundary locations between musical segments and section labels. The overall system is lightweight and performs competitively with previous methods. The evaluation is done on two standard datasets for music structure analysis and an ablation study is conducted in order to gain insight on the role played by its different components.",Morgan Buisson,Morgan Buisson (Telecom-Paris)*; Brian McFee (New York University); Slim Essid (Telecom Paris - Institut Polytechnique de Paris),"Buisson, Morgan*; McFee, Brian; Essid, Slim","Musical features and properties -> structure, segmentation, and form",,,,,,No,https://drive.google.com/file/d/1J7VODLY0uirvo78S5oTCZ2Sj1uZWwc8H/view?usp=sharing,https://drive.google.com/file/d/1ItBVYLES5QsKMp0kVJ6DAfRvT3JgHrqa/view?usp=drive_link,https://drive.google.com/file/d/14wfben86kWD1tsG3bRS-8e_RcM5kjQ6V/view?usp=sharing,https://drive.google.com/file/d/1DZNsWdPBl_rb51o_SxePiDnzqOt7bPY4/view?usp=drive_link,https://drive.google.com/file/d/1rzQS2PNwsEQ17yyUXLO2CHrnRuhHeraG/view?usp=drive_link,https://ismir2024.slack.com/archives/C07V2MY243B,p1-20-using-pairwise-link
510,1,1,10,San Francisco,FALSE,Selective Annotation of Few Data for Beat Tracking of Latin American Music Using Rhythmic Features,,Lucas S. Maia,"Lucas S. Maia, Martín Rocamora, Luiz W. P. Biscainho, Magdalena Fuentes*","Lucas S. Maia, Martín Rocamora, Luiz W. P. Biscainho, Magdalena Fuentes",,,,,,,No,https://transactions.ismir.net/articles/170/files/66434fd260250.pdf,https://drive.google.com/file/d/1EoxlhQcN2jUC6ZIATBrke-TrjLdZOk1B/view?usp=drive_link,https://drive.google.com/file/d/1Mu4TZR7CyZvRZ-fBeQhDMZbxingjkQFr/view?usp=drive_link,https://drive.google.com/file/d/1XVG8U5QsIu466JvgbKwrd3neXkqM5SnB/view?usp=drive_link,https://drive.google.com/file/d/1JiyPGub_pGdn7jn0mYMeDt-9Xa_tVHdg/view?usp=drive_link,https://ismir2024.slack.com/archives/C07UQ1Q1WE6,p1-10-selective-annotation-of
511,1,2,10,San Francisco,FALSE,The Sound Demixing Challenge 2023 – Cinematic Demixing Track,,Stefan Uhlich,"Stefan Uhlich, Giorgio Fabbro*, Masato Hirano, Shusuke Takahashi, Gordon Wichern, Jonathan Le Roux, Dipam Chakraborty, Sharada Mohanty, Kai Li, Yi Luo, Jianwei Yu, Rongzhi Gu, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Mikhail Sukhovei, Yuki Mitsufuji","Stefan Uhlich, Giorgio Fabbro, Masato Hirano, Shusuke Takahashi, Gordon Wichern, Jonathan Le Roux, Dipam Chakraborty, Sharada Mohanty, Kai Li, Yi Luo, Jianwei Yu, Rongzhi Gu, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Mikhail Sukhovei, Yuki Mitsufuji",,,,,,,No,https://transactions.ismir.net/articles/172/files/661fc9bdf1264.pdf,https://drive.google.com/file/d/1BuSK6EAat0nq9HwatIOL6XnZ30CJe8Uy/view?usp=drive_link,https://drive.google.com/file/d/1nlSc9U3nQnb0YIbopOuJ9zJRorr_g7-J/view?usp=drive_link,https://drive.google.com/file/d/1CDwFB9U5aO_1ZaCjtAgaPFhiiqn9Dwck/view?usp=drive_link,https://drive.google.com/file/d/1_J6DSgoiFZ64pLcD1nX1ceDjCuVe7GAU/view?usp=drive_link,https://ismir2024.slack.com/archives/C07U9H0EK2B,p2-10-the-sound-demixing
512,2,3,10,San Francisco,FALSE,Piano Concerto Dataset (PCD): A Multitrack Dataset of Piano Concertos,,Yigitcan Özer,"Yigitcan Özer, Simon Schwär, Vlora Arifi-Müller, Jeremy Lawrence, Emre Sen, and Meinard Müller*","Yigitcan Özer, Simon Schwär, Vlora Arifi-Müller, Jeremy Lawrence, Emre Sen, and Meinard Müller",,,,,,,No,https://transactions.ismir.net/articles/160/files/6501b44ff32d9.pdf,https://drive.google.com/file/d/1Sa42785sl1JOyLRjWGbr4ACCeY43lp1A/view?usp=drive_link,https://drive.google.com/file/d/1kIG7BA9W4UW50walYN7YM4-XdJ2BUnWq/view?usp=drive_link,https://drive.google.com/file/d/1ruV3sh4cOKTG8JRcs2HYbyVsvfbKOHhH/view?usp=drive_link,https://docs.google.com/presentation/d/13tKuq2yBxxhxBeq8PDaYDSFwCvOru4qR/edit?usp=drive_link&ouid=117255166438725489403&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07USJF0M8C,p3-10-piano-concerto-dataset
513,2,4,10,San Francisco,FALSE,A Dataset of Larynx Microphone Recordings for Singing Voice Reconstruction,,Simon Schwär,"Simon Schwär*, Michael Krause, Michael Fast, Sebastian Rosenzweig, Frank Scherbaum, Meinard Müller","Simon Schwär, Michael Krause, Michael Fast, Sebastian Rosenzweig, Frank Scherbaum, Meinard Müller",,,,,,,No,https://transactions.ismir.net/articles/166/files/65d89725d616e.pdf,https://drive.google.com/file/d/1Kri3iUu08qgw67fAQw4VzPZXXzTxuGcB/view?usp=drive_link,https://drive.google.com/file/d/1CYaVjfKHx3wKkEBvak4wmi4tq-YXcEit/view?usp=drive_link,https://drive.google.com/file/d/1enDWiGur_9rUoPcshOAsAKkfaUjz5XOV/view?usp=drive_link,https://docs.google.com/presentation/d/1wFcNmzPu6eSG2B8fYY4k5PW4q7uYFiRu/edit?usp=drive_link&ouid=117255166438725489403&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07U9FACZDM,p4-10-a-dataset-of
514,3,5,10,San Francisco,FALSE,Wagner Ring Dataset: A Complex Opera Scenario for Music Processing and Computational Musicology,,Christof Weiß,"Christof Weiß, Vlora Arifi-Müller*, Michael Krause, Frank Zalkow, Stephanie Klauk, Rainer Kleinertz, and Meinard Müller","Christof Weiß, Vlora Arifi-Müller, Michael Krause, Frank Zalkow, Stephanie Klauk, Rainer Kleinertz, and Meinard Müller",,,,,,,No,https://transactions.ismir.net/articles/161/files/653900bdd2cd7.pdf,https://drive.google.com/file/d/10V8FLOa9-pdpmTe-rrlAN8AEo8poxBiY/view?usp=drive_link,https://drive.google.com/file/d/1sh50djvbVTCB05xMBW0jjxO3ecRWoHjp/view?usp=drive_link,https://drive.google.com/file/d/1vBUJ9LxCcaOu8fUhDEDcpLypQS9lFl8u/view?usp=drive_link,https://docs.google.com/presentation/d/1D2d8tO2G8j9rG6DSIkPX2AeEg1_9QF5k/edit?usp=drive_link&ouid=117255166438725489403&rtpof=true&sd=true,https://ismir2024.slack.com/archives/C07UHCYTLQN,p5-10-wagner-ring-dataset
515,3,6,10,San Francisco,FALSE,Repertoire-Specific Vocal Pitch Data Generation for Improved Melodic Analysis of Carnatic Music,,Genís Plaja-Roglans,"Genís Plaja-Roglans*, Thomas Nuttall, Lara Pearson, Xavier Serra, Marius Miron","Genís Plaja-Roglans, Thomas Nuttall, Lara Pearson, Xavier Serra, Marius Miron",,,,,,,No,https://transactions.ismir.net/articles/137/files/6499954ec1b0b.pdf,https://drive.google.com/file/d/14GqI2ULCkTiWLrm3iUpAdkqLV0t4fyW6/view?usp=drive_link,https://drive.google.com/file/d/1c0Csr3IZrEEsF8klvIve7SypcBMUssWL/view?usp=drive_link,https://drive.google.com/file/d/1kxxmWxBNn2foiu-p7gMGOiR37tnskogh/view?usp=drive_link,https://drive.google.com/file/d/1ilG8WsxGi_SylgfFe4mqz11ihjeuG3mU/view?usp=drive_link,https://ismir2024.slack.com/archives/C07VCR8EVC0,p6-10-repertoire-specific-vocal
516,4,7,10,San Francisco,FALSE,CCOM-HuQin: An Annotated Multimodal Chinese Fiddle Performance Dataset,,Yu Zhang,"Yu Zhang, Ziya Zhou*, Xiaobing Li, Feng Yu, Maosong Sun","Yu Zhang, Ziya Zhou, Xiaobing Li, Feng Yu, Maosong Sun",,,,,,,No,https://transactions.ismir.net/articles/146/files/64aea0bb77021.pdf,https://drive.google.com/file/d/1Bu5VrSktbhzFZoOEPL4RT25PfTjv6D9O/view?usp=sharing,https://drive.google.com/file/d/1GaX3bTk4UDG-OZ9GM14wZpFqoV42bxr-/view?usp=sharing,,https://drive.google.com/file/d/1cAHI13P_zBi4HV5rZ10xsQmMGULMg-LZ/view?usp=sharing,https://ismir2024.slack.com/archives/C07V2PNJ6FK,p7-10-ccom-huqin-an
517,4,7,18,San Francisco,FALSE,PiJAMA: Piano Jazz with Automatic MIDI Annotations,,Drew Edwards,"Drew Edwards*, Simon Dixon, Emmanouil Benetos","Drew Edwards, Simon Dixon, Emmanouil Benetos",,,,,,,No,https://transactions.ismir.net/articles/162/files/650445222bf95.pdf,https://drive.google.com/file/d/1aTJAffJoOecS_Iady6BqKgQSwjXlPzRB/view?usp=sharing,https://drive.google.com/file/d/13YThwqFz8HwvvAMICm-kWlHjswQs5TmO/view?usp=sharing,https://drive.google.com/file/d/1IOZKuKEfq_De9g9a1atWw3czP4dGbG9N/view?usp=sharing,https://drive.google.com/file/d/1hhMKiTDoHWlBoTj_akKzZ7uuR-yCc4ks/view?usp=sharing,https://ismir2024.slack.com/archives/C07U9FAKZ7Z,p7-18-pijama-piano-jazz
518,3,5,18,San Francisco,FALSE,Automatic Note-Level Score-to-Performance Alignments in the ASAP Dataset,,Silvan David Peter,"Silvan David Peter*, Carlos Eduardo Cancino-Chacón, Francesco Foscarin, Andrew Philip McLeod, Florian Henkel, Emmanouil Karystinaios, Gerhard Widmer","Silvan David Peter, Carlos Eduardo Cancino-Chacón, Francesco Foscarin, Andrew Philip McLeod, Florian Henkel, Emmanouil Karystinaios, Gerhard Widmer",,,,,,,No,https://transactions.ismir.net/articles/149/files/6499a04b4a67d.pdf,https://drive.google.com/file/d/1z3jlHFGtFEDkQvFKlT5RWwy8_bD4ZICM/view?usp=drive_link,https://drive.google.com/file/d/1ngBHOzpFe6KJqf-SFGONyqUD_gjTotfp/view?usp=drive_link,https://drive.google.com/file/d/1PPCSE4er97h5bdquGakChSz-4sxCZflI/view?usp=drive_link,https://drive.google.com/file/d/1hUPjZKdPCadetZ_RL67iUN1y-EZu2rWu/view?usp=drive_link,https://ismir2024.slack.com/archives/C07V2MYGCLR,p5-18-automatic-note-level
519,3,6,20,San Francisco,FALSE,The Sound Demixing Challenge 2023 – Music Demixing Track,,Giorgio Fabbro,"Giorgio Fabbro*, Stefan Uhlich, Chieh-Hsin Lai, Woosung Choi, Marco Martínez-Ramírez, Weihsiang Liao, Igor Gadelha, Geraldo Ramos, Eddie Hsu, Hugo Rodrigues, Fabian-Robert Stöter, Alexandre Défossez, Yi Luo, Jianwei Yu, Dipam Chakraborty, Sharada Mohanty, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Nabarun Goswami, Tatsuya Harada, Minseok Kim, Jun Hyung Lee, Yuanliang Dong, Xinran Zhang, Jiafeng Liu, Yuki Mitsufuji","Giorgio Fabbro, Stefan Uhlich, Chieh-Hsin Lai, Woosung Choi, Marco Martínez-Ramírez, Weihsiang Liao, Igor Gadelha, Geraldo Ramos, Eddie Hsu, Hugo Rodrigues, Fabian-Robert Stöter, Alexandre Défossez, Yi Luo, Jianwei Yu, Dipam Chakraborty, Sharada Mohanty, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Nabarun Goswami, Tatsuya Harada, Minseok Kim, Jun Hyung Lee, Yuanliang Dong, Xinran Zhang, Jiafeng Liu, Yuki Mitsufuji",,,,,,,No,https://transactions.ismir.net/articles/171/files/66210ffd7fc02.pdf,https://drive.google.com/file/d/13IaTpOvaDBR_4L3rzcblrNTXql_TZVY9/view?usp=drive_link,https://drive.google.com/file/d/1RPTcdizaKOjQxTpTvB4feswN9ifXgamK/view?usp=drive_link,https://drive.google.com/file/d/1Atvao35NkyxwDwPrAE6QG4teWZb9-40b/view?usp=drive_link,https://drive.google.com/file/d/1p33N4LSEQd7t9fQGj9G6KPxTxarWQvYB/view?usp=drive_link,https://ismir2024.slack.com/archives/C07V2PNPH33,p6-20-the-sound-demixing
